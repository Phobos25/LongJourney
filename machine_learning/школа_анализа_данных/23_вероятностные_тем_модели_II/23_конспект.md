# Вероятностные тематические модели коллекций текстовых документов. Часть II

Вывод предыдущей части лекции: распределение Дирихле --- слишком слабый регуляризатор, необходимо что-то более сильное. 

# Регуляризованный ЕМ-алгоритм

Теорема
Если $$\Phi, \Theta$$ --- решение задачи максимизации регуляризованного правдоподобия, то оно удовлетворяет системе уравнений

$$\left\{\begin{aligned}n_{dwt}=n_{dw}\frac{\phi_{wt}\theta_{td}}{\sum_{s\in T}\phi_{ws}\theta_{sd}}\text{;}\\
\phi_{wt}=\frac{n_{wt}}{n_t};n_{wt}=\left(\sum_{d\in D}n_{dwt}+\phi_{wt}\frac{\partial{R}}{\partial{\phi_{wt}}} \right)_{+}; n_t=\sum_{w\in W}n_{wt};\\
\theta_{td}=\frac{n_{td}}{n_d};n_{td}=\left(\sum_{w\in d}n_{dwt}+\theta_{td}\frac{\partial{R}}{\partial{\theta_{td}}} \right)_{+}; n_d=\sum_{t\in T}n_{td}\\
\end{aligned}\right.$$

PLSA: $$R(\Phi, \Theta)=0$$
LDA: $$R(\Phi, \Theta)=\sum_{t,w}\beta_{w}\ln{\phi_{wt}}+\sum_{d,t}\alpha_{t}\ln{\theta_{td}}$$

Документ всегда состоит из общих слов --- белые слова, или фоновые --- и из специальных терминов --- искомые слова. Для работы любого алгоритма необходимо что-то сделать с этими белыми словами. Для этого создается общая тема, которая содержит все слова, с какими-то не нулевыми вероятностями. А темы для специальных терминов имеют нулевые вероятности для слов из других тем --- предметных областей. 
Документ не должен иметь слишком много тем. Исключение --- энциклопедия, но в этом случае надо порезать энциклопедию на документы. 
Получается, что и темы и документы должны быть очень разреженными матрицами. Ни один алгоритм сам по себе такую матрицу не восстановит, нам надо заставить его так делать. 

# Регуляризатор сглаживания (почти совпадает с LDA)

**Гипотеза сглаженности:**

распределения $$\phi_{wt}$$ близки заданному распределению $$\beta_{w}$$ 
распределения $$\theta_{td}$$ близки к заданному распределению $$\alpha_t$$

$$\sum_{t\in B}KL_w (\beta_{w}||\phi_{wt})\rightarrow min_{\Phi}$$;  $$\sum_{d\in D}KL_t (\alpha_{t}||\theta_{td})\rightarrow min_{\Theta}$$.

Если расписать KL дивергенцию и слегка упростить, то получим суммы регуляризаторов, которые надо максимизировать:
$$R(\Phi,\Theta)=\beta_{0}\sum_{t\in B}\sum_{w\in W}\beta_{w}\ln{\phi_{wt}}$$ 
$$+\alpha_0\sum_{d\in D}\sum_{t\in B}\alpha_{t}\ln{\theta_{td}}\rightarrow max$$.

Подставляем, получаем формулы М-шага LDA, для всех $$t\in B$$
$$\phi_{wt}\propto n_{wt}+\beta_{0}\beta_{w}$$, $$\theta_{td}\propto n_{td} + \alpha_0\alpha_t$$

# Регуляризатор разреживания (обобщение LDA)

**Гипотеза разряженности:** среди $$\phi_{wt}, \theta_{td}$$ много нулей.

*Если мы хотим, чтобы наши темы были более разреженны, было больше нулей, мы можем использовать следующие шаги.*

Чем сильнее разрежено распределение, тем ниже его энтропия. Максимальной энтропией обладает равномерное распределение. 

Максимизируем дивергенцию между распределениями $$\beta_w, \alpha_{t}$$ (равномерными?) и искомыми распределениями $$\phi_{wt}, \theta_{td}$$:

$$R(\Phi, \Theta) = -\beta_{0}\sum_{t\in S}\sum_{w\in W}\beta_w\ln{\phi_{wt}}$$ $$- \alpha_{0}\sum_{d\in D}\sum_{t\in S}\alpha_t\ln{\theta_{wt}}\rightarrow max.$$

Подставляем, получаем "анти-LDA", для всех $$t\in S$$:
$$\phi_{wt}\propto(n_{wt}-\beta_0\beta_w)_+, \theta_{td}\propto(n_{td}-\alpha_0\alpha_t)$$



# Регуляризатор декоррелирования тем

*Что мы еще хотим? Мы хотим, чтобы темы не были похожи друг на друга. Т.е. нам мало нулей, мы хотим, чтобы было мало пересечений. На лингвистическом языке это говорится так, что каждая тема должна содержать свое лексическое ядро. Некоторое количество слов, которое существенным образом отличает ее от остальных тем* 

**Гипотеза:**  в каждой теме должно быть свое лексическое ядро, отличающее ее от других тем. 

Минимизируем ковариации между вектор-столбцами $$\phi_t$$:
$$R(\Phi)=-\frac{\tau}{2}\sum_{t\in T}\sum_{s\in T/t}\sum_{w\in W}\phi_{wt}\phi_{ws}\rightarrow max$$

Подставляем, получаем, еще один вариант разреживания --- постепенное контрастирование строк матрицы $$\Phi$$:

$$\phi_{wt}\propto (n_{wt} - \tau\phi_{wt}\sum_{s\in T/t}\phi_(ws))_+$$

# Регуляризатор удаления незначимых тем

Если мы можем прореживать распределения с помощью регуляризаторов. Теперь мы будем применять регуляризатор, чтобы ответить на вопрос: сколько тем должно быть в вероятностной модели? 
Чтобы ответить на этот вопрос мы будем исходить из простой **гипотезы:**
Если тема собрала мало слов, то она не нужна

Разреживаем распределение p(t) = $$\sum_{d} p(d)\theta_{td}$$, максимизируя KL-дивергенцию между p(t) и равномерным распределением:

$$R(\Theta) = \tau\sum_{t\in T}\ln{\sum_{d\in D}p(d)\theta_{td}}\rightarrow max$$

Для того, чтобы проредить такое распределение, надо обнулить где-то p(t), и какая-то тема станет нулевой вероятностью.  
p(t) = 0, означает, что ни одно слово не было отнесено к теме t, т.е. занулили всю строку матрицы $$\Theta$$. Фактически, ни в одном документе темы t нету. 

Этот регуляризатор зависит только от $$\theta$$ (предыдущий зависил только $$\phi$$). Дифференцируем, подставляем, получаем:
$$\theta_{td}\propto \left( n_{td} - \tau\frac{n_d}{n_t}\theta_{td}\right)_+$$

В знаменателе сидит $$n_t$$, если $$n_t$$ маленький, то эта дробь большая, то мы вычитаем большое число со всех элементов строки матрицы $$\Theta$$. В формуле стоит положительная срезка, значит увеличивая параметр $$\tau$$ --- коэффициент регуляризации, мы будем увеличивать число тем, которые полностью обнулились, как строки в матрице $$\Theta$$. 
Это такой инструмент, которые позволяет, увеличивая коэффициент регуляризации $$n_t$$, отключать маленькие темы. Чем меньше $$n_t$$ (тема набрала мало слов со всей коллекции), тем больше у него шанс вылететь из модели. Если модель работает и видит, что эта тема стала совсем маленькая, то этот регуляризатор поможет ей уйти. Здесь получается отбор тем. 

Строки матрицы $$\Theta$$ могут целиком обнуляться для тем t, собравших мало слов по коллекции, $$n_t = \sum_{d}\sum_{w}n_{dwt}$$

# Регуляризатор для максимизации когерентности тем

Примитивный пример: термины, которые образуют словосочетания. *Машина опорных векторов*, отдельно эти слова ничего не значат, но вместе образуют узанаваемый термин машинного обучения. 

**Гипотеза**: тема лучше интерпретируется, если она содержит когерентные (часто встречающиеся рядом) слова u, w $$\in$$ W.

Пусть $$C_{uw}$$ --- оценка когерентности, например $$\hat{p}(w|u)=\frac{N_{uw}}{N_u}$$. Согласуем $$\phi_{wt}$$ с оценками $$\hat{p}(w|t)$$ по когерентным словам,

$$\hat{p}(w|t)= \sum_{u}p(w|u)p(u|t) = \frac{1}{n_t}\sum_{u}C_{uw}n_{ut}$$;

$$R(\Phi,\Theta) = \tau\sum_{t\in T}n_t\sum_{w\in W}\hat{p}(w|t)\ln{\phi_{wt}}\rightarrow max$$

Подставляем, получаем еще один вариант сглаживания:

$$\phi_{wt}\propto n_{wt}+\tau\sum_{u\in W/w}C_{uw}n_{ut}$$

# Мультимодальная классификация докуметов:

$$R(\Phi,\Theta) =\sum^{m}_{j=1}\tau_j\sum_{d\in D}n_{dx}\ln\sum_{t\in T}\phi_{xt}\theta_{td}\rightarrow max$$

Примеры модальностей в текстах:
* слова - основная модальность
* слова каждого языка --- отдельная модальность. *Следующая модальность те же слова, но на другом языке.*
* пользователи, смотревшие документ. *ID пользователя --- это тоже слово, документ теперь состоит не только из слов, но еще хранит в себе информацию о том, как им пользовались. Это список идентификаторов пользователей, которые посмотрели этот документ. Если это мы увяжем в одну модель, то сможем и описать интересы пользователей нашими темами.*
* рекламные баннеры, просмотренные вместе с документом. *А что если у нас есть реклама в интернете? И пользователь, когда просматривает документ, иногда кликает по этому баннеру. У нас получается множество модальностей; множеств, которые попарно входят с друг другом во взаимоотношения. У нас есть множество страниц, пользователей, слов, баннеров. Пользователь входит на страницы, пользователи кликают по баннерам, слова есть на страницах и слова привязаны к баннерам. Но, все вот это вот, мы хотим описать одним набором тем. Как построить такую тематическую модель, которая позволит одними и теми же темами описать все эти модальности.*
* категории рубрикатора
* авторы документов
* метки времени
* документы, ссылающиеся на данный
* документ, на которые ссылается данный
* сущности (entity), упоминаемые в текстах
* признаки на изображениях, свазянные с текстом.

# Мультимодальные тематические модели

Как определить модальности? Это слова, которые покрашены в разные цвета. 

Произвольное чило модальностей $$X_j$$ (до этого был 1 словарь w, теперь у нас их X штук для каждой модальности j), j=1,...,m
Вероятностное пространство $$D\times T\times X, X=X_1\sqcup...\sqcup X_m$$
Каждый документ d состоит из токенов $$x_1,...,x_{n_d}\in X$$.
Тематическая модель j-й модальности:

$$p(x|d)=\sum_{t\in T} p(x|t)p(t|d)=\sum_{t\in T}\phi_{xt}\theta_{td}, x\in X_j, d\in D$$

Задачи максимизации взвешенного правдоподобия:

$$L(\Phi, \Theta)=\sum^{m}_{j=1}\tau_j\sum_{d\in D}n_{dx}\ln\sum_{t\in T}\phi_{xt}\theta_{td}\rightarrow max$$

при ограничениях нормировки и неотрицательности
$$\phi_{xt}\geq 0; \sum_{x\in X_j}\phi_{xt} = 1; \theta_{td}\geq 0; \sum_t\theta_{td} = 1$$

# Резюме

* Тематическое моделирование --- это восстановление латентных тем по коллекции текстовых документов.
* Задача сводится к стохастическому матричному разложению.
* Стандартные методы --- PLSA и LDA
* Задача является некорректно поставленной, т.к. множество ее решений в общем случае бесконечно
* Уточнение постановки задачи с помощью регуляризации приводит к многокритериальной оптимизации
* Регуляризаторы тематических моделей разнообраны, аддитивная регуляризация позволяет их кобминировать не сильно изменяя EM-алгоритм.
