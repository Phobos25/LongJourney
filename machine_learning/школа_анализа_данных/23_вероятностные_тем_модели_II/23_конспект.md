# Вероятностные тематические модели коллекций текстовых документов. Часть II

Вывод предыдущей части лекции: распределение Дирихле --- слишком слабый регуляризатор, необходимо что-то более сильное. 

# Регуляризованный ЕМ-алгоритм

Теорема
Если $$\Phi, \Theta$$ --- решение задачи максимизации регуляризванного правдоподобия, то оно удовлетворяет системе уравнений

$$\left\{\begin{aligned}n_{dwt}=n_{dw}\frac{\phi_{wt}\theta_{td}}{\sum_{s\in T}\phi_{ws}\theta_{sd}}\text{;}\\
\phi_{wt}=\frac{n_{wt}}{n_t};n_{wt}=\left(\sum_{d\in D}n_{dwt}+\phi_{wt}\frac{\partial{R}}{\partial{\phi_{wt}}} \right)_{+}; n_t=\sum_{w\in W}n_{wt};\\
\theta_{td}=\frac{n_{td}}{n_d};n_{td}=\left(\sum_{w\in d}n_{dwt}+\theta_{td}\frac{\partial{R}}{\partial{\theta_{td}}} \right)_{+}; n_d=\sum_{t\in T}n_{td}\\
\end{aligned}\right.$$

PLSA: $$R(\Phi, \Theta)=0$$
LDA: $$R(\Phi, \Theta)=\sum_{t,w}\beta_{w}\ln{\phi_{wt}}+\sum_{d,t}\alpha_{t}\ln{\theta_{td}}$$

Документ всегда состоит из общих слов --- белые слова, или фоновые --- и из специальных терминов --- искомые слова. Для работы любого алгоритма необходимо что-то сделать с этими белыми словами. Для этого создается общая тема, которая содержит все слова, с какими-то не нулевыми вероятностями. А темы для специальных терминов имеют нулевые вероятности для слов из других тем --- предметных областей. 
Документ не должен иметь слишком много тем. Исключение --- энциклопедия, но в этом случае надо порезать энциклопедию на документы. 
Получается, что и темы и документы должны быть очень разреженными матрицами. Ни один алгоритм сам по себе такую матрицу не восстановит, нам надо заставить его так делать. 

# Регуляризатор сглаживания (почти совпадает с LDA)

**Гипотеза сглаженности:**

распределения $$\phi_{wt}$$ близки заданному распределению $$\beta_{w}$$ 
распределения $$\theta_{td}$$ близки к заданному распределению $$\alpha_t$$

$$\sum_{t\in B}KL_w (\beta_{w}||\phi_{wt})\rightarrow min_{\Phi}$$;  $$\sum_{d\in D}KL_t (\alpha_{t}||\theta_{td})\rightarrow min_{\Theta}$$.

Если расписать KL дивергенцию и слегка упростить, то получим суммы регуляризаторов, которые надо максимизировать:
$$R(\Phi,\Theta)=\beta_{0}\sum_{t\in B}\sum_{w\in W}\beta_{w}\ln{\phi_{wt}}$$ 
$$+\alpha_0\sum_{d\in D}\sum_{t\in B}\alpha_{t}\ln{\theta_{td}}\rightarrow max$$.

Подставляем, получаем формулы М-шага LDA, для всех $$t\in B$$
$$\phi_{wt}\propto n_{wt}+\beta_{0}\beta_{w}$$, $$\theta_{td}\propto n_{td} + \alpha_0\alpha_t$$

# Регуляризатор разреживания (обобщение LDA)

**Гипотеза разряженности:** среди $$\phi_{wt}, \theta_{td}$$ много нулей.

*Если мы хотим, чтобы наши темы были более разреженны, было больше нулей, мы можем использовать следующие шаги.*

Чем сильнее разрежено распределение, тем ниже его энтропия. Максимальной энтропией обладает равномерное распределение. 

Максимизируем дивергенцию между распределениями $$\beta_w, \alpha_{t}$$ (равномерными?) и искомыми распределениями $$\phi_{wt}, \theta_{td}$$:

$$R(\Phi, \Theta) = -\beta_{0}\sum_{t\in S}\sum_{w\in W}\beta_w\ln{\phi_{wt}}$$ $$- \alpha_{0}\sum_{d\in D}\sum_{t\in S}\alpha_t\ln{\theta_{wt}}\rightarrow max.$$

Подставляем, получаем "анти-LDA", для всех $$t\in S$$:
$$\phi_{wt}\propto(n_{wt}-\beta_0\beta_w)_+, \theta_{td}\propto(n_{td}-\alpha_0\alpha_t)$$



# Регуляризатор декоррелирования тем

*Что мы еще хотим? Мы хотим, чтобы темы не были похожи друг на друга. Т.е. нам мало нулей, мы хотим, чтобы было мало пересечений. На лингвистическом языке это говорится так, что каждая тема должна содержать свое лексическое ядро. Некоторое количество слов, которое существенным образом отличает ее от остальных тем* 

**Гипотеза:**  в каждой теме должно быть свое лексическое ядро, отличающее ее от других тем. 

Минимизируем ковариации между вектор-столбцами $$\phi_t$$:
$$R(\Phi)=-\frac{\tau}{2}\sum_{t\in T}\sum_{s\in T/t}\sum_{w\in W}\phi_{wt}\phi_{ws}\rightarrow max$$

Подставляем, получаем, еще один вариант разреживания --- постепенное контрастирование строк матрицы $$\Phi$$:

$$\phi_{wt}\propto (n_{wt} - \tau\phi_{wt}\sum_{s\in T/t}\phi_(ws))_+$$

# Регуляризатор удаления незначимых тем

Если мы можем прореживать распределения с помощью регуляризаторов. Теперь мы будем применять регуляризатор, чтобы ответить на вопрос: сколько тем должно быть в вероятностной модели? 
Чтобы ответить на этот вопрос мы будем исходить из простой **гипотезы:**
Если тема собрала мало слов, то она не нужна

Разреживаем распределение p(t) = $$\sum_{d} p(d)\theta_{td}$$, максимизируя KL-дивергенцию между p(t) и равномерным распределением:

$$R(\Theta) = \tau\sum_{t\in T}\ln{\sum_{d\in D}p(d)\theta_{td}}\rightarrow max$$

Для того, чтобы проредить такое распределение, надо обнулить где-то p(t), и какая-то тема станет нулевой вероятностью.  
p(t) = 0, означает, что ни одно слово не было отнесено к теме t, т.е. занулили всю строку матрицы $$\Theta$$. Фактически, ни в одном документе темы t нету. 

Этот регуляризатор зависит только от $$\theta$$ (предыдущий зависил только $$\phi$$). Дифференцируем, подставляем, получаем:
$$\theta_{td}\propto \left( n_{td} - \tau\frac{n_d}{n_t}\theta_{td}\right)_+$$

В знаменателе сидит $$n_t$$, если $$n_t$$ маленький, то эта дробь большая, то мы вычитаем большое число со всех элементов строки матрицы $$\Theta$$. В формуле стоит положительная срезка, значит увеличивая параметр $$\tau$$ --- коэффициент регуляризации, мы будем увеличивать число тем, которые полностью обнулились, как строки в матрице $$\Theta$$. 
Это такой инструмент, которые позволяет, увеличивая коэффициент регуляризации $$n_t$$, отключать маленькие темы. Чем меньше $$n_t$$ (тема набрала мало слов со всей коллекции), тем больше у него шанс вылететь из модели. Если модель работает и видит, что эта тема стала совсем маленькая, то этот регуляризатор поможет ей уйти. Здесь получается отбор тем. 

Строки матрицы $$\Theta$$ могут целиком обнуляться для тем t, собравших мало слов по коллекции, $$n_t = \sum_{d}\sum_{w}n_{dwt}$$




32:01