# Вероятностные тематические модели коллекций текстовых документов. Часть II

Вывод предыдущей части лекции: распределение Дирихле --- слишком слабый регуляризатор, необходимо что-то более сильное. 

# Регуляризованный ЕМ-алгоритм

Теорема
Если $$\Phi, \Theta$$ --- решение задачи максимизации регуляризванного правдоподобия, то оно удовлетворяет системе уравнений

$$\left\{\begin{aligned}n_{dwt}=n_{dw}\frac{\phi_{wt}\theta_{td}}{\sum_{s\in T}\phi_{ws}\theta_{sd}}\text{;}\\
\phi_{wt}=\frac{n_{wt}}{n_t};n_{wt}=\left(\sum_{d\in D}n_{dwt}+\phi_{wt}\frac{\partial{R}}{\partial{\phi_{wt}}} \right)_{+}; n_t=\sum_{w\in W}n_{wt};\\
\theta_{td}=\frac{n_{td}}{n_d};n_{td}=\left(\sum_{w\in d}n_{dwt}+\theta_{td}\frac{\partial{R}}{\partial{\theta_{td}}} \right)_{+}; n_d=\sum_{t\in T}n_{td}\\
\end{aligned}\right.$$

PLSA: $$R(\Phi, \Theta)=0$$
LDA: $$R(\Phi, \Theta)=\sum_{t,w}\beta_{w}\ln{\phi_{wt}}+\sum_{d,t}\alpha_{t}\ln{\theta_{td}}$$

Документ всегда состоит из общих слов --- белые слова, или фоновые --- и из специальных терминов --- искомые слова. Для работы любого алгоритма необходимо что-то сделать с этими белыми словами. Для этого создается общая тема, которая содержит все слова, с какими-то не нулевыми вероятностями. А темы для специальных терминов имеют нулевые вероятности для слов из других тем --- предметных областей. 
Документ не должен иметь слишком много тем. Исключение --- энциклопедия, но в этом случае надо порезать энциклопедию на документы. 
Получается, что и темы и документы должны быть очень разреженными матрицами. Ни один алгоритм сам по себе такую матрицу не восстановит, нам надо заставить его так делать. 

# Регуляризатор сглаживания (почти совпадает с LDA)

**Гипотеза сглаженности:**

распределения $$\phi_{wt}$$ близки заданному распределению $$\beta_{w}$$ 
распределения $$\theta_{td}$$ близки к заданному распределению $$\alpha_t$$

$$\sum_{t\in B}KL_w (\beta_{w}||\phi_{wt})\rightarrow min_{\Phi}$$;  $$\sum_{d\in D}KL_t (\alpha_{t}||\theta_{td})\rightarrow min_{\Theta}$$.

Если расписать KL дивергенцию и слегка упростить, то получим суммы регуляризаторов, которые надо максимизировать:
$$R(\Phi,\Theta)=\beta_{0}\sum_{t\in B}\sum_{w\in W}\beta_{w}\ln{\phi_{wt}}$$ 
$$+\alpha_0\sum_{d\in D}\sum_{t\in B}\alpha_{t}\ln{\theta_{td}}\rightarrow max$$.

Подставляем, получаем формулы М-шага LDA, для всех $$t\in B$$
$$\phi_{wt}\propto n_{wt}+\beta_{0}\beta_{w}$$, $$\theta_{td}\propto n_{td} + \alpha_0\alpha_t$$

# Регуляризатор разреживания (обобщение LDA)

**Гипотеза разряженности:** среди $$\phi_{wt}, \theta_{td}$$ много нулей.

*Если мы хотим, чтобы наши темы были более разреженны, было больше нулей, мы можем использовать следующие шаги.*

Чем сильнее разрежено распределение, тем ниже его энтропия. Максимальной энтропией обладает равномерное распределение. 

Максимизируем дивергенцию между распределениями $$\beta_w, \alpha_{t}$$ (равномерными?) и искомыми распределениями $$\phi_{wt}, \theta_{td}$$:

$$R(\Phi, \Theta) = -\beta_{0}\sum_{t\in S}\sum_{w\in W}\beta_w\ln{\phi_{wt}}$$ $$- \alpha_{0}\sum_{d\in D}\sum_{t\in S}\alpha_t\ln{\theta_{wt}}\rightarrow max.$$

Подставляем, получаем "анти-LDA", для всех $$t\in S$$:
$$\phi_{wt}\propto(n_{wt}-\beta_0\beta_w)_+, \theta_{td}\propto(n_{td}-\alpha_0\alpha_t)$$



# Регуляризатор декоррелирования тем

*Что мы еще хотим? Мы хотим, чтобы темы не были похожи друг на друга. Т.е. нам мало нулей, мы хотим, чтобы было мало пересечений. На лингвистическом языке это говорится так, что каждая тема должна содержать свое лексическое ядро. Некоторое количество слов, которое существенным образом отличает ее от остальных тем* 

**Гипотеза:**  в каждой теме должно быть свое лексическое ядро, отличающее ее от других тем. 

Минимизируем ковариации между вектор-столбцами $$\phi_t$$:
$$R(|\Phi)=-\frac{\tau}{2}\sum_{t\in T}\sum_{s\in T/t}\sum_{w\in W}\phi_{wt}\phi_{ws}\rightarrow max$$

Подставляем, получаем, еще один вариант разреживания --- постепенное контрастирование строк матрицы $$\Phi$$:

$$\phi_{wt}\propto (n_{wt} - \tau\phi_{wt}\sum_{s\in T/t}\phi_(ws))_+$$







32:01