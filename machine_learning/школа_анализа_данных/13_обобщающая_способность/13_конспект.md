# лекция №13: Обобщающая способность. Методы отбора признакова

Давайте рассмотрим следующую задачу. Пусть у нас есть обучающая выборка - это множество пар объекты-признаки. Т.е. у нас есть объект и у нас есть ответ, который относит объект к тому или иному классу. 

**Задача**. Найти функцию a(x), способную давать правильные ответы на *тестовых объектах*. 

**Типы признаков:**
* $$D_{j}$$={0,1} - бинарный признак. *Признак да или нет, болел не болел и т.д.*;
* |$$D_{j}$$| < $$\infty$$- номинальный (или категориальный) - *это когда объект принадлежит какому то региону, территории или зоне*.
* $$D_{j}$$ упорядочено - порядковый признак;
* $$D_{j}$$ = R - количественный признак. 
 
**Типы задач, в зависимости от Y:**
* Y= {0,1} или Y = {01,+1} - классификация на 2 класса;
* Y = {1,...,M} - на М непересекающихся классов. *Если классов много*
* Y = $$\{0,1\}^{M}$$ - на М классов, которые могут пересекаться; *т.е. один и тот же признак может относиться к нескольким классам*
* Y = R - задача восстановления регрессии;
* Y упорядоченно - задача ранжирования (learning to rank)

## Обучение регрессии

Задача регрессии, Y = R

1) Выбираем *модель регрессии*, например, *линейную*:
   $$a(x,w) = <x,w> = \sum^{n}_{j=1}f_{j}(x)w_{j}$$  x, w $$\in R^{n}$$
   *обычно используют скалярное произведение весов и объектов*
2) Выбираем функцию потерь (например, квадратичную):
   $$L(a,y) = (a-y)^{2}$$
3) Минимизируем потери методом наименьших квадратов:
   $$Q(w)= \frac{1}{l}\sum^{l}_{i=1}(a(x_{i},w)-y)^{2}\rightarrow min_{w}$$
   Рассматриваем среднюю ошибку, и минимизируем его по w. Т.е. подбираем такой w, при котором Q(w) - минимально. 
4) Проверяем прогностическую (обобщающую) способность:
   $$\tilde{Q}(w)= \frac{1}{k}\sum^{k}_{i=1}(a(\tilde{x_{i}},w))^{2}$$
   Этот шаг использует полученные значения весов w, и проверяет на тестовой выборке. 
   
6:32