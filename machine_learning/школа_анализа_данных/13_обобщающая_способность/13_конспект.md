# лекция №13: Обобщающая способность. Методы отбора признакова

Давайте рассмотрим следующую задачу. Пусть у нас есть обучающая выборка - это множество пар объекты-признаки. Т.е. у нас есть объект и у нас есть ответ, который относит объект к тому или иному классу. 

**Задача**. Найти функцию a(x), способную давать правильные ответы на *тестовых объектах*. 

**Типы признаков:**
* $$D_{j}$$={0,1} - бинарный признак. *Признак да или нет, болел не болел и т.д.*;
* |$$D_{j}$$| < $$\infty$$- номинальный (или категориальный) - *это когда объект принадлежит какому то региону, территории или зоне*.
* $$D_{j}$$ упорядочено - порядковый признак;
* $$D_{j}$$ = R - количественный признак. 
 
**Типы задач, в зависимости от Y:**
* Y= {0,1} или Y = {01,+1} - классификация на 2 класса;
* Y = {1,...,M} - на М непересекающихся классов. *Если классов много*
* Y = $$\{0,1\}^{M}$$ - на М классов, которые могут пересекаться; *т.е. один и тот же признак может относиться к нескольким классам*
* Y = R - задача восстановления регрессии;
* Y упорядоченно - задача ранжирования (learning to rank)

## Обучение регрессии

Задача регрессии, Y = R

1) Выбираем *модель регрессии*, например, *линейную*:
   $$a(x,w) = <x,w> = \sum^{n}_{j=1}f_{j}(x)w_{j}$$  x, w $$\in R^{n}$$
   *обычно используют скалярное произведение весов и объектов*
2) Выбираем функцию потерь (например, квадратичную):
   $$L(a,y) = (a-y)^{2}$$
3) Минимизируем потери методом наименьших квадратов:
   $$Q(w)= \frac{1}{l}\sum^{l}_{i=1}(a(x_{i},w)-y)^{2}\rightarrow min_{w}$$
   Рассматриваем среднюю ошибку, и минимизируем его по w. Т.е. подбираем такой w, при котором Q(w) - минимально. 
4) Проверяем прогностическую (обобщающую) способность:
   $$\tilde{Q}(w)= \frac{1}{k}\sum^{k}_{i=1}(a(\tilde{x_{i}},w))^{2}$$
   Этот шаг использует полученные значения весов w, и проверяет на тестовой выборке. 
   
## Обучение классификация

Задача классификации Y = {-1;+1}
1) Выбираем модель классификации, например, линейную:
a(x,w) = sign(x,w)
Для классификации можно разные модели, но мы рассмотрим самую простую - линейную. Здесь, так же, рассматривается скалярное произведение, но не значение а знак. Если скалярное произведение меньше 0, то это -1, а если больше 0 то +1. 
2) Выбираем функцию потерь (бинарную или ее аппроксимацию):
L(a,y)=[<$$x_{i}$$,w>$$y_{i}$$<0] $$\leq L(<x_{i},w>y_{i})$$
для оценки потерь, мы используем простой метод, если произведение ниже 0 (т.е. модель оценила неправильно), то мы штрафуем, если больше, то правильно (если оба + или - мы все равно получим >0 положительное значение) и мы не штрафуем. 
То что написано в квадратной скобке - это скобка Айверсона, которая утверждает: если то, что приведено внутри скобки истинно, то это 1, если ложно то 0. 

3) Минимизируем частоту ошибок на обучающей выборке:
$$Q(w)=\frac{1}{l}\sum^{l}_{i=1}[a(x_{i}, w)y_{i}<0]\leq \frac{1}{l}\sum^{l}_{i=1}L(<x_{i},w>y_{i})\rightarrow min$$
т.к. сложно минимизировать ошибки, мы будем смотреть по верхней оценке и считать, что если верхняя оценка будет небольшой, то и частота ошибок будет небольшой. 

4) Проверяем прогностическую (обобщающую) спобоность
$$\tilde{Q}(w)=\frac{1}{k}\sum^{k}_{i=1}[<\tilde{x_{i}},w>\tilde{y_{i}}<0]$$




16:00