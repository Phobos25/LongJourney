## 15_лекция. Композиции классификаторов

Идея композиции заключается в том, что если мы хотим поточнее узнать длину палки. Мы должны измерять его несколько раз разными линейками и усреднять их. Даже если линейки не зависимы друг от друга, то дисперсия усреднения будет уменьшаться как корень квадртаный от количества измерений ($$\sqrt{N}$$, N - количество измерений). 

В композиционной методике используется несколько алгоритмов для обучения на одной и той же выборке, что исходя из сказанного выше, должна уменьшать среднюю ошибку модели. 

* **Пример 1**: классификация на 2 класса, Y={-1,+1};
    $$a(x)=sign(b(x))$$
где R=R, b:X $$\rightarrow$$ R, c(b)$$\equiv$$sign(b)

* **Пример 2**: классификация на M классов Y = {1,...,M}:
    $$a(x)=argmax_{y\in Y}b_{y}(x)$$
где R=$$R^{M}$$, b: X $$\rightarrow R^{M}$$, C($$b_{1},...,b_{M}$$)$$\equiv argmax_{y\in Y b_{y}}$$

* **Пример 3**: регрессия, Y=R=*R*:
    C(b)$$\equiv$$b - решающее правило не нужно. 

Смесь алгоритмов (Misture of Experts) - алгоритм заключается в предположении, что в своем подпространстве объектов определенные алгоритмы работают лучше. В своем роде, как эксперт в области своей компетенции. Эта компетенция зависит от X. Вместо весов используется функция, которая зависит от x. 

**Адаптивный буст (AdaBoost)**
Базовый алгоритм возвращает либо +1, либо -1. обобщение - возвращает 0, (лучше промолчать, чем соврать) не дает никакого вклада. 

Мы строим взвешенное голосование и берем его знак. 
$$a(x)=sign(\sum^{T}_{t=1})\alpha_{t}b(t)(x)$$, $$x\in X$$

Наша задача одновременно найти базовые алгоритмы $$b_{t}$$ и коэффициенты к нему $$\alpha_{t}$$. Такая задача очень сложна в исполнении, поэтому мы будем применять жадный алгоритм добавления для упрощения. Мы будем добавлять базовые алгоритмы в композицию по очереди, и каждый следующий должен компенсировать недостатки предыдущего. 

$$Q_{T}=\sum^{l}_{i=1}[y_{i}\sum^{T}_{t=1}\alpha_{i}b_{t}(x_{i})<0]$$

Две основные эвристики бустинга:
* фиксация базовый алгоритмов $$\alpha_{1}b_{1}(x),...,\alpha_{t-1}b_{t-1}(x)$$ при добавлении $$\alpha_{t}b_{t}(x)$$;
* гладка аппроксимация пороговой функции потерь $$[M\leq 0]$$

**Алгоритм AdaBoost**

**Вход** Обучающая выборка $$X^{l}$$; параметр Т

**Выход** базовые алгоритмы и их веса $$\alpha_{t}b_{t}$$, t=1,...,T;
1) инициализировать веса объектов
$$w_{i}:=1/l, i=1,...,l$$

2) **для всех** t=1,...,T

3) обучить базовый алгоритм;
$$b_{t}:=argmin_{b} N(b;W^{l})$$

4) $$\alpha_{t}:=\frac{1}{2}ln\frac{1-N(b_{t};W^{l})}{N(b_{t};W^{l})}$$

5)  обновить веса объектов
$$w_{i}:=w_{i}exp(- \alpha_{t}y_{i}b_{t}(x_{i}))$$, i=1,...,l;


6) нормировать веса объектов
$$w_{0}:=\sum^{l}_{j=1}w_{j}$$
$$w_{i}:=w_{i}/w_{0}$$, i=1,...,t

27:05