## 15_лекция. Композиции классификаторов

Идея композиции заключается в том, что если мы хотим поточнее узнать длину палки. Мы должны измерять его несколько раз разными линейками и усреднять их. Даже если линейки не зависимы друг от друга, то дисперсия усреднения будет уменьшаться как корень квадртаный от количества измерений ($$\sqrt{N}$$, N - количество измерений). 

В композиционной методике используется несколько алгоритмов для обучения на одной и той же выборке, что исходя из сказанного выше, должна уменьшать среднюю ошибку модели. 

* **Пример 1**: классификация на 2 класса, Y={-1,+1};
    $$a(x)=sign(b(x))$$
где R=R, b:X $$\rightarrow$$ R, c(b)$$\equiv$$sign(b)

* **Пример 2**: классификация на M классов Y = {1,...,M}:
    $$a(x)=argmax_{y\in Y}b_{y}(x)$$
где R=$$R^{M}$$, b: X $$\rightarrow R^{M}$$, C($$b_{1},...,b_{M}$$)$$\equiv argmax_{y\in Y b_{y}}$$

* **Пример 3**: регрессия, Y=R=*R*:
    C(b)$$\equiv$$b - решающее правило не нужно. 

Смесь алгоритмов (Misture of Experts) - алгоритм заключается в предположении, что в своем подпространстве объектов определенные алгоритмы работают лучше. В своем роде, как эксперт в области своей компетенции. Эта компетенция зависит от X. Вместо весов используется функция, которая зависит от x. 

11:05