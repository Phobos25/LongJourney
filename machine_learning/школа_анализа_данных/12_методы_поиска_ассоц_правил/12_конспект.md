# методы поиска ассоциативных правил 
dfdf
## Определения и обозначения

см. рисунок

ассоциативное правило (association rule) - это пара непересекающихся наборов таких, что 
1) наборы $$\phi$$ и y совместно часто встречаются,
$$\nu(\phi \cap y)\geq \delta$$

2) если встречаются $$\phi$$, то часто встречаются также y
$$\nu(y|\phi)\equiv \frac{\nu(\phi\hat y)}{\nu(\phi)}\geq \kappa$$

где $$\nu(y|\phi)$$ - значимость (confidence) правила
* Параметр $$\delta$$ - минимальная поддержка, MinSupp
* Параметр $$\kappa$$ - минимальая значимость, MinConf

Задача сводится к тому, чтобы найти как можно больше ассоциативных правил, которые удовлетворяют этим двум методам

## классический пример
Это делалось в 90-е годы, для data mining (для выкапывания данных) из БД супермаркетов, которые там накопились. Цель заключалась в том, чтобы оптимизировать расстановку продуктов для увеличения продаж. 

**Анализ рыночных корзин (market basket analysis) [1993]**
признаки - товары (предметы, items)
объекты - чеки (транзакции)

$$f_{j}(x_{i}) = 1$$ в i-м чеке зафиксирована покупка j-го товара

Пример 1: "если куплен хлеб $$\phi$$, то будет куплено и молоко y  с вероятностью $$\nu(y|\phi)$$=60%; причём оба товара покупаются совместно с вероятностью $$\nu(\phi\cup y)$$=2%"

Цели анализа:
* оптимизировать размещение товаров на полках;
* формировать персональные рекомендации;
* планировать рекламные кампании (промо-акции);
* более эффективно управлять ценами и ассортиментом.


Пример 2: **Выявление тематики в коллекциях текстовых документов**

признаки - термины (отдельные слова или выражения)
объекты - текстовые документы

$$f_{j}(x_{i})=1$$ - в i-м тексте (часто) употребляется j-й термин.

Тема - это совокупность терминов, совместно встречающихся в узком подмножестве документов, то есть частый набор. 

**Недостаток:** слишком жесткое требование, чтобы в тексте встречались все слова темы. Вероятностные модели адекватнее.

**Цели анализа:**

* получение признаков для выделения терминов;
* выделение наиболее четких тем;
* формирование начальных приближений для Topic Models. 

## Два этапа построения правил. Свойство антимонотонности

Поскольку $$\phi(x)=\bigwedge_{f\in \phi} f(x)$$ - конъюнкция, имеет место

Свойство антимонотонности:

для любых $$\psi, \phi \subset F$$ из $$\phi \subset \psi$$ следует $$\nu(\phi)\geq \nu(\psi)$$

Следствия:
1) если $$\psi$$ частый, то все его подмножества $$\phi \subset \psi$$ частые.
2) если $$\phi$$ не частый, то все наборы $$\psi \supset \phi$$ также не частые. 
3) $$\nu(\phi \cup \psi)\leq \nu(\phi)$$ для любых $$\phi, \psi$$

Два этапа поиска ассоциативных правил:
1) поиск частых наборов (многократный просмотр транзакционной базы данных)
2) выделение ассоциативных правил (простая эффективная процедура в оперативной памяти)

## Алгоритм APriory (основная идея - поиск в ширину)

**Вход:** $$X^{l}$$ - обучающая выборка; минимальная поддержка $$\delta$$; минимальная значимость $$\kappa$$;
**Выход:** R={($$\phi$$, y)} - список ассоциативных правил;

1) множество всех частых исходных принаков:
$$G_{1}:=$${$$f\in F|\nu(f)\geq \delta$$};
2) для всех j=2,...,n
3) множество всех частых наборов мощности j:
$$G_{j}$$:= {$$\phi \cup$${f}|$$\phi\in G_{j-1}, f\in G_{1}/\phi$$,$$\nu(\phi\cap)$${f}$$\geq$$\delta};
4) если $$G_{j}=\varnothing$$ то
5) выход из цикла по j;
6) R:=$$\varnothing$$;
7) для всех $$\psi\in G_{j}$$, j=2,...,n
8) AssocRules (R, $$\psi, \varnothing$$)

## Этап 2
Считается, что этот этап уже решенный, и тут нечего делать, и ничего не надо делать. 

def. Простой алгоритм, выполняемый быстро, как правило, полностью в оперативной памяти.
**Вход и выход:** R - список ассоциативных правил;
($$\phi, y$$) - ассоциативное правило; 

1) **ПРОЦЕДУРА** AssocRules ($$R, \phi, y$$);
2) **для всех** f$$\in \phi$$: $$id_{f} > max_{g\in y}id_{g}$$ (чтобы избежать повторов y)
3) $$\phi':=\phi$$\ {$$f$$}; $$y':=y\cup$${f};
4) **если** $$\nu(y'|\phi')\geq \kappa$$ **то**
5)  добавить ассоциативное правило ($$\phi',y'$$) в список R;
6)  если |$$\phi'$$|>1 **то**
7)  AssocRules (R,$$\phi'$$,y');

$$id_{f}$$ - порядковый номер признака f в F={$$f_{1},...,f_{n}$$}

когда первый раз вызываем эту процедуру, R - список, будет пустым, $$\phi$$ - не пусто, а y пуст.
Эта процедура делает следующее: мы перебираем все признаки по очереди, которые образуют подмножество $$\phi$$. После чего, мы берем два подмножества $$\phi$$ и y, берем признаки из $$\phi$$ и переносим их в y. Такая пара признаков обозначается $$\phi'$$ и y'. Если y' при условии $$\phi'$$ - это ассоциативное правило (которое определяется по значимости $$\kappa$$), то мы добавляем эту пару, как ассоциативное правило в список R. 
Если у $$\phi'$$ есть еще признак, то мы вызываем ту же процедуру еще раз, но в этот раз $$\phi'$$ содержит на единицу меньше признаков, а y' больше. 

Следует заметить, что шаг 4) очень сильно усекает перебор. Если мы хотим еще сильнее сократить перебор, то мы можем просто увеличить MinConf $$\kappa$$ еще сильнее. 

Чтобы исключить повторное проверку признаков, например 2-17 и 17-2, в организации подмножества y используют правило, что порядковый номер признаков должен возрастать (или убывать). Такой простой технический трюк, может значительно сократить число перебора. 

К сожалению, несмотря на всю простоту и изящность алгоритма APriory, на практике он не применяется. На практике всегда очень много специфики, из-за чего надо данный алгоритм обобщать. 

***Модификации алгоритмов индукции ассоциативных правил***
* Более эффективные структуры данных для быстрого поиска частых наборов.
* Сэмплинг с последующей проверкой правил на полной выборке. (*Если база данных слишком большая, то лучше взять только часть данных и выработать правила на ней, а потом проверить на всей выборке. Такой подход позволит сократить общее время обучения модели и повысит его эффективность*).
* Иерархические алгоритмы, учитывающие иерархию признаков (например, товарное дерево). *Здесь, надо иметь в виду, что когда мы обсуждали, что клиент покупает хлеб с молоком, мы очень сильно упрощали картину. Т.к. сортов хлеба очень много, это может быть и черный хлеб, белый хлеб, булочка и т.д. Они все имеют разные товарные id (items). То же самое с молоком. Дополнительно добавляются такие признаки как, имеет ли значение сорт хлеба? или большее значение имеет молоко? покупает ли клиент любое хлебобулочное изделие и специфичное молоко? Только определнный сорт хлеба и любое молоко? Или любой кисломолочный продукт?*
* Учёт времени: инкрементные и декрементные алгоритмы. *Следует учитывать то, что со временем тренды и вкусы клиентов меняются. В погоне за трендом они могут начать покупать больше черный хлеб, или обезжиренное молоко и т.д.*
* Учёт времени: поиск последовательных шаблонов (sequential pattern).
* Учёт информации о клиентах.

## Префиксное FP-дерево (FP - frequent pattern)

 В каждой вершине v дерева T задаются:
 * признак $$f_{v}\in$$F
 * множество дочерних вершин $$S_{v}\subset$$T;
 * поддержка $$c_{v}=\nu (\phi_{v}$$) набора признаков $$\phi_{v}$$={$$f_{u}:y\in[v_{0},v]$$}, где [$$v_{0},v$$] - путь от корня дерева $$v_{0}$$ до вершины v. 
 
 **Обозначения:**
V(T,f) = {$$v\in T: f_{v}=f$$} - множество всех вершин в дереве, которые помечены данным признаком
C(T,f) = $$\sum_{v\in V(T,f)}c_{v}$$ - суммарная поддержка признака по всем вершинам помеченным данным признаком.

Свойства FP-дерева T, построенного по всей выборке $$X^{l}$$:
1) T содержит полную информацию о всех v($$\phi$$), $$\phi\subseteq$$ F,
2) C(T,f)=v(f) для всех f$$\in F$$.
функция C - поддержка данного признака. 

## Алгоритм FP-growth

**Вход**: $$X^{l}$$ - обучающая выборка;
**Выход**: FP-дерево T, $$<f_{v}, c_{v}, S_{v}>_{v\in T}$$
1) упорядочить признаки f$$\in F: v(f)\geq\delta$$ по убыванию v(f);
ЭТАП 1: построение FP-дерева T по выборке $$X^{l}$$
2) **для всех** $$x_{i}\in X^{l}$$
3) $$v:=v_{0}$$;
4) **для всех** $$f\in F$$ таких, что $$f(x_{i})\not = 0$$
5) **если** нет дочерней вершины $$u \in S_{v}:f_{u}=f$$ **то**
6) создать новую вершину u; $$S_{v}:=S_{v}\cup$${u};
    $$f_{u}:=f; c_{u}=0; S_{u}:=\varnothing$$
7) $$c_{u}:=c_{u}+1/l; v:=u;$$
8) ЭТАП 2: рекурсивный поиск частых наборов по FP-дереву T FP-find ($$T, \varnothing, \varnothing$$);


58:23













