# Конспекты курса machine learning

# линейные методы классификации
история открытия обучаемых нейросетей.
В 40-е 50-е гг. прошлого века, нейрофизиологи
проводили множество экспериментов по исследованию
работы нейронов и их обучаемости. Оказывается, большая
часть обучения проходит не в самих нервных клетках а в 
синапсах. Если два нейрона по соседству возбуждаются 
одновременно (т.е. 1 передал другому импульс и угадал),
то у них связь укрепляется. Т.е. при угадывания
нужного импульса клетки награждаются усилением связи с друг другом.

С точки зрения математики это представляет собой оптимизацию
какой-то градиентной функции. 

Что приводит нас к методу градиентного спуска. Метод заключается в том, чтобы
найти для линейной функции активации антиградиент. Градиент - направлен в сторо-
ну максимального возрастания, а антиградиент в сторону максимального убывания.
Поэтому, данный метод относится к методом минимизации. 
w^0 - начальное приближение
w^(t+1) := w^t - eta * grad(Q(w^t)), где grad(Q(w)) = (dQ/dw_j)^n_j =0
eta - градиентный шаг, или темп обучения (learning rate)

w^(t+1) := w^t - eta * sum(L'((w^t,x_i),y_i))x_i * y_i

Ниже приводится псевдокод стохастического градиентного спуска (stochastic Gradient):

Вход:
    выборка X^l; темп обучения eta, параметр lambda;
Выход:
    веса w_0,w_1,...w_n

1: инициализировать веса w_j, j=0,...n
2: инициализировать текущую оценку функционала:
   Q := sum(L((w,x_i)y_i)
3: повторять
4:    выбрать объект x_i из X^l (например, случайно)
5:    вычислить потерю: epsilon_i := L((w,x_i)y1)
6:    градиентный шаг: w:= w - eta * L'((w,x_i)y_i)x_i* y_i;
7:    оценить значения функционала: Q := (1-lambda)Q + lambda* epsilon_i
8: пока значение Q и/или веса w не стабилизируются

# SG: порядок предъявления объектов

Возможны варианты:

1) перетасовка объектов (shuffling):
   попеременно брать объекты из разных классов
2) чаще брать те бъекты, на которых была допущена большая ошибка
  (чем меньше M_i, тем больше вероятность взять объект)
  (чем меньше |M_i|, тем больше вероятность взять объект)
3) вообще не брать "хорошие объекты, у которых M_i > mu. (при этом
немного ускоряется сходимость)

4) вообще не брать объекты-"выбросы" у которых M_i < mu. (при этом
может улучшиться качество классификации)

Параметры mu_+, mu_- придется подбирать

