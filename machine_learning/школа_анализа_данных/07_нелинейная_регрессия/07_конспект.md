# Нелинейная регрессия

Рассмотрим такой случай: 
* X - объекты (часто $$R^{n}$$ - n-мерная матрица); Y - ответы (часто R, реже $$R^{m}$$);
$$X^{l}=(x_{i},y_{i})^{l}_{i=2}$$ - обучающая выборка; (training set)
$$y_{i}=y(x_{i}), y: X\rightarrow Y$$ - неизвестная зависимость;

* a(x) = f(x,$$\alpha$$) - модель зависимости. Какая-то функция, которая описывает зависимость данных Y от X;
$$\alpha \in R^{p}$$ - вектор параметров модели

* МНК:
$$Q(\alpha,X^{l})=\sum_{i=1}^{l}w_{i}(f(x_{i},\alpha)-i_{i})^{2}\rightarrow min_{\alpha}$$

Где $$w_{i}$$ - вес, степень важности i-го объекта. 
$$Q(\alpha^{*}, X^{l})$$ - остаточная сумма квадратов (residual sum of squares, RSS)

В предыдущей лекции мы использовали линейные модели, в этой рассмотрим нелинейную модель регрессии.
Пусть $$f(x,\alpha)$$ - нелинейная функция, тогда к искомому $$\alpha$$ добавится производная второго порядка вида:

$$\alpha^{t+1}=\alpha^{t}-\eta_{t}(Q''(\alpha^{t}))^{-1}Q'(\alpha^{t})$$
$$Q'(\alpha^{t})$$ - производная 1-го порядка, градиент функционала Q в точке $$\alpha^{t}$$. Представляет собой вектор
$$Q''(\alpha^{t})$$ - матрица частных производных размера t x t. или же гессиан (матрица Гессе) функционала Q в точке  $$\alpha^{t}$$
$$\eta_{t}$$ - величина шага (можно полагать $$\eta_{t}$$ = 1)

Для упрощения такого подхода будет использовать Метод Ньютона-Рафсона. 

Для нахождения градиента, мы дифференцируем 1 раз:
$$\frac{\partial{Q(\alpha)}}{\partial{\alpha_{j}}}=2\sum_{i=1}^{l}(f(x_{i},\alpha)-y_{i})\frac{\partial{f(x_{i},\alpha)}}{\partial{\alpha_{j}}}$$

Для нахождения гессиана, мы дифференцируем во 2-й раз:
$$\frac{\partial^{2}{Q(\alpha)}}{\partial{\alpha_{j}}\partial{\alpha_{k}}}=2\sum_{i=1}^{l}{\frac{\partial{f(x_{i},\alpha)}}{\partial{\alpha_{j}}}}\frac{\partial{f(x_{i},\alpha)}}{\partial{\alpha_{k}}}-2\sum_{i=1}^{l}(f(x_{i},\alpha)-y_{i})\frac{\partial^{2}{f(x_{i},\alpha)}}{\partial{\alpha_{j}}\partial{\alpha_{k}}}$$

При линеаризации полагается, что $$2\sum_{i=1}^{l}(f(x_{i},\alpha)-y_{i})\frac{\partial^{2}{f(x_{i},\alpha)}}{\partial{\alpha_{j}}\partial{\alpha_{k}}} = 0$$

Если допустить, что в некоторой области, в окрестности текущего $$\alpha^{t}$$ функция $$f(x,\alpha)$$ линейна, то вторая часть (см. выше) равняется 0. Т.к. линейная функция при двойной дифференциации зануляется. 
Такой прием часто используется в физике. Если мы имеем дело, с какой то сложной моделью и хотим что-то аналитически сделать с ней, то можем считать что в определенной области эта часть равна 0. 

$$f(x_{i},\alpha)=f(x_{i},\alpha^{t}) + \sum^{p}_{j=1}\frac{\partial{f(x_{i},\alpha_{j})}}{\partial{\alpha_{j}}}(\alpha_{j}-\alpha^{t}_{j})+o(\alpha_{j}-\alpha^{t}_{j})$$

Если исключить второй член той формулы (как следствие операции линеаризации), то мы получим метод Ньютона-Гаусса. По которой пишем:

$$F_{t} = \left( \frac{\partial{f}}{\partial{\alpha_{j}}} (x_{i},\alpha^{t})\right)^{j=1,p}_{i=1,l}$$ - l x p - матрица первых производных;

введем обозначение $$f_{t}$$, который будет равен 
$$f_{t} = (f(x_{i},\alpha^{t}))_{i=1,l}$$ - вектор значений f

Формула t-й итерации метода Ньютона-Гаусса:
$$\alpha^{t+1}:=a^{t}-h_{t}(F_{t}^{t}F_{t})^{-1}F^{t}_{t}(f^{t}-y)$$
$$\beta = (F_{t}^{t}F_{t})^{-1}F^{t}_{t}(f^{t}-y)$$

$$\beta$$ - это решение задачи многомерной линейной регрессии

$$||F_{t}\beta-(f^{t}-y)||^{2}\rightarrow min_{\beta}$$
Нелинейная регрессия сведена к серии линейных регрессий.
Скорость сходимости - как и у метода Ньютона-Рафсона, но для вычислений можно применять стандартные методы. 

Решение нелинейной задачи, может быть сведено к последовательности линейных задач. 

## Задача классификации. Логистическая регрессия

Пусть имеется два класса Y = {-1,+1}, а функция активации a(x,w) = sign($$w^{T}x$$), $$w \in R$$

Функционал аппроксимированного эмпирического риска:

$$Q(w) =\sum^{l}_{i=1}[M_{i}(w)<0]\leq\sum^{l}_{i=1}L(w^{T}x_{i}y_{i})\rightarrow min_{w}$$, где
$$M_{i}(w)$$ - число ошибок или отступ (маржа), заменяется его ограничением сверху, которая зависит от w - веса и минимизируется. Данную функци потерь можно заменить на любую функцию, которая будет удовлетворять условиям. В данном случае это логарифмическая
функция.

L(M) = $$\log{(1+e^{-M})}$$ - логарифмическая функция потерь

Чтобы свести данную задачу к многомерному линейной задаче мы воспльзуемся методом
Ньютона-Рафсона для минимизации функционала Q(w). Для этого рассмотрим вес для итерации t+1:

$$w^{t+1}:=w^{t}-h_{t}(Q''(w^{t}))^{-1}Q'(w^{t})$$

Распишем градиент (вектора первых производных) $$Q'(w^{t})$$:

$$\frac{\partial{Q(w)}}{\partial{w_{j}}}=-\sum^{l}_{i=1}(1-\sigma_{i})y_{i}f_{j}(x_{i}), j=1,...,n.$$

Распишем гессиан (матрица вторых производных) $$Q''(w^{t})$$:

$$\frac{\partial^{2}{Q(w)}}{\partial{w_{j}}\partial{w_{k}}}=\sum^{l}_{i=1}(1-\sigma_{i})\sigma_{i}f_{j}(x_{i})f_{k}(x_{i}),    j,k=1,...,n.$$

где $$\sigma_{i}=\sigma(y_{i}<w^{t}x_{i}>), \sigma(z)=\frac{1}{1+e^{-z}}$$ - сигмоидная функция. $$\sigma$$ - это результат применения сигмоидной функции к $$<w^{t}x_{i}>$$ - скалярному произведению.  
$$\sigma_{i}=P(y_{i}|x_{i})$$ - вероятность правильной классификации $$x_{i}$$ 

Давайте внесем следующие обозначения:
$$F_{l\times n}=\left( (f_{j}(x{i}))\right)$$ - матрица "объекты-признка";
$$\Gamma_{l\times l}=diag(\sqrt{(1-\sigma_{i})\sigma_{i})}$$ - диагоальная матрица;
$$\tilde{F} = \Gamma F$$ - взвешенная матрица "объекты-признаки";
$$\tilde{y}_{i}=y_{i}\sqrt{(1-\sigma_{i})/\sigma{i}}, \tilde{y}=(\tilde{y}_{i})^{l}_{i=1}$$ - взвешенный вектор ответов. 

Заменим этими обозначениями переменные в методе Ньютона-Рафсона и получим:

$$\left( Q''(w)\right)^{-1}Q'(w)=-(F^{T}\Gamma^{2}F)^{-1}F^{T}\Gamma\tilde{y}=-(\tilde{F}^{T}\tilde{F})^{-1}\tilde{F}^{T}\tilde{y}=-\tilde{F}^{+}\tilde{y}$$

Это выражение совпадает с МНК-решением линейно задачи регрессии со взвешенными объектами и модифицированными ответами. 

$$Q(w)=||\tilde{F}w-\tilde{y}||^{2}=\sum^{l}_{i=1}(1-\sigma_{i})\sigma_{i}\left(w^{T}x-\frac{y_{i}}{\sigma_{i}} \right)^{2}\rightarrow min_{w}$$


## Формула Надарая-Ватсона

Несмотря на удобство параметрических функций, их основная проблема в том, что метод сильно зависит от наличия функции, линейной или нелинейной, которая бы описывала ту или иную зависимость. К тому же всегда есть вероятность, что будет досатточно сложный случай, который параметрической функцией не опишешь.  

Поэтому мы подходим к рассмотрению не параметрических функций, хотя надо иметь в виду, что не параметрическими они называются с натяжкой, потому что все равно имеют какие-то параметры. 

Будем искать некую константу, которая бы смещала бы объекты так, чтобы приблизить к одной единственной точке. Вот так мы будем обучать модель, для каждой точки отдельно. 

приближение константой a(x) = $$\alpha$$ в окрестностях $$x\in X$$:
$$Q(\alpha;x^{l})=\sum^{l}_{i=1}w_{i}(x)(\alpha-y_{i})^{2}\rightarrow min_{\alpha\in \mathcal{R}}$$

где $$w_{i}(x) = K\left( \frac{\rho(x,x_{i}}{h})\right)$$ - веса объектов $$x_{i}$$ относительно x; 

В формуле рассматривается вес w_i(x) а не x_i, т.к. мы рассматриваем только одну точку, для нахождения константы для этой точки. Другие точки вносят свой вклад, но имеют вес, который тем больше, чем ближе они находятся к нашей точке. Т.к. чем дальше точки, тем меньший вес они имеют мы вводим функцию K(r), которая представляет собой некую, монотонно-убывающую функцию. 
Еще один коэффициент это h - ширина окна сглаживания. Эта величина определяет размеры или радиус, в пределах которого точки x_i имеют больший вес, а за пределами значительно ниже вес. 

Формула сглаживания представляет собой простую средневзвешенную y:

$$a_{h}(x;X^{l})=\frac{\sum^{l}_{i=1}{y_{i}w_{i}(x)}}{\sum^{l}_{i=1}{w_{i}(x)}}=\frac{\sum^{l}_{i=1}{y_{i}K\left(\frac{\rho(x,x_{i})}{h} \right)}}{\sum^{l}_{i=1}{K\left(\frac{\rho(x,x_{i})}{h} \right)}}$$

Если $$h\rightarrow \infty$$, то мы получим среднюю, константу. Получится, все весы одинаково значимы, либо их нет. Модель получится со слишком большим смещением (**bias**). Если ж $$h\rightarrow 0$$, то модель будет слишком сильно следовать за точками. Т.е. у некоторых весов будет слишком большое значение, что ведет к **оверфиту**.   

Для подбора оптимального значения h, нужно руководствоваться следующими правилами:
* При неравномерной сетке (напр., для малых значений по оси OX много данных, для больших значений по оси OX мало, что является часто встречающейся проблемой в экспериментальной физике) используется переменная ширина (чтобы было определенное число соседей, равных числу k)
$$w_{i}(x)=K\left( \frac{\rho(x,x_{i}}{h(x)}\right)$$, 
где $$h(x)=\rho(x,x^(k+1)), x^{k+1}$$ - k-й сосед объекта x.
* Оптимизация ширины по скользящему котролю (Leave-one-out [LOO]), метод заключается в том, чтобы минимизировать ошибки, т.е. если какая-то точка очень сильно выбивается из общей картины, она игнорируется: 
$$LOO(h,X^{l})=\sum^{l}_{i=1}\left( a_{h}(x_{i};X^{l}/{x_{i}})\right)\rightarrow min_{h}$$

## квантильная регрессия
Квантильная регрессия:
$$L(\varepsilon) = \{ C_{+}| \varepsilon |, \varepsilon>0$$
$$L(\varepsilon) = \{ C_{-}| \varepsilon |, \varepsilon<0$$ 
Основная идея квантильной регрессии в том, что функция потерь будет выглядеть по разному, взависимости от значения $$\varepsilon = a_{i}-y_{i}$$, если наша модель переоценивает (a>y), то у нас один вид ошибок, а если недооценивает то другой вид. Обычно бывает, что при недооценке потери выше, а переоценке потери больше, но все равно
цель уменьшить ошибки:

$$\sum^{l}_{i=1}L(a-y_{i})\rightarrow min_{a} a=y^{(q)}, q=\frac{lC_{-}}{C_{-}+C_{+}}$$

q - это показатель того, выгодно ли нам переоценить или недооценить, т.к. функции описывающие поведение модели при недооценке и переонцке отличаются. 

Пример: закупки определенного товара для дальнейше продажи. Если мы предскажем рынок неправильно и закупим недостаточное число, то мы упустим выгоду и потеряем деньги в конечном счете, если закупим слишком много, то не сможем продать весь товар и он будет пылиться на складе. 

Эту модель можно свести к задаче линейного программирования путем замены переменных:
$$\varepsilon_{i}^{+}=(a(x_{i}-y_{i}))_{+}$$ и $$\varepsilon_{i}^{-} = (y_{i}-a(x_{i}))_{+}$$. При этом надо иметь в виду, что если одно из $$\varepsilon$$ не равно 0, то другое равно 0, т.е. в каждом случае может быть либо переоценка, либо недооценка. 

$$Q=\sum^{l}_{i=1} C_{+}\varepsilon_{i}^{+}+C_{-}\varepsilon_{i}^{-}\rightarrow min_{w}$$
$$<x_{i},w>-y_{i}=\varepsilon_{i}^{+}+\varepsilon_{i}^{-}$$
$$\varepsilon_{i}^{+}\geq 0; \varepsilon_{i}^{-}\geq 0$$

## SVM регрессия 
(см. рис SVM_reg)


