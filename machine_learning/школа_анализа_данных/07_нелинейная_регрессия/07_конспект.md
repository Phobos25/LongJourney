# Нелинейная регрессия

Рассмотрим такой случай: 
* X - объекты (часто $$R^{n}$$ - n-мерная матрица); Y - ответы (часто R, реже $$R^{m}$$);
$$X^{l}=(x_{i},y_{i})^{l}_{i=2}$$ - обучающая выборка; (training set)
$$y_{i}=y(x_{i}), y: X\rightarrow Y$$ - неизвестная зависимость;

* a(x) = f(x,$$\alpha$$) - модель зависимости. Какая-то функция, которая описывает зависимость данных Y от X;
$$\alpha \in R^{p}$$ - вектор параметров модели

* МНК:
$$Q(\alpha,X^{l})=\sum_{i=1}^{l}w_{i}(f(x_{i},\alpha)-i_{i})^{2}\rightarrow min_{\alpha}$$

Где $$w_{i}$$ - вес, степень важности i-го объекта. 
$$Q(\alpha^{*}, X^{l})$$ - остаточная сумма квадратов (residual sum of squares, RSS)

В предыдущей лекции мы использовали линейные модели, в этой рассмотрим нелинейную модель регрессии.
Пусть $$f(x,\alpha)$$ - нелинейная функция, тогда к искомому $$\alpha$$ добавится производная второго порядка вида:

$$\alpha^{t+1}=\alpha^{t}-\eta_{t}(Q''(\alpha^{t}))^{-1}Q'(\alpha^{t})$$
$$Q'(\alpha^{t})$$ - производная 1-го порядка, градиент функционала Q в точке $$\alpha^{t}$$. Представляет собой вектор
$$Q''(\alpha^{t})$$ - матрица частных производных размера t x t. или же гессиан (матрица Гессе) функционала Q в точке  $$\alpha^{t}$$
$$\eta_{t}$$ - величина шага (можно полагать $$\eta_{t}$$ = 1)

Для упрощения такого подхода будет использовать Метод Ньютона-Рафсона. 

Для нахождения градиента, мы дифференцируем 1 раз:
$$\frac{\partial{Q(\alpha)}}{\partial{\alpha_{j}}}=2\sum_{i=1}^{l}(f(x_{i},\alpha)-y_{i})\frac{\partial{f(x_{i},\alpha)}}{\partial{\alpha_{j}}}$$

Для нахождения гессиана, мы дифференцируем во 2-й раз:
$$\frac{\partial^{2}{Q(\alpha)}}{\partial{\alpha_{j}}\partial{\alpha_{k}}}=2\sum_{i=1}^{l}{\frac{\partial{f(x_{i},\alpha)}}{\partial{\alpha_{j}}}}\frac{\partial{f(x_{i},\alpha)}}{\partial{\alpha_{k}}}-2\sum_{i=1}^{l}(f(x_{i},\alpha)-y_{i})\frac{\partial^{2}{f(x_{i},\alpha)}}{\partial{\alpha_{j}}\partial{\alpha_{k}}}$$

При линеаризации полагается, что $$2\sum_{i=1}^{l}(f(x_{i},\alpha)-y_{i})\frac{\partial^{2}{f(x_{i},\alpha)}}{\partial{\alpha_{j}}\partial{\alpha_{k}}} = 0$$

Если допустить, что в некоторой области, в окрестности текущего $$\alpha^{t}$$ функция $$f(x,\alpha)$$ линейна, то вторая часть (см. выше) равняется 0. Т.к. линейная функция при двойной дифференциации зануляется. 
Такой прием часто используется в физике. Если мы имеем дело, с какой то сложной моделью и хотим что-то аналитически сделать с ней, то можем считать что в определенной области эта часть равна 0. 

$$f(x_{i},\alpha)=f(x_{i},\alpha^{t}) + \sum^{p}_{j=1}\frac{\partial{f(x_{i},\alpha_{j})}}{\partial{\alpha_{j}}}(\alpha_{j}-\alpha^{t}_{j})+o(\alpha_{j}-\alpha^{t}_{j})$$















