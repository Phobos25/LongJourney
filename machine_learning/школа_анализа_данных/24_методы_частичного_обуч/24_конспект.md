# Методы частичного обучения (semi-supervised learning)

Этот метод занимает промежуточное состояние между кластеризацией и классификацией. 

Когда у нас есть объекты X и ответы Y --- это supervised learning (обучение с учителем). Самые распространенные --- это методы классификации и регрессии. 
Когда у нас есть только объекты X --- это unsupervised learning (обучение без учителя). Самые распространенные --- это методы класстеризации, поиск аномалий, нейросети и поиск латентных моделей. 

# постановка задачи

Дано:
множество объектов X, множество классов Y;

$$X^l = \{ x_1,..., x_l\}$$ --- размеченная выборка (labeled data);
$$\{y_1,...,y_l\}$$. *Мы можем построить алгоритм классификации и быть готовыми, любой новый объект классифицировать. Не обязательно тот, который не размеченный (находится в неразмеченной выборке), а вообще любой*
$$X^k={x_{l+1},...,x_{l+k}}$$ --- неразмеченная выборка (unlabeled data). 

**Два варианта постановки задачи:**

* Частичное обучение (semi-supervised learning): построить алгоритм классификации a:X $$\rightarrow$$Y.
* Трансдуктивное обучение (transductive learning): зная **все** $$\{ x_{l+1},...,x_{l+k}\}$$, получить метки $$\{y_{l+1},...,y_{l+k} \}$$. *Надо получить метки только для неразмеченных объектах. Мы  считаем, что ничего кроме них нет, и больше не будет.*

Типичные приложения:
классификация и каталогизация текстов, изображений, и т.п.

Математически кажется, что оба подхода одинаковы. В обоих случаю есть выборка объектов, и надо найти к ним ответы. Но, при реализации, методы отличаются друг от друга. 
Самое важное здесь то, что мы должны воспользоваться внутренней кластерной системой неразмеченных данных, для того, чтобы точнее классифицировать. Специфика данной задачи в том, что размеченных данных намного меньше неразмеченной. 

Содержание: методы кластеризации с частичным обучением

* 1) Простые эвристические методы. Особенности задачи SSL. Метод self-training. Композиции алгоритмов классификации. 
* 2) Модификации методов кластеризации. Оптимизационный подход. Кластеризация с ограничениями. 
* 3) Модификации методов классификации. Трансдуктивный SVM. Логистическая регрессия. Expectation Regularization. 

Сначала, познакомимся с простейшими примерами, которые приходят в голову. Затем, посмотрим, как приспособить к методу частичного обучения методы кластеризации --- что делается очень просто. И наконец, посмотрим, как методы классификации приспосабливаются --- в этом случае, сложнее. 

Представим, что у нас есть задача классификации --- две плотности. Но из них нам дали крайне мало точек (размеченых точек, крайне мало) --- дали 5 представителей в одном классе и в другом.  (см SSL_classif). Видя рисунок, можно утверждать, что используя метод кластеризации, мы бы получили более правильные распределения. Отсюда вывод --- надо сначала сделать кластеризацию, а потом по размеченным точкам сделать вывод, где какой класс. 

Если у нас есть какой-то классификационный метод, который способен обучаться по размеченной выборке. Пусть мы используем метод ближайших соседей, и посмотрим, с какой надежностью этот метод относит объекты к [двум] классам. Затем, точки которые с большой надежностью относятся к классам мы добавим в нашу выборку и получим расширенную версию, по которой снова сделаем классификацию. В идеале, два банана должны будут покраситься как и задумано. 

Пусть $$\mu:X^l\rightarrow a$$ --- произвольный метод обучения; классификаторы имеют вид a(x) = $$arg max_{y \in Y}\Gamma_y(x)$$;

"Отступ" объекта --- степень уверенности классификации $$a_i=a(x_i)$$:

$$M_i(a) = \Gamma_{a_i}(x_i) - max_{y\in Y/a_i}\Gamma_y(x_i)$$

 Алгоритм self-training --- обёртка (wrapper) над методом $$\mu$$:
 
 1. Z:=$$X^l$$;
 2. **пока** |Z|<l+k
 3. a:=$$\mu(Z)$$;
 4. $$\Delta$$:=$$\{ x_i\in X^k/Z| M_i(a)\geq M_o\}$$;
 5. $$y_i:=a(x_i)$$ для всех $$x_i\in \Delta$$;
 6. $$Z := Z \cup \Delta$$

$$M_0$$ можно определять, например, из условия $$|\Delta|$$ = 0.05 k

Метод co-training --- это когда два разных метода обучают друг друга. Методы разные, например, решающее дерево и линейный классификатор, либо давать им разные выборки для обучения. 

Пусть $$\mu_1: X^l\rightarrow a_1, \mu_2: X^l \rightarrow a_2$$ --- два существенно различных метода обучения, использующих
--- либо разные наборы признаков;
--- либо разные парадигмы обучения (inductive bias);
--- либо разные источники данных $$X^{l_1}_1, X^{l_2}_2$$

Обучаем один алгоритм, с его помощью классифицируем объекты, доразмечаем их и на этой доразмеченной выборке дообучаем второй алгоритм. Так, по очереди обучает два наших алгоритма. 

Следующий метод co-learning (deSa, 1993)

По сути, является первым методом, но вместо одного алгоритма, мы будем использовать целую группу алгоритмов. В результате мы получим композицию, и будем размечать объекты простым голосованием

Пусть $$\mu_1: X^l\rightarrow a_t$$ --- разные методы обучения, t=1,...,T

**Алгоритм co-learning** --- это self-training для композиции --- простого голосования базовых алгоритмов $$a_1,...,a_T$$:

$$a(x)=argmax_{y\in Y}\Gamma_y(x)$$, $$\Gamma_y(x_i) = \sum^{T}_{t=1}[a_t(x_i)=y]$$

тогда $$M_i(a)$$ --- степень уверенности классификации $$a(x_i)$$

**Графовый метод кластеризации:**
Графовый алгоритм КПН (кратчайший незамкнутый путь)

1. Найти пару вершин $$(x_i,y_j)\in X^{l+k}$$ с наименьшим $$p(x_i,y_i)$$ и соединить их ребром;
2. пока в выборке остаются изолированные точки
3. Найти изолированную точку,
    ближайшую к некоторой неизолированной;
4. соединить эти две точки ребром;
5. удалить K-1 самых длинных ребер;

Всегда находит пару ближайших вершин и соединяет ребром, а потом находит пару вершин (одна соединенная, другая не соединенная) и соединяет. И так, последовательно, соединяет все точки. Если у нас было l+k точек, то ребер должно быть l+k-1. 

Данный алгоритм не применяют на практике, т.к. у него очень много недостатков, в числе которых чувствительность к шуму --- выбросы очень сильно влияют на дерево, которое строит данный алгоритм. 

Этот алгоритм нам интересен потому, что мы можем на его примере посмотреть как кластеризатор может быть применен для частичного обучения:

1. Найти пару вершин $$(x_i,y_j)\in X^{l+k}$$ с наименьшим $$p(x_i,y_i)$$ и соединить их ребром;
2. пока в выборке остаются изолированные точки
3. Найти изолированную точку,
    ближайшую к некоторой неизолированной;
4. соединить эти две точки ребром;
5. $$\sout{\text{удалить K-1 самых длинных ребер;}}$$
6. $$\color{red} {\text{пока есть путь между двумя вершинами разных классов}}$$ 
7. $$\color{red}{\text{удалить самое длинное ребро на этом пути.}}$$

# Метод k-средних: кластеризация

1. начальное приближение центров $$\mu_y, y\in Y$$;
2. повторять
3. E-шаг:
    отнести каждый $$x_i$$ к ближайшему центру;
    $$y_i := argmin_{y\in Y}\rho(x_i,\mu_y), i=1,...,l+k$$
4. М-шаг:
    вычислить новые положения центров;
    $$\mu_j := \frac{\sum^{l+k}_{i=1}[y_i = y]x_i}{\sum^{l+k}_{i=1}[y_i=y]}, \text{для всех } y\in Y$$
5. пока $$y_i$$ не перестанут изменяться;

Идея --- итерационно повторяется два шага: первый шаг --- относит каждый объект к ближайшему центру кластера; второй шаг --- находит новый центр масс для каждого кластера. 
Это была unsupervised кластеризация

Что изменится в случае semi supervised:

1. начальное приближение центров $$\mu_y, y\in Y$$;
2. повторять
3. E-шаг:
    отнести каждый $$x_i\color{red} \in X^k$$ к ближайшему центру;
    $$y_i := argmin_{y\in Y}\rho(x_i,\mu_y)$$, i=$$\color{red} {l+1}$$,...,l+k
4. М-шаг:
    вычислить новые положения центров;
    $$\mu_j := \frac{\sum^{l+k}_{i=1}[y_i = y]x_i}{\sum^{l+k}_{i=1}[y_i=y]}, \text{для всех } y\in Y$$
5. пока $$y_i$$ не перестанут изменяться;

Если раньше мы **каждый** объект относили к ближайшему центру, то теперь будем относить только **неразмеченные** объекты.В коде, меняется только переменная цикла, если раньше цикл мы делали по всем объектам, то теперь только по неразмеченным.

# Логистическая регрессия: классификация на 2 класса

Линейный классификатор на два класса Y = {-1;+1}
$$a(x) = sign<w,x>, x,w \in R^n$$

Вероятность того, что объект $$x_i$$ относится к классу y:
$$P(y|x_i,w) = \frac{1}{1+exp(-<w,x_i>y)}$$

Задача максимизации регуляризованного правдоподобия:
$$Q(w) = \sum^{l}_{i=1}\log{P}(y_i|x_i,w) - \frac{1}{2C}||w^2||\rightarrow max_{w}$$

# Логистическая регрессия: частичное обучение

Теперь учтем неразмеченные данные $$X^k = \{ x_{l+1},..., x_{l+k}\}$$
*Будем считать что у нас есть признаки $$b_j(x)$$, которые мы умеем считать как на размеченных, так и неразмеченных данных. И будем читать что эти признаки бинарны*
Пусть $$b_j(x)$$ --- бинарные признаки, j=1,...,m

*Вероятность класса, при условии, что j-й индекс равен 1*
Оценим вероятности $$P(y|b_j(x) = 1)$$ двумя способами:

1) эмпирическая оценка по размеченным данным $$X^l$$:
$$p_j(y) = \frac{\sum^{l}_{i=1}b_j(x_i)[y_i = y]}{\sum^{l}_{i=1}b_j(x)}$$
*В знаметеле стоит число объектов, в которых данный признак b = 1.
А в числителе стоят те, у которых и классы равны y. Т.е. получается, что это частотная оценка*

2) оценка по неразмеченным данным $$X^k$$ и линейной модели w:
$$p_j(y,w) = \frac{\sum^{l+k}_{i=l+1}b_j(x_i)P(y|x_i,w)}{\sum^{l+k}_{i=l+1}b_j(x_i)}$$
*Если мы по размеченным данным построим линейную модель классификации, например при помощи логистической регрессии. То мы сможем с его помощи классифицировать любые объекты. Давайте применим эту оценку к неразмеченному объекту.*

Для второго способа, мы по размеченным данным строим вероятностную модель классификации $$P(y|x_i,w)$$ и выписываем фактически эмпирическую оценку условной вероятности класса y при условии, что j-й признак равен 1. Но, это другой способ, он зависит от модели классификации размеченных данных. И теперь, эта оценка является функцией от параметра модели w. Если мы поставим задачу согласовать оценку по второму методу с эмпирической оценкой по первому методу, мы достигнем того, что, теперь, мы можем настраивать параметр w не только по размеченной обучающей выборке, но и по не размеченной. 
Таким образом, мы можем настраивать параметр модели не по малому числу размеченных данных, а по всей выборке. 

Теперь, нам надо как-то согласовать условные вероятности, т.е. $$p_j(y,w)$$ должна быть похожа на $$p_j(w)$$. Это значит, нам надо научиться мерять расстояние между распределениями. 

Будем минимизировать расстояние между $$p_j(y)$$ и $$p_j(y,w)$$

**Ликбез по оценке расстояния между двумя распределениями** 
![рис.] (dist_betw_distr.png)

Т.к. распределения у нас дискретные никаких интегралов писать не придется. Проблема: бывают очень длинные "хвосты" у распределений, с очень маленькими вероятностями. В таком случае, надо заранее озаботиться, как их сравнивать, т.к. "хвосты" могут приводить к существенным различиям. 
Пример:
вероятности 0.50 и 0.51 --- разница в 0.01 совсем маленькая
вероятности 0.01 и 0.02 --- разница в те же 0.01, но отличаются друг от друга двойной частотой появления. 
Поэтому, надо брать такие оценки, которые не просто берут разность, но и учитывают такие малые вероятности. Т.е. у малых вероятностей должен быть больший вес. 

Можно использовать статистику хи-квадрат, он достаточно хорошо оценивает. Либо использовать расстояние Хелингера, который в отличие от метода хи-квадрат не имеет знаменателя. Знаменатель может приводить либо к неустойчивости, если значение в знаменателе слишком маленькое, либо приводить к неопределенности, если значение в знаменателе равно нулю. 

Дивергенция Кульбака-Лейблера (см. KL_div). Метод хорош тем, что это не явное применение метода максимума правдоподобия. Надо иметь в виду, что на первом месте должно стоять эмпирическое распределение, на втором модель, которая зависит от параметров. Тогда эта модель окажется под логарифмом, и все пойдет как положено в методе максимума правдоподобия. 

# Построение функционала качества

Кросс-энтропия --- мера согласованности двух оценок
$$p_j(y)$$ и $$p_j(y,w)$$, одной и той же вероятности $$P(y|b_j(x)=1)$$:

$$H_j(w) = \sum_{y\in Y}p_{j}(y)\log{p_{j}(y,w)}\rightarrow max_{w}$$

*Здесь, кросс-энтропия это то же самое, что и минимизация дивергенции Кульбака-Лейблера.*
(максимум достигается при $$p_j(y) \equiv p_j(y,w))$$

Добавим суммарную согласованность по всем m признакам к функционалу регуляризованного правдоподобия: 

$$Q(w) = \sum^{l}_{i=1}\log{P(y_i|x_i,w)} - \frac{1}{2C}\sum_{y\in Y} ||w_{y}||^2 +$$
$$+ \lambda\sum^{m}_{j=1}\sum_{y\in Y}p_{j}(y)\log{\frac{\sum^{l+k}_{i=l+1}b_j(x_i)P(y|x_i,w)}{\sum^{l+k}_{i=l+1}b_j(x_i)}}\rightarrow max_{w}$$

1-й член --- это сумма по длине l (имеется в виду, что их мало, поэтому l маленькое). Поэтому, если бы мы настраивали только логистической регрессией эту штуковину, то, скорее всего, размеченных объектов было бы мало, а размерность w была бы большой.  
2-й член --- регуляризатор, как обычно для многоклассовой задачи. 
3-й член --- та самая функция согласованности. Это сумма кросс-энтропий. $$\lambda$$ --- новый коэффициент регуляризации. Этот коэффициент позволяет нам балансировать два наших требования: с одной стороны мы строим логистическую регрессию; с другой стороны регрессия должна давать похожие (в идеале такие же) результаты, условные вероятности, на имеющихся признаках, как и на размеченных данных. Это дополнительное условие согласованности, которое ограничивает наши параметры w и не дает им разбежаться далеко и давать неадекватные оценки условной вероятности. 

Такой метод впервые был описан в статье *Mann, McCallum. Simple, robust, scalable semi-supervised learning via expectation regularization. ICML. 2007.*

Для начала напишем мотивацию данного метода:
Наши бинарные признаки $$b_j(x)$$ могут быть словами. x --- тексты, $$b_j$$ --- есть в тексте это слово или нет. Если у нас небольшое число классифицированных текстов l и огромное число неклассфицированных текстов из интернета, слов очень много (m - число признаков-слов в словаре). То, что мы делаем, используя отдельные частоты, это очень похоже на наивный байесовский классификатор, который неплохо работает для текстовых задач. 

# Замечания про метод XR (Expectation Regularization)
1. Оптимизация Q(w) --- методом стохастического градиента. *Метод стохастического градиента хорошо работает на больших данных*
2. Возможные варианты задания переменных $$b_j$$:
* $$b_j(x)\equiv 1$$, тогда $$P(y|b_j(x)=1)$$ --- априорная вероятность класса y (label regularization) --- хорошо подходит для задач с несбалансированными классами;
* $$b_j(x)$$ = [термин j содержится в тексте x] --- для задач классификации и каталогизации текстов. 
3. XR слабо чувствителе к выбору C и $$\gamma$$
4. XR очень устойчив к погрешностям оценивания $$p_j(y)$$
5. XR не требователе к числу размеченных объектов l
6. XR хорошо подходит для категоризации текстов.
7. XR показывает в экспериментах очень высокую точность. 

