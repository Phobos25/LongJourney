# Методы частичного обучения (semi-supervised learning)

Этот метод занимает промежуточное состояние между кластеризацией и классификацией. 

Когда у нас есть объекты X и ответы Y --- это supervised learning (обучение с учителем). Самые распространенные --- это методы классификации и регрессии. 
Когда у нас есть только объекты X --- это unsupervised learning (обучение без учителя). Самые распространенные --- это методы класстеризации, поиск аномалий, нейросети и поиск латентных моделей. 

# постановка задачи

Дано:
множество объектов X, множество классов Y;

$$X^l = \{ x_1,..., x_l\}$$ --- размеченная выборка (labeled data);
$$\{y_1,...,y_l\}$$. *Мы можем построить алгоритм классификации и быть готовыми, любой новый объект классифицировать. Не обязательно тот, который не размеченный (находится в неразмеченной выборке), а вообще любой*
$$X^k={x_{l+1},...,x_{l+k}}$$ --- неразмеченная выборка (unlabeled data). 

**Два варианта постановки задачи:**

* Частичное обучение (semi-supervised learning): построить алгоритм классификации a:X $$\rightarrow$$Y.
* Трансдуктивное обучение (transductive learning): зная **все** $$\{ x_{l+1},...,x_{l+k}\}$$, получить метки $$\{y_{l+1},...,y_{l+k} \}$$. *Надо получить метки только для неразмеченных объектах. Мы  считаем, что ничего кроме них нет, и больше не будет.*

Типичные приложения:
классификация и каталогизация текстов, изображений, и т.п.

Математически кажется, что оба подхода одинаковы. В обоих случаю есть выборка объектов, и надо найти к ним ответы. Но, при реализации, методы отличаются друг от друга. 
Самое важное здесь то, что мы должны воспользоваться внутренней кластерной системой неразмеченных данных, для того, чтобы точнее классифицировать. Специфика данной задачи в том, что размеченных данных намного меньше неразмеченной. 

Содержание: методы кластеризации с частичным обучением

* 1) Простые эвристические методы. Особенности задачи SSL. Метод self-training. Композиции алгоритмов классификации. 
* 2) Модификации методов кластеризации. Оптимизационный подход. Кластеризация с ограничениями. 
* 3) Модификации методов классификации. Трансдуктивный SVM. Логистическая регрессия. Expectation Regularization. 

Сначала, познакомимся с простейшими примерами, которые приходят в голову. Затем, посмотрим, как приспособить к методу частичного обучения методы кластеризации --- что делается очень просто. И наконец, посмотрим, как методы классификации приспосабливаются --- в этом случае, сложнее. 

Представим, что у нас есть задача классификации --- две плотности. Но из них нам дали крайне мало точек (размеченых точек, крайне мало) --- дали 5 представителей в одном классе и в другом.  (см SSL_classif). Видя рисунок, можно утверждать, что используя метод кластеризации, мы бы получили более правильные распределения. Отсюда вывод --- надо сначала сделать кластеризацию, а потом по размеченным точкам сделать вывод, где какой класс. 

Если у нас есть какой-то классификационный метод, который способен обучаться по размеченной выборке. Пусть мы используем метод ближайших соседей, и посмотрим, с какой надежностью этот метод относит объекты к [двум] классам. Затем, точки которые с большой надежностью относятся к классам мы добавим в нашу выборку и получим расширенную версию, по которой снова сделаем классификацию. В идеале, два банана должны будут покраситься как и задумано. 

Пусть $$\mu:X^l\rightarrow a$$ --- произвольный метод обучения; классификаторы имеют вид a(x) = $$arg max_{y \in Y}\Gamma_y(x)$$;

"Отступ" объекта --- степень уверенности классификации $$a_i=a(x_i)$$:

$$M_i(a) = \Gamma_{a_i}(x_i) - max_{y\in Y/a_i}\Gamma_y(x_i)$$

 Алгоритм self-training --- обёртка (wrapper) над методом $$\mu$$:
 
 1. Z:=$$X^l$$;
 2. **пока** |Z|<l+k
 3. a:=$$\mu(Z)$$;
 4. $$\Delta$$:=$$\{ x_i\in X^k/Z| M_i(a)\geq M_o\}$$;
 5. $$y_i:=a(x_i)$$ для всех $$x_i\in \Delta$$;
 6. $$Z := Z \cup \Delta$$

$$M_0$$ можно определять, например, из условия $$|\Delta|$$ = 0.05 k

Метод co-training --- это когда два разных метода обучают друг друга. Методы разные, например, решающее дерево и линейный классификатор, либо давать им разные выборки для обучения. 

Пусть $$\mu_1: X^l\rightarrow a_1, \mu_2: X^l \rightarrow a_2$$ --- два существенно различных метода обучения, использующих
--- либо разные наборы признаков;
--- либо разные парадигмы обучения (inductive bias);
--- либо разные источники данных $$X^{l_1}_1, X^{l_2}_2$$

Обучаем один алгоритм, с его помощью классифицируем объекты, доразмечаем их и на этой доразмеченной выборке дообучаем второй алгоритм. Так, по очереди обучает два наших алгоритма. 

Следующий метод co-learning (deSa, 1993)

По сути, является первым методом, но вместо одного алгоритма, мы будем использовать целую группу алгоритмов. В результате мы получим композицию, и будем размечать объекты простым голосованием

Пусть $$\mu_1: X^l\rightarrow a_t$$ --- разные методы обучения, t=1,...,T

**Алгоритм co-learning** --- это self-training для композиции --- простого голосования базовых алгоритмов $$a_1,...,a_T$$:

$$a(x)=argmax_{y\in Y}\Gamma_y(x)$$, $$\Gamma_y(x_i) = \sum^{T}_{t=1}[a_t(x_i)=y]$$

тогда $$M_i(a)$$ --- степень уверенности классификации $$a(x_i)$$

**Графовый метод кластеризации:**
Графовый алгоритм КПН (кратчайший незамкнутый путь)

1. Найти пару вершин $$(x_i,y_j)\in X^{l+k}$$ с наименьшим $$p(x_i,y_i)$$ и соединить их ребром;
2. пока в выборке остаются изолированные точки
3. Найти изолированную точку,
    ближайшую к некоторой неизолированной;
4. соединить эти две точки ребром;
5. удалить K-1 самых длинных ребер;

Всегда находит пару ближайших вершин и соединяет ребром, а потом находит пару вершин (одна соединенная, другая не соединенная) и соединяет. И так, последовательно, соединяет все точки. Если у нас было l+k точек, то ребер должно быть l+k-1. 

Данный алгоритм не применяют на практике, т.к. у него очень много недостатков, в числе которых чувствительность к шуму --- выбросы очень сильно влияют на дерево, которое строит данный алгоритм. 

Этот алгоритм нам интересен потому, что мы можем на его примере посмотреть как кластеризатор может быть применен для частичного обучения:

1. Найти пару вершин $$(x_i,y_j)\in X^{l+k}$$ с наименьшим $$p(x_i,y_i)$$ и соединить их ребром;
2. пока в выборке остаются изолированные точки
3. Найти изолированную точку,
    ближайшую к некоторой неизолированной;
4. соединить эти две точки ребром;
5. $$\sout{\text{удалить K-1 самых длинных ребер;}}$$
6. $$\color{red} {\text{пока есть путь между двумя вершинами разных классов}}$$ 
7. $$\color{red}{\text{удалить самое длинное ребро на этом пути.}}$$

# Метод k-средних: кластеризация

1. начальное приближение центров $$\mu_y, y\in Y$$;
2. повторять
3. E-шаг:
    отнести каждый $$x_i$$ к ближайшему центру;
    $$y_i := argmin_{y\in Y}\rho(x_i,\mu_y), i=1,...,l+k$$
4. М-шаг:
    вычислить новые положения центров;
    $$\mu_j := \frac{\sum^{l+k}_{i=1}[y_i = y]x_i}{\sum^{l+k}_{i=1}[y_i=y]}, \text{для всех } y\in Y$$
5. пока $$y_i$$ не перестанут изменяться;

Идея --- итерационно повторяется два шага: первый шаг --- относит каждый объект к ближайшему центру кластера; второй шаг --- находит новый центр масс для каждого кластера. 
Это была unsupervised кластеризация

Что изменится в случае semi supervised:

1. начальное приближение центров $$\mu_y, y\in Y$$;
2. повторять
3. E-шаг:
    отнести каждый $$x_i\color{red} \in X^k$$ к ближайшему центру;
    $$y_i := argmin_{y\in Y}\rho(x_i,\mu_y)$$, i=$$\color{red} {l+1}$$,...,l+k
4. М-шаг:
    вычислить новые положения центров;
    $$\mu_j := \frac{\sum^{l+k}_{i=1}[y_i = y]x_i}{\sum^{l+k}_{i=1}[y_i=y]}, \text{для всех } y\in Y$$
5. пока $$y_i$$ не перестанут изменяться;

Если раньше мы **каждый** объект относили к ближайшему центру, то теперь будем относить только **неразмеченные** объекты будет перекластеризовывать. Меняется только переменная цикла, если раньше цикл мы делали по всем объектам, то теперь только неразмеченным.



23:32

