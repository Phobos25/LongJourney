# Методы восстановления регрессиии

Одним из примера линейной регрессии является метод наименьших квадратов (МНК). Чтобы он был многомерный, то ответ должен быть многомерный. Т.е. функция описывающая поведение данных, для подбора параметров к которому используется МНК, должна быть многомерной, как и сами параметры.

Рассмотрим такой случай: 
* X - объекты (часто $$R^{n}$$ - n-мерная матрица); Y - ответы (часто R, реже $$R^{m}$$);
$$X^{l}=(x_{i},y_{i})^{l}_{i=2}$$ - обучающая выборка; (training set)
$$y_{i}=y(x_{i}), y: X\rightarrow Y$$ - неизвестная зависимость;

* a(x) = f(x,$$\alpha$$) - модель зависимости. Какая-то функция, которая описывает зависимость данных Y от X;
$$\alpha \in R^{p}$$ - вектор параметров модели

* МНК:
$$Q(\alpha,X^{l})=\sum_{i=1}^{l}w_{i}(f(x_{i},\alpha)-i_{i})^{2}\rightarrow min_{\alpha}$$

Где $$w_{i}$$ - вес, степень важности i-го объекта. 
$$Q(\alpha^{*}, X^{l})$$ - остаточная сумма квадратов (residual sum of squares, RSS)

еще один алгоритм, который очень похож на МНК и, по сути, им и является - это  метод максимума правдоподобия. Данный метод утверждает, что для любой серии данных, можно подобрать такую функцию, что ошибки будут обычным шумом, распределенными по нормальному гауссовскому закону. 

Модель данных с некоррелированным гауссовским шумом:
$$y(x_{i})=f(x_{i},\alpha)+\varepsilon_{i}, \varepsilon_{i}\sim N(0,\sigma^{2}_{i}), i=1,..l$$
Метод максимума правдоподобия (ММП):
$$L(\varepsilon_{1},..., \varepsilon_{l}|\alpha)$$$$=\prod_{i=1}^{l}\frac{1}{\sigma_{i}\sqrt{2\pi}}exp\left( -\frac{1}{2\sigma^{2}_{i}}\varepsilon_{i}^{2}\right)\rightarrow max_{\alpha}$$;

Если прологарифмируем, то получим вместо умножения сумму:
$$-lnL(\varepsilon_{1},...,\varepsilon_{l}|\alpha)=const(\alpha) + \frac{1}{2}\sum_{i=1}^{l}\frac{1}{\sigma^{2}_{i}}(f(x_{i},\alpha)-y_{i})^{2}\rightarrow min_{\alpha}$$;

Решения МНК и ММП, совпадают, причем веса объектво обратно пропорциональны дисперсии шума, $$w_{i}=\sigma_{i}^{-2}$$

### Многомерная линейная регрессия

Часто решение можно представить через множество линейных функций. Линейные функции несмотря на свои ограничения очень популярны, ввиду простоты, скорости имплементации и интерпретируемости. 
В данном случае, искомую функцию f(x,$$\alpha$$) можно представить 

$$f(x,\alpha)=\sum_{j=1}^{n}\alpha_{j}f_{j}(x)$$,   $$\alpha \in R^{n}$$

в виде суммы линейных функций/признаков. Здесь, f1(x)..fn(x) - это числовые признаки. 
К примеру, если нам надо найти зависимость стоимости квартиры от некоторых параметров. В качестве линейных признаков мы можем использовать такие параметры как, площадь квартиры, наличие балкона, этаж, возраст здания и т.д. Но, следует иметь в виду, что параметры должны быть аддитивны, для простоты интерпретируемости, т.е. если рассматривать такой параметр, как расстояние до ближайшего метро, то, т.к. от увеличения данного параметра цена должна падать, мы должны взять некую функцию - типа, близости к метро, которая чем больше, тем выше цена (если больше, то ближе к метро). 

Для простоты записи, мы будем использовать матричные обозначения: $$F_{l\times n}$$ - матрица числовых признаков, где l - число объектов, n - число признаков. y и $$\alpha$$ - векторы. $$y_{l\times 1}, \alpha_{n\times 1}$$

Следуя матричному обозначению, функционал квадрата ошибки будет выглядеть так:

$$Q(\alpha, X^{l})=||F\alpha - y||^{2}\rightarrow min_{\alpha}$$

Для нахождения минимума дифференцируем данную формулу, $$||F\alpha-y||^{2}$$ можно написать в виде $$(F\alpha-y)^{T}(F\alpha-y)$$, что в результате дифференцирования дает:

$$\frac{\partial Q}{\partial\alpha}(\alpha)=2F^{T}(F\alpha-y)=0$$

Откуда следует нормальная система задачи МНК:

$$F^{T}F\alpha=F^{T}y$$

где $$F^{T}F$$ - ковариационная матрица набора признаков f1,...,fn. 

Решение системы: $$\alpha^{*}=(F^{T}F)^{-1}F^{T}y=F^{+}y$$

## Сингулeярное разложение
произвольная $$l\times n$$-матрица представима в виде сингулярного разложения (singular value decomposition, SVD):

$$F=VDU^{T}$$

Основные свойства:
1) $$l\times n$$ - матрица V=(v1,...,vn) ортогональна, $$V^{T}V=l_{n}$$, столбцы $$v_{j}$$-собственные векторы матрицы $$FF^{T}$$;
2) $$n\times n$$ - матрица U=(u1,...,un) ортогональна, $$U^{T}U=l_{n}$$, $$u_{j}$$ - собственные векторы матрицы $$F^{T}F$$;
3) $$n\times n$$ - матрица D диагональна, $$D=diag(\sqrt{\lambda_{1}},...\sqrt{\lambda_{n}})$$, $$\lambda_{j}\geq 0$$ - собственные значения матриц $$F^{T}F$$ и $$FF^{T}$$.


Если матрица приближается к вырожденной, то его собственные значения становяться почти равными нулю. Что ведет к не очень хорошим результатам. Если собственные значения $$\lambda_{j}\rightarrow 0$$, то уменьшается наказание за очень большие значения параметров, т.е. найденное решение будет неустойчиво. $$||\alpha||\rightarrow \infty$$. Погрешности увеличиваются и 
модель становится **переобученной.** В линейных моделях мультиколлинеарность и переобучение идут рука об руку. 


Три стратегии устранения мультиколлинеарности:
* Регуляризация: $$\alpha\rightarrow min$$;
* Преобразование признаков: $$f_{1},...f_{n}\rightarrow g_{1},...,g_{m}, m<<n$$;
* Отбор признаков: $$f_{1},..,f_{n}\rightarrow f_{j_{i}},...,f_{j_{m}}, m<<n$$.






