# Dillinger

# Конспекты курса machine learning
## линейные методы классификации: Метод опорных векторов (support vector machine - SVM)

Рассмотрим задачу:
Задача классификации: X = $$R^n$$, Y = {-1, +1}
по обучающей выборке $$X^l$$ = $$(x_i, y_i)^{l}_{i=1}$$
найти параметры w $$\in R^n$$, $$w_0 \in R$$ алгоритма классификации

a (x,w) = sign(<x,w> - $$w_0$$)

Метод минимизации аппроксимированного регуляризованного эмпирического риска:

$$\sum^{l}_{i=1}$$ (1 - $$M_i(w,w_0))_+$$ + $$\frac{1}{2C}$$ ||w||$$^2$$ $$\rightarrow$$ $$min_{w, w_0}$$

где $$M_i$$(w,$$w_0$$) = $$y_i$$=$$(<x_i, w> - w_{0})$$ - отступ (margin) объекта $$x_i$$. $$||w||$$ - нормаль, которая разделяет классы из выборки

## оптимальная разделяющая гиперплоскость
Линейный классификатор:
a(x,w) = sign(<x,w> - $$w_0$$)

Нам нужна полоса максимальной ширины, чтобы разделяющая полоса между двумя классами отстояла на максимальном расстоянии от соседних классов. 

В случае, если выборка линейно разделима, мы, после простейших математических операций, приходим к равенству 
Ширина полосы:
$$\frac{2}{||w||}$$ $$\rightarrow$$ max, или ||w|| $$\rightarrow$$ min. 

Но, в реальном случае выборка редко бывает линейно разделимой, и данное условие не выполняется. В этом случае необходимо ослабить систему ограничений, что приводит нас к формуле:

C $$\sum^{l}_{i=1}$$ (1-$$M_i (w,w{_0}))_+$$ + 1/2 $$||w||^2\rightarrow$$$$min_{w,{w_0}}$$

## Конструктивные методы синтеза ядер
**Что такое ядро?**
1) K(x,X') = (x, x') - ядро;

2) константа K(x,x')=1 - ядро;

3) произведение ядер K(x,x')=$$K_1$$(x,x')$$K_2$$(x,x') - ядро;

4) $$\psi$$:X $$\rightarrow$$ R произведение K(x,x')=$$\psi$$(x)$$\psi$$(x') - ядро; Если взять любую функцию в пространстве объектов, взять произведение и получим ядро;

5)K(x,x')=$$\alpha_{1}K_{1}$$(x,x') + $$\alpha_{2}K_{2}$$(x,x') при $$\alpha_{1},\alpha_{2}$$>0 - ядро; Если взять сумму не отрицательных ядер, то получим ядро;

6)$$\phi$$:X $$\rightarrow$$ X если K$$_0$$ ядро, то K(x,x')=K($$\phi(x),\phi(x')$$) - ядро;

7)если s: X x X $$\rightarrow$$ R - симметричная интегрируемая функция, то K(x,x') =; 

8)если K0 - ядро и функция f:R $$\rightarrow$$ R представима в виде сходящегося степенного ряда с неотрицательными коэффициентами, то K(x,x')=f(K0(x,x')) - ядро.  

## Рассмотрим, как SVM строит разделяющие полиномиальные кривые

Для примера, возьмем $$R^{2}$$, т.е. два признака (двумерная матрица). 
Пусть X = $$R^{2}$$, K(u,v) = $$<u,v>^2$$, где u вектор с координатами (u1,u2), v - вектор с координатами (v1,v2). 
Задача: найти пространство H и преобразование $$\psi$$: X $$\rightarrow$$ H, при которых K(x,x') = <$$\psi(x)$$,$$\psi(x')$$>$$_H$$

Если разложим квадрат скалярного произведения K(u,v), то получим сумму трех слагаемые, который тоже имеет вид скалярного произведения: 
$$K(u,v) = u_1^{2}v_1^{2} + u_2^{2}v_2^{2} + 2u_1v_1u_2v_2=<(u_1^{2},u_2^{2},\sqrt{2}u_1u_2),(v_1^{2},v_2^{2},\sqrt{2}v_1v_2)>$$

Таким образом, получается, что H = $$R^{3}$$, а $$\psi :(u_{1},u_{2})$$
 
в R3 - это будет разделяющая гиперплоскость. 

### Примеры ядер

1) K(x,x')=$$<x,x'>^2$$ - квадратичное ядро
2) K(x,x')=$$<x,x'>^d$$ - полиномиальное ядро с мономами степени d;
3) K(x,x')=$$(<x,x'>+1)^d$$ - полиномиальное ядро с мономами степени $$\leq$$ d;
4) K(x,x')=$$\sigma(<x,x'>)$$ - нейросеть с заданной функцией активации $$\sigma(z)$$ (не при всех $$\sigma$$ является ядром);
5) K(x,x')=$$th(k_0+k_1<x,x'>), k_0, k_1 \geq 0$$ - нейросеть с сигмоидными функциями активации;
6) K(x,x')=$$exp (-\beta ||x-x'||^{2})$$ - сеть радиальных базисных функций.


Нейросетью является несколько линейных классификаторов (например SVM), выходы которых служат признаками другим классификаторам. К примеру, суперпозиция таких двух линейных классификаторов является двухслойной нейронной сетью (нн с двумя слоями). 

## SVM  как двухслойная нейронная сеть

Перенумеруем объекты так, чтобы $$x_1,...,x_h$$ были опорными.

$$a(x) = sign(\sum_{i=1}^{h}\lambda_{i}y_{i}K(x,x_{i})-w_0)$$

Здесь, h - число опорных объектов, т.е. число скрытых слоев в нейросети должно быть равно числу опорных объектов. 

Преимущества SVM перед SG и нейронными сетями: 
* Задача выпуклого квадратичного программирования имеет единственное решение.
* Число нейронов скрытого слоя определется автоматически - это число опорных векторов.

Недостатки классического SVM:
* Неустойчивость к шуму.
* Нет общих подходов к оптимизации K(x,x') под задачу
* Приходится подбирать константу C.
* Нет отбора признаков. 

# 1-norm SVM (LASSO SVM)
регуляризатор для SVM. Данный SVM был разработан для решения проблемы отбора признаков у стандартного SVM. 

выглядит алгоритм в качестве формулы так:
$$\sum_{i=1}^{l}(1-M_{i}(w,w_{0}))_{+}+\mu\sum_{j=1}^{n}|w_{j}|\rightarrow min_{w,w_{0}}$$

Из плюсов можно выделить, что у этого SVM есть отбор признаков с параметров селективности $$\mu$$: чем больше $$\mu$$, тем меньше признаков останется. Здесь необходимо иметь в виду, что $$\mu$$ = 1/C, просто обозначение другое;
Из недостатков выделяется то, что у LASSO нет гарантии, что вначале не будут отброшены значимые признаки а не шумы. В реальных случаях, нередко бывает, что сначала отбрасываются значимые признаки, а только потом, шумовые. 
Нету эффекта группировки (grouping effect): значимые зависимые признаки должны отбираться вместе и иметь примерно равные веса $$w_{j}$$

## Doubly Regularized SVM (Elastic Net SVM)

$$C\sum_{i=1}^{l}(1-M_{i}(w,w_{0}))_{+}+\mu\sum_{j=1}^{n}|w_{j}|+\frac{1}{2}\sum_{j=1}^{n}w_{j}^{2}\rightarrow min_{w,w_{0}}$$
(+) отбор признаков с параметров селективности $$\mu$$: чем больше $$\mu$$, тем меньше признаков останется. Здесь необходимо иметь в виду, что $$\mu$$ = 1/C, просто обозначение другое;
(+) есть эффект группировки;
(-) Шумовые признаки также группируются вместе, и группы значимых признаков могут отбрасываться, когда еще не все шумовые отброшены


