# Dillinger

# Конспекты курса machine learning
## линейные методы классификации: Метод опорных векторов (support vector machine - SVM)

Рассмотрим задачу:
Задача классификации: X = $$R^n$$, Y = {-1, +1}
по обучающей выборке $$X^l$$ = $$(x_i, y_i)^{l}_{i=1}$$
найти параметры w $$\in R^n$$, $$w_0 \in R$$ алгоритма классификации

a (x,w) = sign(<x,w> - $$w_0$$)

Метод минимизации аппроксимированного регуляризованного эмпирического риска:

$$\sum^{l}_{i=1}$$ (1 - $$M_i(w,w_0))_+$$ + $$\frac{1}{2C}$$ ||w||$$^2$$ $$\rightarrow$$ min$$_{w, w_0}$$

где $$M_i$$(w,$$w_0$$) = $$y_i$$=$$(<x_i, w> - w_{0})$$ - отступ (margin) объекта $$x_i$$. $$||w||$$ - нормаль, которая разделяет классы из выборки

## оптимальная разделяющая гиперплоскость
Линейный классификатор:
a(x,w) = sign(<x,w> - $$w_0$$)

Нам нужна полоса максимальной ширины, чтобы разделяющая полоса между двумя классами отстояла на максимальном расстоянии от соседних классов. 

В случае, если выборка линейно разделима, мы, после простейших математических операций, приходим к равенству 
Ширина полосы:
$$\frac{2}{||w||}$$ $$\rightarrow$$ max, или ||w|| $$\rightarrow$$ min. 

Но, в реальном случае выборка редко бывает линейно разделимой, и данное условие не выполняется. В этом случае необходимо ослабить систему ограничений, что приводит нас к формуле:

C $$\sum^{l}_{i=1}$$ (1-$$M_i (w,w{_0}))_+$$ + 1/2 $$||w||^2\rightarrow$$$$min_{w,{w_0}}$$

