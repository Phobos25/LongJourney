# Методы кластеризации

Задача кластеризации похожа на метод ближайших соседей, но без учителя. 

Решение задачи кластеризации принципиально неоднозначно
* точной постановки задачи кластеризации нет;
* существует много критериев качества кластеризации;
* существует много эвристических методов кластеризации;
* число кластеров |Y|, как правило, неизвестно заранее;
* результат кластеризации существенно зависит от метрики $$\rho$$, которую эксперт задает субъективно

# Цели кластеризации
* **Упростить дальнейшую обработку данных.** разбить множество $$X^l$$ на группы схожих объектов чтобы работать с каждой группой в отдельности (задачи классификации, регрессии, прогнозирования)
* **Сократить объем хранимых данных**, оставив по одному представителю от каждого кластера (задачи сжатия данных).
* **Выделить нетипичные объекты**, которые не подходят ни к одному из кластеров (задачи одноклассовой классификации). 
* **Построить иерархию множества объектов** (задачи таксономии). 
 
**иерархическая кластеризация** основана на очень простой идее:
Пусть наши объекты $$x_1...x_l$$ образуют одноэлементные кластеры. Возьмем два ближайших объекта и сольем их в один кластер, и далее будем считать их одним элементом. Но для этого нового элемента-кластера нам надо будет посчитать расстояние до всех других кластеров. А потом снова находим пары и сливаем их в новый элемент-кластер и т.д. 

Расстояние между кластерами считатеся по формуле Ланса-Уильямса
$$R(U \cup V, S) = \alpha_U\cdot R(U,S)+\alpha_V\cdot R(V,S)+ 
\beta\cdot R(U,V)+ \gamma |R (U,S) - R(V,S)|$$,
где $$\alpha_U, \alpha_V, \beta, \gamma$$ --- числовые параметры

Расстояние между кластерами может быть решено такой рекурентной формулой. 

Мерой оценки качества методом кластеризации является наличие свойства монотонности. Которую визуально можно увидеть на дендрограмме. Дендрограмма отражает расстояние $$R_t$$ между кластерами. 

# Свойство монотонности
Кластеризация *монотонна*, если при каждом слиянии расстояние между объединяемыми кластерами только увеличивается: $$R_2\leq R_3\leq R_l$$

Теорема (Миллиган, 1979):
Кластеризация монотонна, если выполняются условия
$$\alpha_U \geq 0, \alpha_V \geq 0, \alpha_U+\alpha_V + \beta \geq 1, min\{\alpha_U, \alpha_v\}+\gamma \geq 0$$

Если кластеризация монотонна, то дендрограмма не имеет самопересечений. 

Еще одним свойством является свойство сжатия и растяжения. Кластеризация сжимающаяся если расстояние уменьшается с каждой итерацией; расстягивающаяся если расстояние увеличивается с каждой итерацией. 

Свойство **растяжения** наиболее желательно, так как оно способствует более чёткому отделению кластеров. 

Метод ближайших соседей является сильно сжимающимся. Методы дальнего соседа и уорда являются растягивающимися. 

# Рекомендации и выводы
Стратегия выбора параметра $$\delta$$ на шагах 2 и 4:
* Если $$|C_t|\leq n_1$$, то $$P(\delta) := \{(U,V): U,V \in C_t\}$$
* Иначе выбрать $$n_2$$ случайных расстояний R(U,V); $$\delta$$:= минимальное из них;
* $$n_1,n_2$$ влияют только на скорость, но не на результат кластеризации; сначала можно положить $$n_1 = n_2 = 20$$

Общие рекомендации по иерархической кластеризации:
* лучше пользоваться $$R^y$$ --- расстояние Уорда;
* лучше пользоваться быстрым алгоритмом;
* определение числа кластеров --- по максимуму $$|R_{t+1} - R_t|$$, тогда результирующее множество кластеров := $$C_t$$

иерархической клстеризацией метод называется, как раз из-за дендрограммы

# Неиерархические методы

Идея первого метода заключается в использовании статистических методов, в корне которого лежат две гипотезы: гипотеза о вероятностной природе данных и гипотеза о пространстве объектов и форме кластеров. 

Предположим, что выборка объектов получается из параметрического семейства арспределений $$p(x) = \sum_{y \in Y}w_y p_y(x), \sum_{y\in Y}w_y = 1$$, где $$p_y(x) - \text{плотность} w_y - \text{априорная вероятность кластера y}$$. У нас случайно величиной является y --- номер кластера, а $$w_y$$ --- является его вероятностью. Данный метод, получается, выплевывает сначала y, а потом по y мы находим x --- объект.

Теперь, предположим, что эти $$p_y$$ не просто распределения, а многомерные гауссовские распределения с диагноальными матрицами ковариации. А диагональная ковариация это все равно, что произведение одномерных гауссиан. 


52:31