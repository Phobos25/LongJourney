# Методы кластеризации

Задача кластеризации похожа на метод ближайших соседей, но без учителя. 

Решение задачи кластеризации принципиально неоднозначно
* точной постановки задачи кластеризации нет;
* существует много критериев качества кластеризации;
* существует много эвристических методов кластеризации;
* число кластеров |Y|, как правило, неизвестно заранее;
* результат кластеризации существенно зависит от метрики $$\rho$$, которую эксперт задает субъективно

# Цели кластеризации
* **Упростить дальнейшую обработку данных.** разбить множество $$X^l$$ на группы схожих объектов чтобы работать с каждой группой в отдельности (задачи классификации, регрессии, прогнозирования)
* **Сократить объем хранимых данных**, оставив по одному представителю от каждого кластера (задачи сжатия данных).
* **Выделить нетипичные объекты**, которые не подходят ни к одному из кластеров (задачи одноклассовой классификации). 
* **Построить иерархию множества объектов** (задачи таксономии). 
 
**иерархическая кластеризация** основана на очень простой идее:
Пусть наши объекты $$x_1...x_l$$ образуют одноэлементные кластеры. Возьмем два ближайших объекта и сольем их в один кластер, и далее будем считать их одним элементом. Но для этого нового элемента-кластера нам надо будет посчитать расстояние до всех других объектов. А потом снова находим пары и сливаем их в новый элемент-кластер и т.д. 

Расстояние между кластерами считается по формуле Ланса-Уильямса
$$R(U \cup V, S) = \alpha_U\cdot R(U,S)+\alpha_V\cdot R(V,S)$$+ 
$$+\beta\cdot R(U,V)+ \gamma |R (U,S) - R(V,S)|$$,
где $$\alpha_U, \alpha_V, \beta, \gamma$$ --- числовые параметры

Расстояние между кластерами может быть решено такой рекуррентной формулой. 

На каждом шаге необходимо уметь быстро подсчитывать расстояние от образовавшегося кластера W = $$U\cup V$$ до любого другого кластера S, используя известные расстояния с предыдущих шагов. 

Мерой оценки качества методом кластеризации является наличие свойства монотонности. Которую визуально можно увидеть на дендрограмме. Дендрограмма отражает расстояние $$R_t$$ между кластерами. 

# Свойство монотонности
Кластеризация *монотонна*, если при каждом слиянии расстояние между объединяемыми кластерами только увеличивается: $$R_2\leq R_3\leq R_l$$

Теорема (Миллиган, 1979):
Кластеризация монотонна, если выполняются условия
$$\alpha_U \geq 0, \alpha_V \geq 0, \alpha_U+\alpha_V + \beta \geq 1, min\{\alpha_U, \alpha_v\}+\gamma \geq 0$$

Если кластеризация монотонна, то дендрограмма не имеет самопересечений. 

Еще одним свойством является свойство сжатия и растяжения. Кластеризация сжимающаяся если расстояние уменьшается с каждой итерацией; расстягивающаяся если расстояние увеличивается с каждой итерацией. 

Свойство **растяжения** наиболее желательно, так как оно способствует более чёткому отделению кластеров. 

Метод ближайших соседей является сильно сжимающимся. Методы дальнего соседа и уорда являются растягивающимися. 

# Рекомендации и выводы
Стратегия выбора параметра $$\delta$$ на шагах 2 и 4:
* Если $$|C_t|\leq n_1$$, то $$P(\delta) := \{(U,V): U,V \in C_t\}$$
* Иначе выбрать $$n_2$$ случайных расстояний R(U,V); $$\delta$$:= минимальное из них;
* $$n_1,n_2$$ влияют только на скорость, но не на результат кластеризации; сначала можно положить $$n_1 = n_2 = 20$$

Общие рекомендации по иерархической кластеризации:
* лучше пользоваться $$R^y$$ --- расстояние Уорда;
* лучше пользоваться быстрым алгоритмом;
* определение числа кластеров --- по максимуму $$|R_{t+1} - R_t|$$, тогда результирующее множество кластеров := $$C_t$$

иерархической клстеризацией метод называется, как раз из-за дендрограммы

# Неиерархические методы

Идея  метода заключается в использовании статистических методов, в корне которого лежат две гипотезы: гипотеза о вероятностной природе данных и гипотеза о пространстве объектов и форме кластеров. 

Предположим, что выборка объектов получается из параметрического семейства распределений $$p(x) = \sum_{y \in Y}w_y p_y(x), \sum_{y\in Y}w_y = 1$$, где $$p_y(x) - \text{плотность }; w_y - \text{априорная вероятность кластера y}$$. У нас случайно величиной является y --- номер кластера, а $$w_y$$ --- является его вероятностью. Данный метод, получается, выплевывает сначала y, а потом по y мы находим x --- объект.

Теперь, предположим, что эти $$p_y$$ не просто распределения, а многомерные гауссовские распределения с диагноальными матрицами ковариации. А диагональная ковариация это все равно, что произведение одномерных гауссиан. 

# Метод k-средних (k-means)
Данный алгоритм является более известным, более популярным, и более простым. 

1) начальное приближение центров $$\mu_y, y\in Y$$
2) **повторять**
- 3) аналог E-шага: отнести каждый $$x_i$$ к ближайшему центру: $$y_i = argmin_{y\in Y}\rho(x_i,\mu_y), i = 1,...,k$$
- 4) аналог M-шага: вычислить новые положения центров: $$\mu_{yj} = \frac{\sum^{l}_{i=1}[y_i=y]f_j(x_i)}{\sum^{l}_{i=1}[y_i=y]}, y\in Y, j=1,...,n$$
5) **пока** $$y_i$$ не перестанут изменяться.

# метод стохастического градиента

Минимизации среднего внутрикластерного расстояния:

$$Q(wlX^l) = \frac{1}{2}\sum^{l}_{i=1}\rho^2 (x_i;w_{a(x_i)})\rightarrow min_{w}, w=(w_1,...,w_M)$$
выражается как квадрат евклидова расстояния между каждым объектом и центром кластера к которому относятся данные объекты. 
Оптимизация происходит по весам, которые сидят внутри данного алгоритма.

Пусть метрика евклидова: $$\rho^2(x_i,w_m)=||w_m - x_i||^2$$

Градиентный шаг в методе SG, для случайного $$x_i \in X^l$$

$$w_m := w_m + \eta(x_i - w_m)[a(x_i)=m]$$
(если $$x_i$$ относится к кластеру m, то $$w_m$$ сдвигается в сторону $$x_i$$)

**сеть кохонена** - это специальная нейронная сеть для решения кластеризации. Почему специальная? потому что на втором слое обычно стоит сумматор, и на третьем тоже. В нашем случае на 3-м слое стоит arg min, который выдает наименьшие значение a(x) (см. рис)

# Карта Кохонена (Self Organizing Map, SOM)

Важно понимать, что несмотря на похожие названия карта Кохонена и сети Кохонена не одно и то же. Сети Кохонена кластеризуют данные методом стохастического градиента. 

Карта Кохонена создана для того, чтобы не только кластеризовать, но и визуализивароть кластеры. В данном методе кластеры расположены сеткой (прямоугольной или шестигранной, т.к. шестигранная сетка выглдятет приятнее). Одним из характерных свойств карты является то, что число кластеров --- точек в сетке, может быть сильно больше чем объектов в выборке. Это сделано для того, чтобы можно было визуализировать многомерные объекты. 

Y = $$\{1,...,M\}\times \{1,...,H\}$$ --- прямоугольная сетка кластеров. Каждому узлу (m,h) приписан нейрон Кохонена $$w_{mh} \in R^n$$. Наряду с метрикой $$\rho(x_i,x)$$ на $$X^l$$ вводится метрика на сетке Y:

$$r((m_i,h_i),(m,h)) = \sqrt{(m-m_i)^2+(h-h_i)^2}$$
см. рис. SOM

Используется одновременно две метрики: 
1) то самое $$\rho$$ в n-мерной пространстве
2) двумерное евклидово расстояние (см. формула $$r((m_i,h_i)(m,h))$$)

В основе алгоритма лежит тот же стохастический градиент. Для начала 1) мы случайным образом выбираем $$x_i$$ из $$X^l$$. 2) мы методом жесткой конкуренции WTA (победитель забирает все) вычисляем координаты кластеров, к которому относится объект. 
Для этих кластеров, затем, мы применяем градиентный спуск, и приближаем кластер к объекту. 
Основное отличие от сети Кохонена в том, что используется евклидова метрика в градиентном спуске. 

Интерпретация карты Кохонена

Два типа графиков --- цветных карт $$M\times H$$

* Цвет узла (m,h) --- локальная плотность в точке (m,h) --- среднее расстояние до k ближайших точек выборки;
* По одной карте на каждый признак: цвет узла (m,h) --- значение j-й компоненты вектора $$w_{m,h}$$

**Пример:**
Задача UCI house-votes (US Congress voting patterns)
Объекты --- когрессмены;
Признаки --- вопросы, выносившиеся на голосование;
Есть целевой признак {демократ, республиканец}

На примерах можно увидеть, что голосование почти всегда отражало партийную принадлежность. Хотя были и исключения, когда демократы и республиканцы голосоваи по разному. 

## Достоинства и недостатки карт Кохонена

***Достоинства:***
* Возможность визуального анализа многомерных данных

***Недостатки:***
* **Субъективность.** Карта зависит не только от кластерной структуры данных, но и:
* 1) от свойств сглаживающего ядра;
* 2) от (случайной) инициализации;
* 3) от (случайного) выбора $$x_i$$ в ходе операций. 
* **Искажения.** Близкие объекты исходного пространства могут переходить в далекие точки на карте и наоборот. 
 
**Рекомендуется** только для разведочного анализа данных

