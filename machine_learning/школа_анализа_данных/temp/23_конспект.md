# Вероятностные тематические модели коллекций текстовых документов. Часть II

Вывод предыдущей части лекции: распределение Дирихле --- слишком слабый регуляризатор, необходимо что-то более сильное. 

# Регуляризованный ЕМ-алгоритм

Теорема
Если $$\Phi, \Theta$$ --- решение задачи максимизации регуляризванного правдоподобия, то оно удовлетворяет системе уравнений

$$\left\{\begin{aligned}n_{dwt}=n_{dw}\frac{\phi_{wt}\theta_{td}}{\sum_{s\in T}\phi_{ws}\theta_{sd}}\text{;}\\
\phi_{wt}=\frac{n_{wt}}{n_t};n_{wt}=\left(\sum_{d\in D}n_{dwt}+\phi_{wt}\frac{\partial{R}}{\partial{\phi_{wt}}} \right)_{+}; n_t=\sum_{w\in W}n_{wt};\\
\theta_{td}=\frac{n_{td}}{n_d};n_{td}=\left(\sum_{w\in d}n_{dwt}+\theta_{td}\frac{\partial{R}}{\partial{\theta_{td}}} \right)_{+}; n_d=\sum_{t\in T}n_{td}\\
\end{aligned}\right.$$

PLSA: $$R(\Phi, \Theta)=0$$
LDA: $$R(\Phi, \Theta)=\sum_{t,w}\beta_{w}\ln{\phi_{wt}}+\sum_{d,t}\alpha_{t}\ln{\theta_{td}}$$

Документ всегда состоит из общих слов --- белые слова, или фоновые --- и из специальных терминов --- искомые слова. Для работы любого алгоритма необходимо что-то сделать с этими белыми словами. Для этого создается общая тема, которая содержит все слова, с какими-то не нулевыми вероятностями. А темы для специальных терминов имеют нулевые вероятности для слов из других тем --- предметных областей. 
Документ не должен иметь слишком много тем. Исключение --- энциклопедия, но в этом случае надо порезать энциклопедию на документы. 
Получается, что и темы и документы должны быть очень разреженными матрицами. Ни один алгоритм сам по себе такую матрицу не восстановит, нам надо заставить его так делать. 

18:29