# Вероятностные тематические модели коллекций текстовых документов

 Это один из современных (на 2014 г.) подходов к анализу текстов. 
 
 **Понятие "латентной темы"**
 * *Тема* --- специальная терминология предметной области
 * *Тема* --- набор терминов (слов или словосочетаний), совместно часто встречающихся в документах
 * *Тема* --- вероятностное распределение на терминах p(w|t) --- вероятность встретить термин w в теме t
 
Также, следует иметь в виду, что значит термины? Это последовательность слов? это просто слова, которые встречаются в тексте? (тогда будет сильно зависеть от длины документа). Документы бывают очень разных длин, новостные --- довольно короткие, твиты --- супер короткие, научные статьи --- несколько страниц, или вообще энциклопедия, которая представляет собой набор документов, или это не набор документов а один документ? Все эти вопросы, которые можно по разному интерпретировать и они будут влиять на подходы к их решению.

 Документ имеет ненаблюдаемый *тематический профиль* p(t|d) --- неизвестная частота темы t в документе d
Когда автор писал термин w в документ d он думал о теме t
Документ d состоит из наблюдаемых терминов $$w_1,...,w_{n_d}$$, p(w|d) --- известная частота термина w в документе d
тематическая модель пытается выявить латентые темы. 

# Цели и приложения тематического моделирования

* Выявить скрытую тематическую структуру коллекции текстов
* Выявить тематический профиль каждого документа.
 
**Приложения:**
* Семантический поиск по текстовому эффекту любой длины
* Категоризация, классификация, аннотирование, суммаризация, сегментация текстовых дкоументов
* поиск научной информации, трендов, фронта исследований
* Поиск специалистов (expert reach), рецензентов, проектов
* Анализ и агрегирование новостных потоков
* Рубрикация документов, изображений, видео, музыки
* Рекомендующие системы, коллаборативная фильтрация
* Аннотация генома и другие задачи биоинформатики.
* Анализ дискретизированных биомедицинских сигналов

# Основные предположения

* Порядок, документов в коллекции не важен
* Порядок слов в документе не важен (bag of words). *С точки зрения категоризации действительно не важен порядок слов. Представьте, если случайным образом переставить все слова в документе/научной статье, то он потеряет весь смысл, но все равно можно будет догадаться, была ли статья по математика или по биологии*
* Слова, встречающиеся "почти во всех" документах, не важны. 
* Слово в разных формах --- это одно и то же слово. *Надо уметь приводить слова в изначальную форму. Лемматизировать --- заранее обработать документ*
* Документ обычно относится к небольшому числу тем
* Тема обычно определяется небольшим числом терминов.


# Вероятностная формализация постановки задачи

Формализация основных предположений:
* каждое слово в документе связано с некоторой темой $$t\in T$$
* $$D\times W\times T$$ --- дискретное вероятностнео пространство
* коллекция --- это i.i.d. выборка $$(d_i,w_i,t_i)^{n}_{t=1}\sim(d,w,t)$$
* $$d_i,w_i$$ ---наблюдаемые, темы $$t_i$$ --- скрытые
* гипотеза условной независимости: $$p(w|d,t) = p(w|t)$$

Мы считаем, что у нас вероятностным пространством является множество *троек*: документ-слово-тема. В каждой тройке, документ-слово мы можем наблюдать --- слово встретилось в данном документе. А тема, которую автор писал, нам неизвестна --- она латентная, она скрытая. Несмотря на то, что мы говорим, что наблюдаем тройки, документ-слово-тема, на самом деле мы наблюдаем только документ-слово --- какое слово встретилось в каком документе. Это называется случайной независимой выборкой, т.к. мы приняли гипотезу о том, что не важен порядок документов и не важен порядок слов в документе, и мы надеемся, что сможем определить тему правильно. Еще одной гипотезой является то, что частота встречаемости слов в документе зависит **только** от темы, и не зависит от документа, т.е. тема определяет состав слов в документе, а не сам документ. 


Вероятностная модель порождения документа d:
$$\bold{p(w|d)} = \sum_{t\in T}p(w|d,t)p(t|d)=\bold{\sum_{t\in T}p(w|t)p(t|d)}$$

Дано p(w|d) = $$n_{dw}/n_d$$, **найти:**
* $$\phi_{wt}$$ = p(w|t) --- распределеине терминов в темах $$t\in T$$;
* $$\theta_{td}=p(t|d)$$ --- распределение тем в документах $$d\in D$$.
 

# Принцип максимума правдоподобия

**Правдоподобие** --- это плотность распределения выборки D;

$$p(D) = \prod^{n}_{i=1}p(d_i,w_i) = \prod_{d\in D}\prod_{w\in d}p(d,w)^{n_{dw}}$$

где $$n_{dw}$$ --- число вхождений термина w в документ d. 

Пусть p(w|d,$$\alpha$$) --- параметрическая вероятностная модель документа d, зависящая от вектора параметров $$\alpha = (\Phi, \Theta)$$. 

**Логарифм правдоподобия** выборки D:
$$\log{p(D,\alpha)}=\sum_{d\in D}\sum_{w\in d}n_{dw}\log{p(w|d,\alpha)p(d)}\rightarrow max_{\alpha}$$

Избавимся от p(d), не влияющего на точку максимума: 
$$L(d,\alpha)=\sum_{d\in D}\sum_{w\in d}n_{dw}\log{p(w|d,\alpha)}\rightarrow max_{\alpha}$$

# Модель PLSA (Probabilistic Latent Semantic Analysis)

Задача: найти максимум правдоподобия:
$$\sum_{d\in D}\sum_{w\in D}n_{dw}\ln{\sum_{t\in T}\phi_{wt}\theta_{td}}\rightarrow max_{\Phi,\Theta}$$

при ограничениях неотрицательности и нормировки
$$\phi_{wt}\geq 0; \sum_{w\in W}\phi_{wt}=1; \theta_{td}\geq 0; \sum_{t\in T}\theta_{td}=1$$

Интерпретация: стохастическое матричное разложение
$$F\approx \Phi\Theta$$

$$F=(\hat{p}(w|d))_{W\times D}$$ --- известная матрица исходных данных;
$$\Phi = (\phi_{wt})_{W\times T}$$ --- искомая матрица терминов тем $$\phi_{wt} = p(w|t)$$
$$\Theta = (\theta_{td})_{T\times D}$$ --- искомая матрица тем документов $$\theta_{td}=p(t|d)$$

Такой функционал эквивалентен решению такой системы уравнений:

**Теорема**
Точка максимума правдоподобия $$\Phi, \Theta$$ удовлетворяет системе уравнений со вспомогательными переменными $$n_{dwt}$$: (см. рис.)

Формула записана в очень удобном для вычислений виде. Фактически, это метод простых итераций. Фи и тета это основные переменные; $$n_{dwt}$$ --- вспомогательные переменные, которые вычисляются по основным переменным;  $$n_{dw}$$ --- исходные данные. Зная, начальные приближения для фи и тета мы можем найти вспомогательные переменные. Зная вспомогательные переменные мы можем подсчитать суммы (М-шаг) и найти оценки основных переменных фи и тета. 

В итоге, повторяя эти шаги итерация за итерацией, мы можем найти фи и тета. Процесс всегда сходится, но не гарантированно к глобальному экстремуму.  

$$n_{dwt} = n_{dw}p(t|d,w)$$, где $$p(t|d,w)$$ --- условная вероятность темы, $$n_{dw}$$ --- сколько раз число встретилось в документе. Получается, сколько раз слово w в документе d  было связано с темой t. Очень классная величина, которая разложила все слова во всех документах по темам. Если мы будем суммировать эту величину по любому из трех индексов мы будем получать что-то интересное.

Пример:
$$n_{wt} = \sum_{d\in D} n_{dwt}$$ --- сумма по документам в коллекции. Получили, сколько раз тема t была связана со словом w во всей коллекции документов. 

$$n_{t} = \sum_{w\in W}n_{wt}$$ --- сумма по словам. Получили, сколько слов связано с темой t во всей коллекции документов. 

$$n_{td} = \sum_{w\in d} n_{dwt}$$ --- просуммировали по всем словам документа. сколько слов документа связано с темой t (внутри одного документа).

$$n_d = \sum_{t\in T} n_{td}$$ --- какова длина документа. 

Поделив, мы получим условную вероятность слова в теме, вернее его частотную оценку. 

Можно было бы не применять метод максимума правдоподоия и распсиать просто через формулу Байеса. Результат был бы тот же, но это была бы эвристика, а так, у нас есть некое обоснование. 

# Модель LDA (Latent Dirichet Allocation)

Если матрицы Фи и Тета слишком богаты параметрами, то давайте наложим дополнительные ограничения:
* Столбцы матриц --- векторы, они не произвольные, а порождаются еще одним вероятностным распределением; Таких распределение несколько, одним из них является распределение Дирихле. 
* Распределение Дирихле позволяло описать матрицы Фи и Тета, как сильно разреженные распределения, сконцентрированные и равномерные распределения Дирихле. 

Отклонение восстановленных распределений p(i|j) от исходных модельных распределений $$p_0(i|j)$$ измеряются средним расстоянием Хеллингера:

$$H(p,p_0) = \frac{1}{m}\sum^{m}_{j=1}\sqrt{\frac{1}{2}\sum^{n}_{i=1}(\sqrt{p(i|j)}-\sqrt{p_0(i|j)})^2}$$

Это общепринятый способ оценки распределения вероятностей. Т.к. если мы возьмем привычным нам среднеквадратичное отклонение то получится не очень хорошая картина. Пример:
допустим у нас есть вероятность 0.01 и 0.02, их разность будет равна 0.01 так же как разность 0.50 и 0.51. Если второй случай вероятности почти равны, случаются в половине случаев, то в первом разница в 2 раза. 
Такой подход с квадратным помогает чуть лучше оценить подобные случаи. 













