# Обучение с подкреплением (Reinforcement Learning)

Когда мы говорим о машинном обучении, когда говорим об обучении алгоритма, метода и т.д. Это все не правда, это не обучение, это аппроксимация. Обучение с учителем это не обучение, это что-то другое. 
Обучение --- это как обучаются животные, как обучаются дети, когда они существуют в какой среде, и сами учаться взаимодействовать со средой и что-то делать. 

# Задача о многоруком бандите
Название взято из аналогии того, что если у нас есть несколько автоматов --- одноруких бандитов, у них процент побед будет разным. И у нас возникает желание поэкспериментировать с ними, чтобы накопить какую-либо статистику, и дальше играть с тем, у которого процент выигрыша самый большой. 

A --- множество возможных действий
$$p_a(r)$$ --- неизвестное распределение премии $$r\in R$$ за $$\forall_{a}\in A$$
$$\pi_t(a)$$ --- стратегия агента в момент t, распределение на A

**Игра агента со средой**
1) инициализация стратегии $$\pi_1(a)$$
2) **для всех** t=1,...,T,...
3) агент выбирает действие $$a_t\sim\pi_t(a)$$;
4) среда генерирует премию $$r_t \sim p_{a_t}(r)$$;
5) агент корректирует стратегию $$\pi_t+1(a)$$

$$Q = \frac{\sum^{t}_{i=1}r_i[a_i=a]}{\sum^{t}_{i=1}[a_i=a]}$$ --- средняя премия в t играх

$$Q^*(a) = \lim_{t\rightarrow \infty}Q_t(a)\rightarrow max_{a\in A}$$ --- ценность действия **a**

Агент должен не только подобрать подходящую стратегию, но и со временем его корректировать, т.е. правила среды не стационарны. 

Когда агент начинает играть со средой, она для него неизвестна, поэтому ему надо как-то ее исследовать. 

# Примеры прикладных задач

* Управление технологическими процессами
* Управление роботами
* Показ рекламы в Интернете
* Управление ценами и ассортиментом в сетях продаж
* Игра на бирже
* Маршрутизации в телекоммуникационных сетях
* Маршрутизация в беспроводных сенсорных сетях
* Логичекие игры (шашки, нарды, и т.д.)

# Жадные и полужадные стратегии

Одним из естественных методов является жадный метод, который будет всегда отбирать стратегии с максимальным $$Q_t(a)$$. Действительно, такой подход позволит получить достаточно высокую точность, но из недостатков можно выделить то, что не исследуется среда. 

Полужадный алгоритм ($$\varepsilon$$-жадный алгоритм) такой же, как и жадный, но время от времени наш алгоритм исследует среду. $$\varepsilon$$ --- это как раз вероятность, с которой наш алгоритм будет исследовать среду. Со временем эту величини $$\varepsilon$$ надо уменьшать

Еще одна стратегия это softmax (распределение Гиббса):
$$\pi_{t+1}(a)=\frac{\exp{(Q_t(a)/\tau)}}{\sum_{b\in A}\exp{(Q_t(a)/\tau)}}$$
где $$\tau$$ - параметр температуры
при $$\tau\rightarrow 0$$ стратегия стремится к жадной
при $$\tau\rightarrow \infty$$ --- к равномерной, т.е. чисто исследовательской

**Эвристика**: параметр $$\tau$$ имеет смысл уменьшать со временем.

Какая из стратегий лучше?
--- зависит от конкретной задачи
--- решается в эксперименте

Почему так важно исследовать среду, а не фокусироваться сразу на самых точных оценках? Потому что некоторые методы могут быть сначала хуже оценивать, а потом становиться все лучше и лучше. Методы, которые сразу дают неплохую оценку обычно бывают очень простыми, и многого не учитывают, у них большой bias. Если есть возможность, такие методы надо избегать. 

# Метод преследования (pursuit) жадной стратегии

Вместо собственно жадной стратегии

$$\pi_{t+1}(a) = \frac{|a \in A_t|}{|A_t|}$$

предлагается *преследование* (сглаживание) жадной стратегии:

применяем формулу экспоненциального скользящего среднего к жадной стратегии:
$$\pi_{t+1}(a) = \pi_t(a) + \beta \left( \frac{[a\in A_t]}{|A_t|}-\pi_t(a)\right)$$
Здесь везде появляются функции от (а)

**Эвристика:** начальное $$\pi_0(a)$$ можно взять равномерным

**Экспериментальный факт**: метод преследования, сравнение с подкреплением и $$\varepsilon$$-жадные стратегии каждый имеет свою область применения. 

Этот метод оказывается лучше предыдущих методов: метода с подкреплением (reinforcement comparion), $$\varepsilon$$-жадной стратегии ($$\varepsilon$$-greedy). 

Давайте сделаем среду посложнее:
вводится множество состояний среды. 
**Игра агента со средой:**
1) инициализация стратегии $$\pi_1(a|s)$$ и состояние среды $$s_1$$
2) для всех t=1,...,T:
* 3) агент выбирает действие $$a_t\sim$$$$\pi_t(a|s_t)$$
* 4) среда генерирует премию $$r_{t+1}\sim$$$$p(r|a_t,s_t)$$ и новое состояние $$s_{t+1}\sim p(s|a_t,s_t)$$;
* 5) агент корректирует стратегию $$\pi_{t+1}(a|s)$$
 
Типичный пример для такой игры, это оптимизация работы лифтов. Если лифтов много, состояние --- это то, на каком этаже сейчас находится лифт, и система должна реагировать на то, что подошел человек и нажал кнопку. Какой лифт ему выгодней прислать. 

Метод SARSA (state-action-reward-state-action)

**Игра агента со средой:**
1) инициализация стратегии $$\pi_1(a|s)$$ и состояние среды $$s_1$$
2) для всех t=1,...,T:
* 3) агент выбирает действие $$a_t\sim$$$$\pi_t(a|s_t)$$: $$a_t=argmax_aQ(s_t,a)$$ --- жадная стратегия (но возможны и другие: $$\varepsilon$$-жадная, по Гиббсу...)
* 4) среда генерирует премию $$r_{t+1}\sim$$$$p(r|a_t,s_t)$$ и новое состояние $$s_{t+1}\sim p(s|a_t,s_t)$$;
* 5) агент генерирует еще один шаг: $$a'\sim\pi_{t+1}(a|s_{t+1})$$
* 6) $$Q(s_t, a_t):= Q(s_t, a_t)+\alpha(r_{t+1}+\gamma Q(s_{t+1},a'))-Q(s_t,a_t))$$

т.к. метод Q всегда одна и та же, другой метод позволяет менять Q

# Метод Q-обучения

Что мы же хотим оценивать? Какую такую ценность мы ищем? Попытка приблизить реальную ситуацию к условиям игры --- это использование оптимальной функции. Мы будем использовать не $$Q^\pi$$ а Q* --- которая является оптимальной функцией ценности. Если каждый раз мы будем использовать оптимальную фунцию, то она при условии изменчивости игры, тоже будет меняться. 

$$Q^*(s_t, a_t):= E(r_{t+1}+\gamma max_{a'}Q^*(s_{t+1},a')) | s_t=s, a_t=a$$ 
Здесь мы используем max, т.к. считается что при максимизации мы оптимизируем нашу функцию. 

Оценка Q*(s,a) экспоненциальным скользящим средним:
$$Q(s_t, a_t):= Q(s_t, a_t)+\alpha(r_{t+1}+\gamma max_{a'}Q(s_{t+1},a'))-Q(s_t,a_t))$$

Отличие от SARSA: выкидывается шаг 5 и меняется шаг 6.

# Резюме в конце лекции
* В обучении с подкреплением нет ответов учителя, есть только ответная реакция среды;
* Задача о многоруком бандите --- это простой случай среды с одним состоянием;
* В общей задаче ценность состояний и действий зависит от состояний среды, но также может быть оценена экспоненциальным скользящим средним;
* Компромисс "изучение-применение" обычно подбирается под задачу экспериментальным путем.
