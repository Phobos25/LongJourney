# Обучение с подкреплением (Reinforcement Learning)

Когда мы говорим о машинном обучении, когда говорим об обучении алгоритма, метода и т.д. Это все не правда, это не обучение, это аппроксимация. Обучение с учителем это не обучение, это что-то другое. 
Обучение --- это как обучаются животные, как обучаются дети, когда они существуют в какой среде, и сами учаться взаимодействовать со средой и что-то делать. 

# Задача о многоруком бандите
Название взято из аналогии того, что если у нас есть несколько автоматов --- одноруких бандитов, у них процент побед будет разным. И у нас возникает желание поэкспериментировать с ними, чтобы накопить какую-либо статистику, и дальше играть с тем, у которого процент выигрыша самый большой. 

A --- множество возможных действий
$$p_a(r)$$ --- неизвестное распределение премии $$r\in R$$ за $$\forall_{a}\in A$$
$$\pi_t(a)$$ --- стратегия агента в момент t, распределение на A

**Игра агента со средой**
1) инициализация стратегии $$\pi_1(a)$$
2) **для всех** t=1,...,T,...
3) агент выбирает действие $$a_t\sim\pi_t(a)$$;
4) среда генерирует премию $$r_t \sim p_{a_t}(r)$$;
5) агент корректирует стратегию $$\pi_t+1(a)$$

$$Q = \frac{\sum^{t}_{i=1}r_i[a_i=a]}{\sum^{t}_{i=1}[a_i=a]}$$ --- средняя премия в t играх

$$Q^*(a) = \lim_{t\rightarrow \infty}Q_t(a)\rightarrow max_{a\in A}$$ --- ценность действия **a**

Агент должен не только подобрать подходящую стратегию, но и со временем его корректировать, т.е. правила среды не стационарны. 

Когда агент начинает играть со средой, она для него неизвестна, поэтому ему надо как-то ее исследовать. 

# Примеры прикладных задач

* Управление технологическими процессами
* Управление роботами
* Показ рекламы в Интернете
* Управление ценами и ассортиментом в сетях продаж
* Игра на бирже
* Маршрутизации в телекоммуникационных сетях
* Маршрутизация в беспроводных сенсорных сетях
* Логичекие игры (шашки, нарды, и т.д.)

# Жадные и полужадные стратегии

Одним из естественных методом является жадный метод, который будет всегда отбирать стратегии с максимальным $$Q_t(a)$$. Действительно, такой подход позволит получить достаточно высокую точность, но из недостатков можно выделить то, что не исследуется среда. 

Полужадный алгоритм ($$\varepsilon$$-жадный алгоритм) такой же, как и жадный, но время от времени наш алгоритм исследует среду. $$\varepsilon$$ --- это как раз вероятность, с которой наш алгоритм будет исследовать среду. Со временем эту величини $$\varepsilon$$ надо уменьшать

Еще одна стратегия это softmax (распределение Гиббса):
$$\pi_{t+1}(a)=\frac{\exp{(Q_t(a)/\tau)}}{\sum_{b\in A}\exp{(Q_t(a)/\tau)}}$$
где $$\tau$$ - параметр температуры
при $$\tau\rightarrow 0$$ стратегия стремится к жадной
при $$\tau\rightarrow \infty$$ --- к равномерной, т.е. чисто исследовательской

**Эвристика**: параметр $$\tau$$ имеет смысл уменьшать со временем.

Какая из стратегий лучше?
--- зависит от конкретной задачи
--- решается в эксперименте

Почему так важно исследовать среду, а не фокусироваться сразу на самых точных оценках? Потому что некоторые методы могут быть сначала хуже оценивать, а потом становиться все лучше и лучше. Методы, которые сразу дают неплохую оценку обычно бывают очень простыми, и многого не учитывают, у них большой bias. Если есть возможность, такие методы надо избегать. 

15:52