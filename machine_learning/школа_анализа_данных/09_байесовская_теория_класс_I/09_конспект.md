# байесовская теория классификации

допустим мы знаем, плотность распределения p(x,y)

p(x,y) = p(x)P(y|x)=P(y)p(x|y)

P - вероятность (большие буквы)
p - плотность  (маленькие буквы)

P(y) - означает, что объекты которые приходят, с некоторой вероятностью относится к классу y (априорная вероятность)
P(y|x) - апостериорная вероятность класса y. Вероятность, которая была выставлена после ознакомления с данным x

Априорная вероятность выставляется сразу, даже если нам ничего не известно о конкретных данных. 
Пр.
Допустим, по данным банка мы знаем, что 1 из 11 заемщиков является недобросовестным и деньги скорее всего нам не вернет. Это
априорная вероятность. 
Если к нам подходит человек, а мы про него ничего не знаем, то с вероятность 1 из 11 он недобросовестный. 
Если окажется, что он уже брал кредиты и возвращал, то у него апостериорная вероятность 10 из 11.
Если окажется, что у него несколько судимостей, не погашенные кредиты и т.п., то он является недобросовестным, как раз 1 из 11

плотность распределения p(x) является общей, если мы рассматриваем совокупность распределений p(x) = p(x)хор) + p(x)плох). 
у плохих и хороших плотность распределения своя, но при совместном рассмотрении они должны давать p(x). 

## Оптимальный байесовский классификатор

Теорема 
если известны P(y) и p(x|y), то минимальный средний риск R(a) имеет байесовский классификатор

$$a(x)= argmin\sum_{s\in Y}\lambda_{ys}P(y)p(x|y)$$

$$\lambda_{ys}$$ -  учет потерь, если мы перепутали класс y с s. 

Классификатор может быть оптимальным, только если мы правильно отпределили принажлежность к классам.

При обучении модели следует иметь в виду. что выборка может быть сделана не правильно. Например, если у нас два класса, но один класс
превышает другой в 100 раз, но в выборке они отобраны поровну. В таком случае, выборка была сделана неправильно. 

Для решения такой задачи, надо оценить плотность вероятности 

## Три подхода к оцениванию плотностей

1) Параметрическое оценивание плотности:
$$p(x) = \phi(x,\theta)$$

2) Восстановление смеси распределений:
$$p(x)=\sum^{k}_{j=1}w_{j}\phi(x,\theta_{j}), k<<m$$

3) Непараметрическое оценивание плотности:
$$p(x) = \sum^{m}_{i=1}\frac{1}{mV(h)}K(\frac{\rho(x,x_{i})}{h})$$

Рассмотрим первый подход. Для оценки плотности вероятностей используется некая функция фи, которую мы вводим сами. Из недостатков можно 
выделить то, что мы можем совсем промахнемся с функцией и она будет оценивать неправильно. Из преимуществ, какая-либо функция гораздо лучше,
чем вообще без функций. 

Рассматривать будем 3й случай для одномерного случая, или т.н. наивный байесовский классификатор

Первое допущение - это то, что данные случайные величины независимы.

надо выбирать окно достаточного размера, чтобы оценка плотности не вырождалась. 
Если окно слишком большое, то оценка плотности выпрямляется, и будует оцениваться плохо.

# Обоснование оценки Парзена-Розенблатта
1) Пусть плотность распределения пришла с этой выборки; (обязательное условие)
2) ядро непрерывно и ограничено, убывает быстро, но не слишком быстро
3) С ростом числа выборки, уменьшаем ширину окна до нуля. Но не слишком быстро и не слишком медленно. 


Для многомерного случая наивный байесовский подход не работает

Поэтому ядро надо сделать многомерным. 
Многомерное ядро = произведение одномерных ядер. 

1) $$\lambda{p}(x) = \frac{1}{m}\sum^{m}_{i=1}\prod^{n}_{j=1}\frac{1}{h_{j}}K\left(\frac{f_{j}(x)-f_{j}(x_{i})}{h_{j}}\right)$$

Надо иметь в виду что данное выражение **не** является наивным байесовским методом, т.к. стоит сумма произведений. Для наивного было бы произведение сумм, большая разница. 
Еще одним отличием является то, что мы не предполагаем независимость, мы ничего не предполагаем, мы просто выбираем ядро. 

Если на X задана функция расстояния $$\rho(x, x')$$
2) $$\lambda{p}_{h}(x) = \frac{1}{mV(h)}\sum^{m}_{i=1}K\left( \frac{\rho(x,x_{i}}{h}\right)$$

где $$V(h) = \int_{X}K\frac{\rho(x,x_{i})}{h}dx$$ - нормирующий множитель

Здесь, V(h) - это такой шар на точке x, у которого объем не меняется от того, в какую точку x мы его поместили (пространство должно быть однородным).

Если ядро K гауссовское а метрика $$\rho$$ - евклидово , то уравнения 1 и 2 совпадут. 

# Метод парзеновского окна

Парзеновская оценка плотности для каждого класса $$y\in Y$$:

$$\lambda{p}_{h}(x|y) = \frac{1}{l_{y}V(h)}\sum_{i:y_{i}=y}K(\frac{\rho(x,x_{i})}{h})$$

Метод парзеновского окна (Parzen window):

$$a(x;X^{l},h)=argmax_{y\in Y}\lambda_{y}\frac{P(y)}{l_{y}}\sum_{i:y_{i}=y}K(\frac{\rho(x,x_{i})}{h})$$

Во втором выражении V(h) выбрасывается, т.к. есть такое свойство, по которому в argmax аргументы, которые не зависят от argmax выбрасываются. 
Здесь, K - это функция близости. Чем ближе объект тем больше значение и наоборот, чем дальше тем ниже. K - убывающая функция.

## Выбор метрики (функция расстояния):
Несмотря на то, что метод у нас был непараметрический, т.е. безмодельный. Нам все равно надо выбрать модель функции расстояния или метрику
Одним из способов выбора метрики является взвешенная метрика Минковского:
$$\rho(x,x')=(\sum^{n}_{j=1}w_{j}|f_{j}(x)-f_{j}(x')|^{p})^{1/p}$$
где $$w_{j}$$ - неотрицательные веса признаков, p>0.

если w = 1, p= 2, то получим евклидову метрику.
 
## Часто используемые ядра
см. рисунок

Следует иметь в виду, что несмотря на многообразие моделей, они мало влияют на точность. Поэтому следует выбирать их не из-за того, что какой-то
точнее, какой-то быстрее, а исходя из задачи. Если нам надо, анализировать всю выборку, то используем гауссовский, если только часть, то
оптимальные и т.д. 
Другими словами, все зависит от задачи и от личного мнения. Нравится один метод, а другой нет? Используем тот, который нравится. 
Выбор ядра, зависит от того, насколько дифференцированную картину мы хотим получить, хотим ли мы смотреть только локальную область или всю? 

## Выбор ширины окна

И наоборот, ширина окна играет решающую роль. 
Для подбора мы можем использовать метод Leave-one-out LOO, обучаем наш классификатор от всех данных, кроме одного. И на этом одном, мы 
находим классификатор. Всегда находится такой интервал значений окна, при котором ошибка будет минимальной.  Очень редко бывает так,
что у нас будет два локальных минимума. 

## Принцип максимума правдоподобия

Пусть известна параметрическая модель плотности

$$p(x) = \phi(x;\theta)$$
где $$\theta$$ - параметр, $$\phi$$ - фиксированная функция.

Данный метод основан на предположении, что нам известна функция плотности, вплоть до параметра $$\phi$$. 

 В этом условии кроется все плюсы и минусы данного метода. Если модель плотности была задана правильно, то мы получим хорошую оценку, если некорректно, то ничего у нас не получится и оценка будет плохая. 
 
Принцип максимума прадоподобия:  
$$L(\theta ; X^{m}) = \sum^{m}_{i=1}ln\phi(x_{i};\theta)\rightarrow  max_{\theta}$$

Необходимое условие оптимума:
$$\frac{\partial}{\partial}L(\theta;X^{m})=\sum^{m}_{i=1}\frac{\partial}{\partial}ln\phi(x_{i};\theta) = 0$$

где функция $$\phi$$ достаточно гладкая по параметру $$\theta$$









