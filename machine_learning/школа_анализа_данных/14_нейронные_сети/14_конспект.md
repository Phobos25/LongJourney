# 14. Искуственные нейронные сети

нервная клетка это черный ящик, которые принимает на входе какие то значения (отрицательные ионы), потом генерирует импульс. 

# Стохастический градиент (Stochastic gradient)

Задача минимизации суммарных потерь:
$$Q(w):=\sum^{l}_{i=1}L(w,x_{i},y_{i})\rightarrow min_{w}$$

**Вход:** выборка $$X^{L}$$; темп обучения $$\eta$$ (learning rate); параметр $$\lambda$$;
**Выход:** веса $$w\equiv (w_{jh}, w_{hm})\in R^{H *(n+m+1)+M}$$

1) инициализировать веса w и текущую оценку Q(w);
2) **повторять**
3) выбрать объект $$x_{i}$$ изи $$X^{L}$$ (например, случайно);
4) вычислить потерю $$L_{i}:=L(w,x_{i},y_{i})$$;
5) градиентный шаг: w:=w-$$\eta \nabla L(w,x_{i},y_{i})$$
6) оценить значение функционала: Q:= $$(1-\lambda)Q+\lambda L_{i}$$
7) **пока** значение Q и/или веса w не стабилизируется;

т.к. сложность вычислений такого градиента достаточно большая, мы будем использовать метод back-propagation, что сильно сократит затраты на вычисления. 
Основная идея обратного распространения ошибок заключается в том, чтобы при первом проходе через нейронную сеть, что-то где-то сохранить в узлах, чтобы при повторных проходах можно было быстро подсчитать градиент. 

Выходные значения сети $$a^{m}(x_{i})$$, m=1..M на объекте $$x_{i}$$:

$$a^{m}(x_{i})=\sigma_{m}(\sum^{H}_{h=0}w_{hm}u^{h}(x_{i}))$$; $$u^{h}(x_{i})=\sigma_{h}\sum^{J}_{j=1}w_{jh}f_{j}(x_{j})$$.

Пусть для конкретности $$L_{i}(w)$$ - средний квадрат ошибки:
$$L_{i}(w)=\frac{1}{2}\sum^{M}_{m=1}(a^{m}(x_{i})-y^{m}_{i})^{2}$$

**Промежуточная задача:** найти частные производные
$$\frac{\partial L_{i}(w)}{\partial a^{m}}$$; $$\frac{\partial L_{i}(w)}{\partial u^{h}}$$

29:50