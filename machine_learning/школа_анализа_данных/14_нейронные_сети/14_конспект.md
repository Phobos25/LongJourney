# 14. Искуственные нейронные сети

нервная клетка это черный ящик, которые принимает на входе какие то значения (отрицательные ионы), потом генерирует импульс. 

# Стохастический градиент (Stochastic gradient)

Задача минимизации суммарных потерь:
$$Q(w):=\sum^{l}_{i=1}L(w,x_{i},y_{i})\rightarrow min_{w}$$

**Вход:** выборка $$X^{L}$$; темп обучения $$\eta$$ (learning rate); параметр $$\lambda$$;
**Выход:** веса $$w\equiv (w_{jh}, w_{hm})\in R^{H *(n+m+1)+M}$$

1) инициализировать веса w и текущую оценку Q(w);
2) **повторять**
3) выбрать объект $$x_{i}$$ из $$X^{L}$$ (например, случайно);
4) вычислить потери $$L_{i}:=L(w,x_{i},y_{i})$$;
5) градиентный шаг: w:=w-$$\eta \nabla L(w,x_{i},y_{i})$$
6) оценить значение функционала: Q:= $$(1-\lambda)Q+\lambda L_{i}$$
7) **пока** значение Q и/или веса w не стабилизируется;

т.к. сложность вычислений такого градиента достаточно большая, мы будем использовать метод back-propagation, что сильно сократит затраты на вычисления. 
Основная идея обратного распространения ошибок заключается в том, чтобы при первом проходе через нейронную сеть, что-то где-то сохранить в узлах, чтобы при повторных проходах можно было быстро подсчитать градиент. 

Выходные значения сети $$a^{m}(x_{i})$$, m=1..M на объекте $$x_{i}$$:

$$a^{m}(x_{i})=\sigma_{m}(\sum^{H}_{h=0}w_{hm}u^{h}(x_{i}))$$; $$u^{h}(x_{i})=\sigma_{h}(\sum^{n}_{j=0}w_{jh}f_{j}(x_{j}))$$.

Пусть для конкретности $$L_{i}(w)$$ - средний квадрат ошибки:
$$L_{i}(w)=\frac{1}{2}\sum^{M}_{m=1}(a^{m}(x_{i})-y^{m}_{i})^{2}$$

**Промежуточная задача:** найти частные производные
$$\frac{\partial L_{i}(w)}{\partial a^{m}}$$; $$\frac{\partial L_{i}(w)}{\partial u^{h}}$$

**быстрое вычисление градиента**

$$\frac{\partial L_{i}(w)}{\partial a^{m}} = a^{m}(x_{i})-y_{i}^{m}=\varepsilon_{i}^{m}$$; 
-это ошибка на выходном слое

$$\frac{\partial L_{i}(w)}{\partial u^{h}}=\sum^{M}_{m=1}(a^{m}(x_{i})-y^{m}_{i})\sigma'_{m}w_{hm}=\sum^{M}_{m=1}\varepsilon^{m}_{i}\sigma'_{m}=\varepsilon^{h}_{i}$$
-это ошибка на выходном слое

где $$\sigma'_{m}$$ это $$\sigma'_{m}(\sum^{H}_{h=0}w_{hm}u^{h}(x_{i}))$$

назовем это ошибкой на выходном слое. Похоже, что $$\varepsilon^{h}_{i}$$ вычисляется по $$\varepsilon^{m}_{i}$$, если запустить сеть "задом наперед"
Из-за этой аналогии данный метод получил название *обратной распространении ошибки*. Хотя это не ошибка, и обратно ничего не распространяется. Такой термин является удоным спосбом запомнить алгоритм для данного метода. 

Теперь имея частные производные L(w) по $$a^{m}$$ и $$u^{h}$$, легко выписать градиент $$L_{i}(w)$$ по всем w:

$$\frac{\partial L_{i}(w)}{\partial w_{hm}}=\frac{\partial L(w)}{\partial a^{m}}\frac{\partial a^{m}}{\partial w_{hm}}=\varepsilon^{m}_{i}\sigma'_{m}u^{h}(x_{i})$$  m=1..M, h=0..H;

$$\frac{\partial L_{i}(w)}{\partial w_{jh}}=\frac{\partial L(w)}{\partial u^{h}}\frac{\partial u^{h}}{\partial w_{jh}}=\varepsilon^{m}_{i}\sigma'_{h}f_{j}(x_{i})$$  h=1..H, h=0..n;

***Алгоритм обратного распространения ошибки Backpropagation:***

**Вход:** $$X^{l}=(x_{i},y_{i})^{l}_{i=1}\subset R^{n} x R^{m}$$; параметры H, $$\lambda, \eta$$;
**Выход:** синаптические веса $$w_{jh}, w_{hm}$$
1) инициализировать весь $$w_{jh}$$, $$w_{hm}$$;
2) **повторять:**
3) выбрать объект $$x_{i}$$ из $$X^{l}$$ (например, случайно);
4) прямой ход:
        $$u^{h}_{i}:=\sigma_{h}(\sum^{n}_{j=0}w_{jh}x^{j}_{i})$$, h=1..H;
        $$a^{m}_{i}:=\sigma_{m}(\sum^{H}_{h=0}w_{hm}u^{h}_{i=1}), \varepsilon^{m}_{i}:=a^{m}_{i}-y_{i}^{m}$$, m=1..M;
        $$L_{i}:=\sum^{M}_{m=1}(\varepsilon^{m}_{i})^{2}$$
5) обратный ход:
        $$\varepsilon^{h}_{i}:=\sum^{M}_{m=1}\varepsilon^{m}_{i}\sigma'_{m}w_{hm}$$, h=1..H
6) градиентный шаг:
        $$w_{hm} := w_{hm} - \eta\varepsilon^{m}_{i}\sigma'_{m}u^{h}_{i}$$, h=0..H, m=1..M;
        $$w_{jh} := w_{jh} - \eta\varepsilon^{h}_{i}\sigma'_{h}x{j}_{i}$$, j=0..n, h=1..H;
7)  $$Q := (1-\lambda)Q+\lambda L$$
8) **пока** Q не стабилизируется;

Пояснение: в 4-м шаге, также, надо подсчитать производные $$\sigma'_{h}$$ и $$\sigma'_{m}$$.

**Стандартные эвристики для метода SG**
* инициализация весов;
* порядок предъявления объектов;
* оптимизация величины градиентного шага;
* регуляризация (сокращение весов)

## Прореживание сети (OBD - optimal brain damage)
Данная методика используется в том случае, если наша сеть переобучена, но тратить время на постройку новой сети с меньшими параметрами мы не хотим. 

Пусть w - локальный минимум Q(w), тогда Q(w) можно аппроксимировать квадратичной формой:

$$Q(w+\delta)=Q(w)+\frac{1}{2}\delta^{T}Q''(w)\delta + o(|\delta|^{2})$$,
где Q''(w)'=$$\frac{\partial^{2}Q(w)}{\partial w_{jh}\partial w_{j'h'}}$$ - гессиан размера $$(H(n+M+1)+M)^{2}$$

**Эвристика** Пусть гессиан Q''(w)' диагонален, тогда 

$$\delta^{T}Q''(w)\delta = \sum^{n}_{j=0}\sum^{h=1}_{H}\delta^{2}_{jh}\frac{\partial^{2}Q(w)}{\partial w^{2}_{jh}}+\sum^{H}_{h=0}\sum^{M}_{m=0}\delta^{2}_{hm}\frac{\partial^{2}Q(w)}{\partial w^{2}_{hm}}$$

Хотим обнулить вес: $$w_{jh} + \delta_{jh}=0$$. Как изменится Q(w)

**Определение**. *Значимость (salience)* веса $$w_{jh}$$ - это изменение функционала Q(w) при его обнулении: $$S_{jh}=w^{2}_{jh}\frac{\partial^{2}Q(w)}{\partial^{2}_{jh}}$$

Касательно упрощения нейросети возникает вопрос, будет ли модель предсказывать так же хорошо, если уменьшить число скрытых слоев до того уровня, который был достигнут прореживанием. Как показывает эксперимент - нет. Действительно, надо было обучать модель и постепенно уменьшать число слоев, тогда и только тогда, точность будет высокая а число слоев не большим. 

1) в BackPropagation вычислять только производные $$\frac{\partial^{2}Q}{\partial w_{jh}^{2}}$$, $$\frac{\partial^{2}Q}{\partial w_{hm}^{2}}$$
2) Если процесс минимизации Q(w) пришел в минимум то
    * упорядочить все веса по убыванию $$S_{jh}$$;
    * удалить N связей с наименьшей значимостью;
    * снова запустить BackPropagation.
3) Если Q(w, $$X^{l}$$) или Q(w,X^{k}), существенно ухудшился то вернуть последние удаленные связи и выйти.

**Отбор признаков** с помощью OBD - аналогично. 
Суммарная значимость признака: $$S_{j}=\sum^{H}_{h=1}S_{jh}$$

**Эмпирический опыт**: результат постепенного прореживания обычно лучше, чем BackProp изначально прореженной сети. 
