# 14. Искуственные нейронные сети

нервная клетка это черный ящик, которые принимает на входе какие то значения (отрицательные ионы), потом генерирует импульс. 

# Стохастический градиент (Stochastic gradient)

Задача минимизации суммарных потерь:
$$Q(w):=\sum^{l}_{i=1}L(w,x_{i},y_{i})\rightarrow min_{w}$$

**Вход:** выборка $$X^{L}$$; темп обучения $$\eta$$ (learning rate); параметр $$\lambda$$;
**Выход:** веса $$w\equiv (w_{jh}, w_{hm})\in R^{H *(n+m+1)+M}$$

1) инициализировать веса w и текущую оценку Q(w);
2) **повторять**
3) выбрать объект $$x_{i}$$ изи $$X^{L}$$ (например, случайно);
4) вычислить потерю $$L_{i}:=L(w,x_{i},y_{i})$$;
5) градиентный шаг: w:=w-$$\eta \nabla L(w,x_{i},y_{i})$$
6) оценить значение функционала: Q:= $$(1-\lambda)Q+\lambda L_{i}$$
7) **пока** значение Q и/или веса w не стабилизируется;

т.к. сложность вычислений такого градиента достаточно большая, мы будем использовать метод back-propagation, что сильно сократит затраты на вычисления. 
Основная идея обратного распространения ошибок заключается в том, чтобы при первом проходе через нейронную сеть, что-то где-то сохранить в узлах, чтобы при повторных проходах можно было быстро подсчитать градиент. 

Выходные значения сети $$a^{m}(x_{i})$$, m=1..M на объекте $$x_{i}$$:

$$a^{m}(x_{i})=\sigma_{m}(\sum^{H}_{h=0}w_{hm}u^{h}(x_{i}))$$; $$u^{h}(x_{i})=\sigma_{h}\sum^{J}_{j=0}w_{jh}f_{j}(x_{j})$$.

Пусть для конкретности $$L_{i}(w)$$ - средний квадрат ошибки:
$$L_{i}(w)=\frac{1}{2}\sum^{M}_{m=1}(a^{m}(x_{i})-y^{m}_{i})^{2}$$

**Промежуточная задача:** найти частные производные
$$\frac{\partial L_{i}(w)}{\partial a^{m}}$$; $$\frac{\partial L_{i}(w)}{\partial u^{h}}$$

**быстрое вычисление градиента**

$$\frac{\partial L_{i}(w)}{\partial a^{m}} = a^{m}(x_{i})-y_{i}^{m}=\varepsilon_{i}^{m}$$; 
где $$\sigma'_{m}=\sigma_{m}(\sum^{H}_{h=0}w_{hm}u^{h}(x_{i}))$$
-это оишбка на выходном слое

$$\frac{\partial L_{i}(w)}{\partial u^{h}}=\sum^{M}_{m=1}(a^{m}(x_{i})-y^{m}_{i})\sigma'_{m}w_{hm}=\sum^{M}_{m=1}\epsilon^{m}_{i}\sigma'_{m}=\varepsilon^{h}_{i}$$
где $$\sigma'_{h}=\sigma_{h}\sum^{J}_{j=0}w_{jh}f_{j}(x_{j})$$
назовем это ошибкой на выходном слое. Похоже, что $$\varepsilon^{h}_{i}$$ вычисляется по $$\varepsilon^{m}_{i}$$, если запустить сеть "задом наперед"
Из-за этой аналогии данный метод получил название *обратной распространении ошибки*. Хотя это не ошибка, и обратно ничего не распространяется. Такой термин является удоным спосбом запомнить алгоритм для данного метода. 

Теперь имея частные производные L(w) по $$a^{m}$$ и $$u^{h}$$, легко выписать градиент $$L_{i}(w)$$ по всем w:

$$\frac{\partial L_{i}(w)}{\partial w_{hm}}=\frac{\partial L(w)}{\partial a^{m}}\frac{\partial a^{m}}{\partial w_{hm}}=\varepsilon^{m}_{i}\sigma'_{m}u^{h}(x_{i})$$  m=1..M, h=0..H;

$$\frac{\partial L_{i}(w)}{\partial w_{jm}}=\frac{\partial L(w)}{\partial u^{h}}\frac{\partial u^{h}}{\partial w_{jh}}=\varepsilon^{m}_{i}\sigma'_{h}f_{j}(x_{i})$$  h=1..H, h=0..n;

***Алгоритм обратного распространения ошибки Backpropagation:***

**Вход:** $$X^{l}=(x_{i},y_{i})^{l}_{i=1}\subset R^{n} x R^{m}$$; параметры H, $$\lambda, \eta$$;
**Выход:** синаптические веса $$w_{jh}, w_{hm}$$
1) инициализировать весь $$w_{jh}$$, $$w_{hm}$$;
2) **повторять:**
3) выбрать объект $$x_{i}$$ из $$X^{l}$$ (например, случайно);
4) прямой ход:
        $$u^{h}_{i}:=\sigma_{h}(\sum^{J}_{j=0}w_{jh}x^{j}_{i})$$, h=1..H;
        $$a^{m}_{i}:=\sigma_{m}(\sum^{H}_{h=0}w_{hm}u^{h}_{i=1}), \varepsilon^{m}_{i}:=a^{m}_{i}-y_{i}^{m}$$, m=1..M;
        $$L_{i}:=\sum^{M}_{m=1}(\varepsilon^{m}_{i})^{2}$$
5) обратный ход:
        $$\varepsilon^{h}_{i}:=\sum^{M}_{m=1}\varepsilon^{m}_{i}\sigma'_{m}w_{hm}$$, h=1..H
6) градиентный шаг:
        $$w_{hm} := w_{hm} - \eta\varepsilon^{m}_{i}\sigma'_{m}u^{h}_{i}$$, h=0..H, m=1..M;
        $$w_{jh} := w_{jh} - \eta\varepsilon^{h}_{i}\sigma'_{h}x{j}_{i}$$, j=0..n, h=1..H;
7)  $$Q := (1-\lambda)Q+\lambda L$$
8) **пока** Q не стабилизируется;

Пояснение: в 4-м шаге, также, надо подсчитать производные $$\sigma'_{h}$$ и $$\sigma'_{m}$$.






48:50