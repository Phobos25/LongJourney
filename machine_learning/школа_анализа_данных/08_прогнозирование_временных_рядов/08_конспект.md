# прогнозирование временных рядов

Отличие от других задач машинного обучения в том, что данные приходят в течении времени по определенному закону. 

1) Задачи прогнозирования:
* понятие временного ряда
* Примеры прикладных задач
* Обзор методов прогнозирования

2) Адаптивные методы краткосрочного прогнозирования
* Экспоненциальное скользящее среднее
* Модели с трендом и сезонностью
* Анализ адекватности адаптивных моделей

3)Адаптивная селекция и композиция
* Адаптивная селекция
* Адаптивная композиция
* Эксперименты с адаптивными композициями. 


Постановка задачи: временной ряд - (det) последовательность чисел       
Нам дается последовательность чисел 
y0,y1,...yt,... - временной ряд yi $$\in$$ R. y0..yt прошлое (числа, которые нам доступны), начиная с t+1 это будущее

\tilda{y}t+d(w) = ft(y1,...yt;w) - модель временного ряда. 
где d - это горизон прогнозирования d = 1,...D
w - вектор параметров модели. 

Для простоты мы будем рассматривать условие с d=1. 

Метод наименьших квадратов: 
Будем использовать мнк, т.к. все таки это задача регрессии
формула


Проблемы:
* решение задачи регрессии - это долго;
* рядов может быть очень много;
* их поведение может быть очень различным;
* функция потерь может быть неквадратичной. 


Дожны построить какую-то функцию (которую надо откуда-то взять), которая бы описывала поведение данных до t. Затем оценивать точность прогнозов. 

## Одна из прикладных задач это эконометрика. 
Прогнозирование цен рыночных продуктов;
биржевые цены;
Объемы грузовых и пассажирских перевозок;
дорожный трафик;
объемы продаж в торговых сетях. 

Из-за таких прикладных случаев и возникает одно из условий задач. Задача должна быть не слишком сложной и решаться быстро и легко,
иначе ожидаемая прибыль будет ниже затрат (либо по времени будет слишком долго и прогноз потеряет актуальность).

Основные явления в эконометрических временных рядах:
* тренды
* сезонности
* разладки (смены модели ряда)

## Экспоненциальное скользящее среднее (ЭСС)

Простейшая регрессионная модель - константа $$\tilde{y}_{t+1}=c$$ наблюдения учитываются с весами, убывающими в прошлое:

$$\sum^{t}_{i=0}\beta^{t-i}(y_{i}-c)^{2}\rightarrow min_{c}$$, $$\beta \in (0,1)$$

Аналитическое реешение - формула Надарая-Ватсона:

$$c\equiv\tilde{y}_{t+1}=\frac{\sum^{t}_{i=0}\beta^{i}y_{t-i}}{\sum^{t}_{i=0}\beta^{i}}$$

Запишем аналогично $$\tilde{y}_{t}$$ и оценим $$\sum^{t}_{i=0}\beta^{i}\approx \sum^{\infty}_{i=0}\beta^{i}=\frac{1}{1-\beta}$$, получим $$\tilde{y}_{t+1}=\tilde{y}_{t}\beta+(1-\beta)\tilde{y}_{t}$$, заменим $$\alpha=1-\beta$$

$$\tilde{y}_{t+1}:=\tilde{y}_{t}+\alpha(y_{t}-\tilde{y}_{t})=\alpha\tilde{y}_{t}+(1-\alpha)\tilde{y}_{t}$$

$$\alpha\in(0,1)$$ - называется параметров сглаживания

если $$\alpha$$ приближается к 1, то такая величина считается большой и указывает на то, что завтра (следующее значение) будет такая же погода (прогноз по предыдущему значению). 

если $$\alpha$$ приближается к 0, $$\alpha$$ - маленький, тогда мы усредняем много предыдущих значений и примерно с одинаковыми весами, т.е. среднее повсему ряду (среднеарифмитическое). 

## Рекуррентная формула для среднего арифмитического 

Экспоненциальное скользящее среднее (ЭСС):

$$\tilde{y}_{t+1}:=\tilde{y}_{t}+\alpha(y_{t}-\tilde{y}_{t})$$

Среднее арифмитическое:

$$\tilde{y}_{t}+\frac{1}{t+1}(y_{t}-\tilde{y}_{t})$$

При $$\alpha=\frac{1}{t+1}$$ имеем среднее арифметическое
При $$\alpha=const$$ имеем экспоненциальное скользящее среднее

Вместо $$\alpha$$ можно взять значение между арифметическим и константой, в зависимости от условий. 

ЭСС подходит также и для нестационарных задач. 

К сожалению, для прикладных задач, эсс не всегда подходит, т.к. изменение рядов может происходить не постепенно (не стационарно), а очень резко. 

## Подбор параметра сглаживания

Существует такой эвристический метод для определения альфа, является ли он стационарным или не стационарным. Используется метод определения качества, функция от альфа ($$Q(\alpha)$$) решение которого является такое альфа, при которым значение квадрата ошибок является минимальным:

$$Q(\alpha)=\sum^{T_{1}}_{t=T_{0}}(\tilde{y}_{t}(\alpha)-y_{t})^{2}\rightarrow min_{\alpha}$$

Тогда $$\alpha^{*}$$ которое является решением функции Q:
1) если $$\alpha^{*}\in(0,0.3)$$, то ряд стационарен, ЭСС работает;
2) если $$\alpha^{*}\in(0.3,1)$$, то ряд нестационарен, нужна модель тренда. 

В 1) это не значит, что нельзя использовать более сложные модели, может есть какие то более сложные зависимости, которые ЭСС не в состоянии уловить. А более сложная модель и их опишет.

##Модель Хольта

Линейный тренд без сезонных эффектов:

$$y_{t+d}=a_{t}+b_{t}d$$

где $$a_{t}, b_{t}$$ - адаптивные коэффициенты линейного тренда

Рекуррентная формула:
$$a_t := \alpha_1 * y_t+(1-\alpha_1)(a_{t-1}+b_{t-1})$$
$$b_t := \alpha_2 * (a_t-a_{t-1})+(1-\alpha_2) * b_{t-1}$$

##Модель Тейла-Везджа

Линейный тренд с аддитивной сезонностью периода s:

$$\tilde{y}_{t+d}=(a_t+b_t d)+\theta_{t+(d mod s)-s}$$

$$a_t+b_t d$$ - тренд, очищенный от сезонных колебаний
$$\theta_{0},...,\theta_{s-1}$$ - сезонный профиль периода s.

Рекуррентная формула:

$$a_t := \alpha_1(y_t-\theta_{t-s})+(1-\alpha_1)(a_{t-1}+b_{t-1})$$
$$b_t := \alpha_2(a_t-a_{t-1})+(1-\alpha_2)b_{t-1}$$
$$\theta_t:=\alpha_3(y_t-a_t)+(1-\alpha_3)\theta_{t-s}$$

Модель состоит из вклада сезонных колебаний (например если неделя, то 7 коэффициентов) и вклада конкретного дня.

## Модель Уинтерса

Мультипликативная сезонность периода s:

$$\tilde{y}_{t+d}=a_t \theta_{t+(d mod s)-s}$$

$$\theta_{0},...,\theta_{s-1}$$ - сезонный профиль периода s.

Рекуррентная формула:

$$a_t := \alpha_1(y_t/\theta_{t-s})+(1-\alpha_1)a_{t-1}$$
$$\theta_t := \alpha_2(y_t/a_t)+(1-\alpha_2)\theta_{t-s}$$

Чтобы взять МНК, логарифмируем, получаем сумму, заменяем переменные и получаем решение МНК. 

нам нужна модель, которая оцениват насколько модель адекватно прогнозирует. Т.е. нам нужен определенный контроль

## Следящий контрольный сигнал

epsilon_t = y_t - y_t

K_t = epsilon1_t/epsilon2_t, где

epsilon1 - это скользящая средняя для самих ошибок
epsilon2 - это скользящая стредняя для модулей ошибок

Если есть систематическая ошибка, то будет K -> +/-1,
если K -> 0, то ошибки более или менее равномерны

если все нормально, то K_t должен быть маленьким и распределен по нормальному распределению

## Адаптивная композиция моделей
Еще, есть возможность использовать несколько лучших моделей. Использовать среднюю ошибку нескольких моделей

Это приводит нас к так называемому адаптивной композиции моделей. 
Пусть имеется k моделей прогнозирования
y_j,t+d - прогноз j-й модели на момент t+d
epsilon_jt = y_t - \tilde{y_jt} - ошибка прогноза в момент t, 
\tilde{epsilon} := \gamma|\epsilon_jt|+(1-\gamma)\tilde{\epsilon_jt} - экспоненциально сглаженная ошибка.

Линейная (выпуклая) комбинация моделей:

y_t+d = \sum^{k}_{j=1}w_jt\tilde{y}_j,t+d    \sum^{k}_{j=1}w_jt = 1

Адаптивный подбор весов:
w_jt = \frac{\tilde{\epsilon_jt}^{-1}}{\sum^{k}_{s=1}(\tilde{\epsilon_st})^{-1}}

Требуется подбор \gamma, рекомендация: \gamma = 0.01...0.1

Резюме:
* Адаптивные методы хорошо работают, когда рядов много, и прогнозировать их надо быстро;
* Простота адаптивных методов компенсируется селективными и композиционными моделями;
* При этом различные особенности рядов моделируются в базовых алгоритмах;
* Требование монотонности для композиции - мощный регуляризатор;
* Для более сложных рядов можно использовать адаптивные авторегрессионные модели. 


 















