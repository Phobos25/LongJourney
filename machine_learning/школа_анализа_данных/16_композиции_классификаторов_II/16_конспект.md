# Композиции классфикиаторов. Часть II

**Стохастические методы построения композиций.**

Чтобы алгоритмы в композиции были различными
* их обучают по (случайным) подвыборкам
* либо по (случаынйм) подмножеством признаков.
 
**Первую идею** реализует *bagging* (bootstrap aggregation) [Breiman, 1996], причем подвыборки берутся длины l с возвращениями, как в методе bootstrap.

**Вторую идею** реализует RSM (random subspace method) [Duin, 2002]

Совместим обе идеи в одном алгоритме. 

F = {$$f_1,...,f_n$$} --- признаки
$$\mu(G,U)$$ --- метод обучения алгоритма по подвыборке U $$\subseteq X^l$$ использующий только признаки из G $$\subseteq$$ F

## Бэггинг и метод случайных подпространств

**Вход:** обучающая выборка $$X^l$$; параметры T
l' --- длина обучающих подвыборок;
n' --- длина признакового подописания;
$$\varepsilon_1$$ --- порог качества базовых алгоритмов на обучении;
$$\varepsilon_2$$ --- порог качества базовых алгоритмов на контроле. 

**Выход:** базовые алгоритмы $$b_t$$, t = 1,...,T;

1) для всех t = 1,...,T;
2)  U:=случайное подмножество $$X^l$$ длины l';
3)  G:=случаное подмножество G длины n';
4)  $$b_t := \mu(G,U)$$;
5)  если Q($$b_t$$, U) > $$\varepsilon_1$$ или Q($$b_t$$, $$X^l$$\U) > $$\varepsilon_2$$
6)  не включать $$b_t$$ в композицию;

Композиция --- простое голосование a(x) = C $$\left( \sum^{T}_{t=1} b_t(x)\right)$$

Несмотря на то, что в алгоритме написано для всех T, мы не обязаны выполнять их по порядку. В отличие от бустинга мы можем обучать наши базовые алгоритым в любом порядке, даже параллельно друг другу, что является одним из преимуществ данного алгоритма. 

Мы выбираем случайное подмножество объектов, случайное подмножество признаков, их длины --- это довольно хитрый параметр, от выбора которого очень многое зависит. Выбор качества композиции, также, является важным, т.к. если мы выберем его слишком высоким, то обучение затянется на очень долгое время, если выбираем слишком маленький, то ошибки будут очень большие. 

После обучения базовых алгоритмов получились различными, они оубчались на разных подвыборках, и никак не связаны с друг другом. Над ними будет проделано простое голосование, это была основная идея основополагающих работ бэггинга и методе случайных подпространств. Но никто не запрещает использовать взвешенное голосование, например SVM.  Ведь построенные базовые алгоритмы мы можем трактовать как признаки и добавить к ним веса. Любой линейный классификатор может нам построить взвешенное голосование. 

### Сравнение: boosting - bagging - RSM

* Бустинг лучше для больших обучающих выборок и для классов с границами сложной формы; *Бустинг требователен к длине выборке, чем длиннее тем лучше*
* Бэггинг и RSM лучше для коротких обучающих выборок;
* RSM лучше в тех случаях, когда признаков больше, чем объектов, или когда много неинформативных признаков; *Данный метод хорошо, когда у нас очень много признаков, и он случайно может выдать нам очень хорошую комбинацию признаков*
* Бэггинг и RSM эффективно распараллеливаются, бустинг выполняется строго последовательно.
 
**И еще несколько эмпирических наблюдений:**
* Веса алгоритмов не столь важны для выравнивания отступов;*можно не брать веса, а использовать метод простого голосования.*
* Веса объектов не столь важны для обеспечения различности;
* Короткие композиции из "сильных" алгоритмов типа SVM строить труднее, чем длинные из слабых. 

12:19















