# Байесовская теория классификации II часть

## Линейный дискриминант Фишера

Допущение:
ковариационные матрицы классов равны: $$\sum_{y}=\sum, y\in Y$$

если рассматривать какой-нибудь объект $$x_{i}$$ то центром должен быть центр именно его класса $$y_{i}$$, другими словами $$\tilde{\mu}_{y_{i}}$$:
$$\tilde{\sum}=\frac{1}{l}\sum^{l}_{i=1}(x_{i}-\tilde{\mu}_{y_{i}})(x_{i}-\tilde{\mu}_{y_{i}})^{T}$$

Линейный дискримант - подстановочный алгоритм :

$$a(x)=argmax(x^{T}\alpha_{y}+\beta_{y})$$

## Проблема мультиколлинеарности

Проявления мультиколлинеарности:
* матрица $$\tilde{\Sigma}$$ близка к вырожденной;
* есть (приближённые) линейные зависимости признаков;
* есть собственные значения $$\tilde{\Sigma}$$, близкие к нулю;
* число обусловленности $$\mu(\tilde{\Sigma})=\frac{\lambda_{max}}{\lambda_{min}}>> 1$$

Последствия мультиколлинеарности:
* обратная матрица $$\tilde{\Sigma^{-1}}$$ неустойчива;
* относительные погрешности растут: если v = $$\tilde{\Sigma}^{-1}u$$.

## Регуляризация ковариационной матрицы

Идея:
преобразовать матрицу $$\tilde{\Sigma}$$ так, чтобы все собственные векторы v остались,
а все собственные значения $$\lambda$$ увеличились на $$\tau$$:
$$(\tilde{\Sigma} + \tau l_{n})v = \lambda v+ \tau v = (\lambda + \tau)v$$

Рецепт:
1) обращение $$\tilde{\Sigma} + \tau l_{n}$$ вместо $$\tilde{\Sigma}$$;
2) выбор параметра регуляризации $$\tau$$ по скользящему контролю. 

## Диагонализация ковариационной матрицы

Пусть все недиагональные элементы в матрице равны нулю: $$\sigma_{ij}=0, i\ne j$$
Что означает, что признаки не коррелированы
Замечание? для нормального распределения некоррелированность - это независимость. 

Получаем **наивный байесовский классификатор:**

$$\tilde{p_{j}(\xi|y)} = \frac{1}{\sqrt{2\pi\tilde{\sigma}_{yj}}}exp(-\frac{(\xi-\mu_{yj})^{2}}{2\tilde{\sigma}_{yj}^{2}}), y\in Y, j=1,..., n;$$

$$a(x) = argmax_{y\in Y}(ln\lambda_{y}\tilde{P}(y)+\sum^{n}_{j=1}ln\tilde{p_{j}(\xi|y)}, x=(\xi_{1},...,\xi_{n})$$

где $$\mu_{yj}$$ и $$\sigma_{yj}$$ - оценки среднего и дисперсии j-uj признака, вычисленные по $$X_{y}$$ - подвыборке класса y. 

a(x) - квадратичная оценка. 

## Редукция размерности по А.М. Шурыгину

Идея:
сведение n-мерной задачи к серии двумерных задач путем подключения признаков по одному. 

Набросок алгоритма:
1) найти два признака, в подпространстве которых классы наилучшим образом разделимы;
2) новый признак: $$\psi(x) = x^{T}\alpha_{y}$$ - проекция на нормаль к разделяющей прямой в пространстве двух признаков;
3) выбрать из оставшихся признаков тот, который в паре с $$\psi(x)$$ дает наилучшую разделимость;
4) если разделимость не улучшилась, прекратить;
5) иначе GOTO 2).

## Проблема выбросов (outliers)

Эмпирическое среднее является оценкой матожидания, неустойчивой к редким большим выбросам.
**Пример.** Одномерная нормальная плотность N (0,1), загрязненная равномерным на [-20,+20] распределением, l = 50, смещение эмпирического среднего 0.359. 

Из этого следует, что нужно игнорировать эти редкие выбросы. Для этого используются разные робастные (robust) методы, одним из которых является, к примеру, медиана.  
либо использовать цензурирование выборки (отсев выбросов)

Идея: задача решается дважды; после первого раза объекты с наибольшими ошибками исключаются из обучения. 
Алгоритм заключается вот в чем: мы рассматриваем распределения ошибок, по методу максимума правдоподобия, в случае, если значения резко отличаются, например в хвосте, мы их выкидываем и решаем ту же задачу заново, но уже с меньшими точками. 

Алгоритм (для задачи восстановления плотности):
1) оценить параметр $$\tilde{\theta}$$ оп всей выборке $$X^{m}$$;
2) вычислить правдоподобия $$\pi_{i} = \phi(x_{i};\tilde{\theta})$$ для всех $$x_{i} \in X^{m}$$;
3) отсортировать выборку по убыванию: $$\pi_{1}\geq ...\geq \pi_{m}$$;
4) удалить из $$X^{m}$$ объекты, попавшие в конец ряда;
5) оценить параметр $$\tilde{\theta}$$ по укороченной выборке $$X^{m}$$. 










