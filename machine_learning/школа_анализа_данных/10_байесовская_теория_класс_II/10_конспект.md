# Байесовская теория классификации II часть

## Линейны дискриминант Фишера

Допущение:
ковариационные матрицы классов равны: $$\sum_{y}=\sum, y\in Y$$

если рассматривать какой-нибудь объект $$x_{i}$$ то центром должен быть центр именно его класса $$y_{i}$$, другими словами $$\tilde{\mu}_{y_{i}}$$:
$$\tilde{\sum}=\frac{1}{l}\sum^{l}_{i=1}(x_{i}-\tilde{\mu}_{y_{i}})(x_{i}-\tilde{\mu}_{y_{i}})^{T}$$

Линейный дискримант - подстановочный алгоритм :

$$a(x)=argmax(x^{T}\alpha_{y}+\beta_{y})$$

## Проблема мулитиколлинеарности

Проявления мулитиколлинеарности:
* матрица $$\tilde{\Sigma}$$ близка к вырожденной;
* есть (приближённые) линейные зависимости признаков;
* есть собственные значения $$\tilde{\Sigma}$$, близкие к нулю;
* число обусловленности $$\mu(\tilde{\Sigma})=\frac{\lambda_{max}}{\lambda_{min}}>> 1$$

Последствия мулитиколлинеарности:
* обратная матрица $$\tilde{\Sigma^{-1}}$$ неустойчива;
* относительные погрешности растут: если v = $$\tilde{\Sigma}^{-1}u$$.

33:00