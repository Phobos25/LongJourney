# Байесовская теория классификации III часть
Даже если классы очень сложны будет существовать достаточное число случаев при котором
разделяющая кривая будет представлять собой линейную плоскость. 

## Теорема о линейности байесовского классификатора

Оптимальный байесовский классификатор для двух классов:
$$a(x) = sign (\lambda_{+}P(+1|x) - \lambda_{-}P(-1|x)) = sign(\frac{p(x|+1)P(+1)}{p(x|-1)P(-1)}-\frac{\lambda{-}}{\lambda{+}})$$
где $$P(+1|x)$$ - вероятность, что класс +1 будет во множестве x. 
где $$P(-1|x)$$ - вероятность, что класс -1 будет во множестве x. 

Теорема:
Если $$p_{y}$$ экспонентны, параметры d(.) и $$\delta$$ не зависят от y, и среди признаков f1(x),...,fn(x) есть константа, то байесовский классификатор линеен:
$$a(x) = sign(<w,x> - w_{0}), w_{0} = ln(\lambda_{-}/\lambda_{+})$$
апостериорные вероятности классов:
$$P(y|x)=\sigma(<w,x>y)$$, 
где $$\sigma(z)=\frac{1}{1+e^{-z}}$$ - логистическая (сигмоидная) функция. 

сигмоидная функция это такая функция, которая принимает значения от 0 до 1, при 0 принимает значение 1/2. 

Апостериорная вероятность, здесь - это margin (маржа). 

Доказывается теорема двумя шамагми:

Шаг 1:
После подстановки экспонентных плотностей классов
$$p(x|\pm1)=exp(c_{\pm}(\delta)<\theta_{\pm},x>+b_{\pm}(\delta,\theta_{\pm})+d(x,\delta))$$

в формулу байесовского классификатора, мы логарифмируем, чтобы упростить расчеты,
получаем
$$ln\frac{P(+1|x)}{P(-1|x)}=<c(\delta)(\theta_{+}-\theta_{-}),x>+b_{+}(\delta,\theta_{+})-b_{-}(\delta,\theta_{-})+ln\frac{p_{+}}{p_{-}}$$
см. вкладку

Получается, что отношение апостериорных вероятностей +1 и -1 по x есть ничто иное как экспонента скалярных произведений w и x. Где w - это параметр алгоритма классификации (обучаемый)

18:49