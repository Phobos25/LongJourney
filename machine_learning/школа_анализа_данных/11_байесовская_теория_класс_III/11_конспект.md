# Байесовская теория классификации III часть
Даже если классы очень сложны будет существовать достаточное число случаев при котором
разделяющая кривая будет представлять собой линейную плоскость. 

## Теорема о линейности байесовского классификатора

Оптимальный байесовский классификатор для двух классов:
$$a(x) = sign (\lambda_{+}P(+1|x) - \lambda_{-}P(-1|x)) = sign(\frac{p(x|+1)P(+1)}{p(x|-1)P(-1)}-\frac{\lambda{-}}{\lambda{+}})$$
где $$P(+1|x)$$ - вероятность, что класс +1 будет во множестве x. 
где $$P(-1|x)$$ - вероятность, что класс -1 будет во множестве x. 

Теорема:
Если $$p_{y}$$ экспонентны, параметры d(.) и $$\delta$$ не зависят от y, и среди признаков f1(x),...,fn(x) есть константа, то байесовский классификатор линеен:
$$a(x) = sign(<w,x> - w_{0}), w_{0} = ln(\lambda_{-}/\lambda_{+})$$
апостериорные вероятности классов:
$$P(y|x)=\sigma(<w,x>y)$$, 
где $$\sigma(z)=\frac{1}{1+e^{-z}}$$ - логистическая (сигмоидная) функция. 

сигмоидная функция это такая функция, которая принимает значения от 0 до 1, при 0 принимает значение 1/2. 

Апостериорная вероятность, здесь - это margin (маржа). 

Доказывается теорема двумя шамагми:

Шаг 1:
После подстановки экспонентных плотностей классов
$$p(x|\pm1)=exp(c_{\pm}(\delta)<\theta_{\pm},x>+b_{\pm}(\delta,\theta_{\pm})+d(x,\delta))$$

в формулу байесовского классификатора, мы логарифмируем, чтобы упростить расчеты,
получаем
$$ln\frac{P(+1|x)}{P(-1|x)}=<c(\delta)(\theta_{+}-\theta_{-}),x>+b_{+}(\delta,\theta_{+})-b_{-}(\delta,\theta_{-})+ln\frac{p_{+}}{p_{-}}$$
см. вкладку

Получается, что отношение апостериорных вероятностей +1 и -1 по x есть ничто иное как экспонента скалярных произведений w и x. Где w - это параметр алгоритма классификации (обучаемый)

В кредитном скоринге наиболее часто используется логистический метод. 
Особенно, если используется бинаризация признаков. 


## Задача восстановления смеси распределений

Порождающая модель смеси распределений:

$$p(x) = \sum^{k}_{j=1}w_{j}p_{j}(x;\theta_{j})  \sum^{k}_{j=1}w_{j}=1, w_{j}\geq 0$$

$$p_{j}(x;\theta_{j})$$ - функция правдоподобия j-й компоненты смеси; $$w_[j]$$ - её априорная вероятность; k - число компонент смеси. 


## Итерационный алгоритм Expectation-Maximization:
1) начальное приближение вектора параметров $$\Theta$$;
2) **повторять**
3)      G:=E-шаг $$(\Theta)$$; // оцениваются скрытые переменные G
4)      $$\Theta$$:=М-шаг $$(\Theta, G)$$;
5) **пока** $$\Theta$$ и G не стабилизируются. 

ЕМ алгоритм - это метод простый итераций. 

Е-шаг - это формула Байеса:

$$g_{ij} = P(j|x_{i}) = \frac{P_{j}p(x_{i|j})}{p(x_{i})}=\frac{w_{j}p_{j}(x_{i};\theta_{j})}{p(x_{i})}=\frac{w_{j}p_{j}(x_{i};\theta_{j})}{\sum^{k}_{s=1}w_{s}p_{s}(x_{i};\theta_{s})}$$

Очевидно, выполнено условие нормировки: $$\sum^{k}_{j=1}g_{ij}=1$$

М-шаг - это максимизация взвешенного правдоподобия, с весами объектов $$g_{ij}$$ для j-й компоненты смеси:

$$\theta_{j}=argmax_{\theta}\sum^{m}_{i=1}g_{ij}\ln p_{j}(x_{i};\theta)$$

$$w_{j}=\frac{1}{m}\sum^{m}_{i=1}g_{ij}$$

## ЕМ-алгоритм

Вход: $$X^{m}$$={$$x_{1},...,x_{m}$$}, k $$\delta$$, начальное $$\Theta = (w_{j},\theta_{j})^{k}_{j=1}$$
Выход: $$\Theta = (w_{j},\theta_{j})^{k}_{j=1}$$ - параметры смеси распределений
1: повторять
2:      Е-шаг (expectation):
        для всех i=1,...,m, j=1,...,k
        $$g_{ij}^{0}:=g_{ij}$$;  $$g_{ij}:=\frac{w_{j}p_{j}x_{i};\theta_{j}}{\sum^{k}_{s=1}w_{s}p_{s}x_{i};\theta_{s}}$$

3:      М-шаг (maximization):
        для всех j=1,...,k
            $$\theta_{j}:=argmax_{\theta}\sum^{m}_{i=1}g_{ij}\ln p_{j}(x_{i};\theta)$$;
            $$w_{j}:=\frac{1}{m}\sum^{m}_{i=1}g_{ij}$$

4:      **пока** $$max_{i,j}|g_{ij}-g_{ij}^{0}|>\delta$$
5:      **вернуть** $$(w_{j},\theta_{j})^{k}_{j=1}$$

Вначале, алгоритм (Е-шаг) считает вероятность того, к какому классу принадлежит объект (к какому классу ближе по значению, см example_EM_algorithm). Поэтому выкрашивает соответственно классам. 
Далее, алгоритм (М-шаг) рассчитывает новые значения для классификатора классов, т.е. по
синим объектам считает и по красным объектам. В результате мы получаем картину показанной на 3-м рисунке сверху. 

Таким образом, алгоритм продолжает считать и оценивать пока не достигнет минимума. 

Следует иметь в виду, что при неудачных начальных условиях четко классы не будут выделены и объекты будут классифицированы неправильно. 

Если компонент слишком мало, то обучение будет слишком грубое и ошибки будут очень большие

Если слишком много, то модель переобучиться, и на тестовых данных модель провалится. 

Поэтому всегда есть оптимальное число компонент классификаций, для этого и используется скользящий контроль. 

## GEM - обобщенный EM-алгоритм

Идея:
Не обязательно добиваться высокой точности на М-шаге. Достаточно лишь сместиться в направлении максимума, сделав одну или несколько итераций, и затем выполнить Е-шаг

Преимущество:
уменьшение времени работы при сопоставимом качестве решения. 

**После Е-шага, в М-шаге. М-шаг итеративный, можно использовать меньшее число итераций (2-3 достаточно)**

## SEM - стохастический ЕМ-алгоритм

**Идея:** на М-шаге вместо максимизации 

$$\theta_{j}:= argmax_{\theta}\sum^{m}_{i=1}g_{ij}\ln p_{j}(x_{i};\theta)$$

максимизируется обычное, невзвешенное, правдоподобие:

$$\theta_{j}:= argmax_{\theta}\sum_{x_{i}\in X_{j}}\ln p_{j}(x_{i};\theta)$$

где $$p_{j}(x_{i};\theta)$$ - вероятность того, что $$\theta$$ придет из подмножества $$x_{i}$$
Выборки X_{j} строятся путём стохастического моделирования:
для каждого i=1,...,m генерируется j $$\sim P(\theta_{j}|x_{i})\equiv g_{ij}$$ и объект $$x_{i}$$ помещается в $$X_{j}$$. 

**Преимущества:**
ускорение сходимости, предотвращение зацикливаний. 

Получается, что j-е подмножество получается методом сэмплирования всего множества. 

## HEM - иерархический ЕМ-алгоритм

**Идея:**
"Плохо описанные" компоненты расщепляются на две или более *дочерних* компонент.

**Преимущество:**
автоматически выявляется иерархическая структура каждого класса, которую затем можно интерпретировать содержательно. 

В качестве преимущества мы получаем более детальную информацию о классе. С одной стороны - это может помочь при оптимизации алгоритма. С другой стороны - это не для всех задач нужно. 

## Гауссовская смесь с диагональными матрицами ковариации
Рассмотрим частный случай:

Гауссовская смесь GMM - Gaussian Mixture Model
Допущения:
* 1) функции правдоподобия классов p(x|y) представимы в виде смесей $$k_{y}$$ компонент, y $$\in$$ Y = {1,...,M}.
* 2) Компоненты имеют n-мерные гауссовский плотности с некоррелированными признаками:

$$\mu_{yj} = (\mu_{yj1},...,\mu_{yjn})$$, $$\sum_{yj}=diag(\sigma^{2}_{yj1},..., \sigma^{2}_{yjn}), j=1,...k_{y}$$

$$p(x|y)=\sum^{k_{y}}_{j=1}w_{yjp_{yj}}(x), p_{yj}(x)=N(x;\mu_{yj1}, \Sigma_{yj})$$

$$\sum^{k_{y}}_{j=1}w_{yj}=1, w_{yj}\geq 0$$

## Эмпирические оценки средних и дисперсий

Числовые признаки: $$f_{d}:X\rightarrow R, d=1,...,n$$ 

Решение задачи М-шага:
Для всех классов $$y\in Y$$ и всех компонент $$j=1,...,k_{y}$$

$$w_{yj=\frac{1}{l_{y}}}\sum_{i:y_{i}=y}g_{yij}$$

Для всех размерностей (признаков) d=1,...,n 

$$\tilde{\mu}_{yjd} = \frac{1}{l_{y}w_{yj}}\sum_{i:y_{i}=y}g_{yij}f_{d}(x_{i})$$

$$\tilde{\sigma}^{2}_{yjd} = \frac{1}{l_{y}w_{yj}}\sum_{i:y_{i}=y}g_{yij}(f_{d}(x_{i})-\tilde{\mu}_{yjd})^{2}$$

Замечание: компоненты "наивны", но смесь не "наивна"

В шаге М решение такое же, как и у "наивного" байесовского метода, находим матожидание (среднее) и дисперсию. Но в этом случае мы находим средние и дисперсии для смеси кмпонент. И каждый компонент со своим весом

для алгоритма классификации см. рис. class_algorithm

Здесь, классификатор специально написан в таком виде, чтобы показать сходство с линейными классификаторами.

В линейных классификаторах использовались эталоны, от которых рассчитывались расстояния до объектов. 
В этом случае используются компоненты, которые по сути те же эталоны (центры классов от которых рассчиытваются расстояния).



