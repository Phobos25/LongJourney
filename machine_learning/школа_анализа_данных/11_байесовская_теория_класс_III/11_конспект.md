# Байесовская теория классификации III часть
Даже если классы очень сложны будет существовать достаточное число случаев при котором
разделяющая кривая будет представлять собой линейную плоскость. 

## Теорема о линейности байесовского классификатора

Оптимальный байесовский классификатор для двух классов:
$$a(x) = sign (\lambda_{+}P(+1|x) - \lambda_{-}P(-1|x)) = sign(\frac{p(x|+1)P(+1)}{p(x|-1)P(-1)}-\frac{\lambda{-}}{\lambda{+}})$$
где $$P(+1|x)$$ - вероятность, что класс +1 будет во множестве x. 
где $$P(-1|x)$$ - вероятность, что класс -1 будет во множестве x. 

Теорема:
Если $$p_{y}$$ экспонентны, параметры d(.) и $$\delta$$ не зависят от y, и среди признаков f1(x),...,fn(x) есть константа, то байесовский классификатор линеен:
$$a(x) = sign(<w,x> - w_{0}), w_{0} = ln(\lambda_{-}/\lambda_{+})$$
апостериорные вероятности классов:
$$P(y|x)=\sigma(<w,x>y)$$, 
где $$\sigma(z)=\frac{1}{1+e^{-z}}$$ - логистическая (сигмоидная) функция. 

сигмоидная функция это такая функция, которая принимает значения от 0 до 1, при 0 принимает значение 1/2. 

Апостериорная вероятность, здесь - это margin (маржа). 

Доказывается теорема двумя шамагми:

Шаг 1:
После подстановки экспонентных плотностей классов
$$p(x|\pm1)=exp(c_{\pm}(\delta)<\theta_{\pm},x>+b_{\pm}(\delta,\theta_{\pm})+d(x,\delta))$$

в формулу байесовского классификатора, мы логарифмируем, чтобы упростить расчеты,
получаем
$$ln\frac{P(+1|x)}{P(-1|x)}=<c(\delta)(\theta_{+}-\theta_{-}),x>+b_{+}(\delta,\theta_{+})-b_{-}(\delta,\theta_{-})+ln\frac{p_{+}}{p_{-}}$$
см. вкладку

Получается, что отношение апостериорных вероятностей +1 и -1 по x есть ничто иное как экспонента скалярных произведений w и x. Где w - это параметр алгоритма классификации (обучаемый)

В кредитном скоринге наиболее часто используется логистический метод. 
Особенно, если используется бинаризация признаков. 


## Задача восстановления смеси распределений

Порождающая модель смеси распределений:

$$p(x) = \sum^{k}_{j=1}w_{j}p_{j}(x;\theta_{j})  \sum^{k}_{j=1}w_{j}=1, w_{j}\geq 0$$

$$p_{j}(x;\theta_{j})$$ - функция правдоподобия j-й компоненты смеси; $$w_[j]$$ - её априорная вероятность; k - число компонент смеси. 


## Итерационный алгоритм Expectation-Maximization:
1) начальное приближение вектора параметров $$\Theta$$;
2) **повторять**
3)      G:=E-шаг $$(\Theta)$$; // оцениваются скрытые переменные G
4)      $$\Theta$$:=М-шаг $$(\Theta, G)$$;
5) **пока** $$\Theta$$ и G не стабилизируются. 

ЕМ алгоритм - это метод простый итераций. 

Е-шаг - это формула Байеса:

$$g_{ij} = P(j|x_{i}) = \frac{P_{j}p(x_{i|j})}{p(x_{i})}=\frac{w_{j}p_{j}(x_{i};\theta_{j})}{p(x_{i})}=\frac{w_{j}p_{j}(x_{i};\theta_{j})}{\sum^{k}_{s=1}w_{s}p_{s}(x_{i};\theta_{s})}$$

Очевидно, выполнено условие нормировки: $$\sum^{k}_{j=1}g_{ij}=1$$

М-шаг - это максимизация взвешенного правдоподобия, с весами объектов $$g_{ij}$$ для j-й компоненты смеси:

$$\theta_{j}=argmax_{\theta}\sum^{m}_{i=1}g_{ij}\ln p_{j}(x_{i};\theta)$$

$$w_{j}=\frac{1}{m}\sum^{m}_{i=1}g_{ij}$$


46:00