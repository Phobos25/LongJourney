# Активное обучение. Active Learning

* Сэмплирование из коллекции (Pool based sampling)
* Сэмплирование из потока (Stream based selective sampling)
* Генерация запросов (Query synthesis)
 
Активное обучение обычно применяется в поисковых системах. Т.к. для таких систем реально использовать людей для калибровки нереально, то используется активное обучение. Это когда резульатты обучения используются далее для создания ответов к признакам. 

## AL: Общая схема

1) выбрать случайно L $$\subseteq$$ U
2) оценить оракулом все точки из L
3) $$\theta \leftarrow$$ оптимальный векто параметров модели, обученной на L
4) пока L недостаточно велико
4.1. x* = $$argmax_{x\in U \}$$
4.2. y* $$\leftarrow$$ оценки выбранного примера оракулом
4.3. L = L U <x*, y*>
4.4. $$\theta \leftarrow$$ оптимальный вектор параметров на изменившемся L

$$\phi_{0}$$ --- функционал-эвристика, оценивающий прирост качества модели при добавлении точки x к обучающему множеству. 

Тут следует иметь в виду, что цикл обучения почти бесконечен, т.е. он будет идти до тех пор, пока мы не скажем, что модель обучилась достаточно (прерывание инженером). 4.2 Находим некое значение y*, при котором у нас оценка улучшается максимально, и даем нашему оракулу оценить его вручную. Надо иметь в виду, что в шаге 4.1 разумнее брать несколько или целый набор точек. 

Один из примеров это **Uncertainly sampling**
Данный метод лучше всего рассматривать на примере бинарной классификации, т.е. когда наша модель возвращает не только 0 или 1, но и вероятность принадлежности к классам. В этом случае, мы будем отбирать те данные у которых вероятность близка к 0.5, отбирать их из выборки и отдавать нашему оракулу для ручной проверки. При таком подходе/эвристике наша модель будет обучаться наиболее быстро. 

В зависимости от модели использовать точку с наименьшей вероятностью может быть недостаточно обоснованным. К примеру SVM, у которого вероятность будет чем ниже, чем точка будет ближе к разделяющей плоскости. Ценность таких точек будет очень низкой, поэтому имеет смысл вводить взвешиваюмую плотность. 

$$x* = argmax_{x}(\phi_\theta(x) - (\frac{1}{|U|}\sum_{x^\theta\in U}/rho(x,x^\mu)^\beta)$$
где $$\rho(x,y)$$ --- мера близости объектов, например косинусная метрика
$$\beta$$ --- нормирующий коэффициент 

# Query by Committee
Идея заключается в том, чтобы у нас была не одна модель, а целый набор моделей --- комитет. Мы отбираем те объекты, которые уменьшает пространство моделей. Т.е. уменьшает различные способы фита и интерпретации данных. 

Также, используется усредненная дивергенция Кульбакка-Лейблера. Эта мера оценки близости распределений, чем больше дивергенция тем дальше. 

В задачах регрессии мы будем использовать дисперсию комитета, данные с максимальной дисперсией мы будем отсылать оракулу. 

# AL: Ожидаемое уменьшение ошибки (Expected Error Reduction)

Выбираем точку, максимизирующий уверенность на остальных примерах. 

#AL - обзор

* Выбор по степени неуверенности
* Отбор комитетом
* Максимизация влияния на модель
* Ожидаемое уменьшение ошибки
* Уменьшение дисперции

