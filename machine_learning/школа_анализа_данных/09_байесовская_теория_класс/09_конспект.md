# байесовская теория классификации

допустим мы знаем, плотность распределения p(x,y)

p(x,y) = p(x)P(y|x)=P(y)p(x|y)

P - вероятность (большие буквы)
p - плотность  (маленькие буквы)

P(y) - означает, что объекты приходят с некоторой вероятностью относится к классу y (априорная вероятность)
P(y|x) - апостериорная вероятность класса y. Вероятность, которая была выставлена после ознакомления с данным x

Априорная вероятность выставляется сразу, даже если нам нчиего не известно о конкретный данных. 
Пр.
Допустим, что по данным банка мы знаем, что 1 из 11 заемщиков является недобросовестным и деньги скорее всего не вернет. Это
априорная вероятность. 
Если к нам подходит человек, а мы про него ничего не знаем, то с вероятность 1 из 11 он недобросовестный. 
Если окажется, что он уже брал кредиты и возвращал, то у него апостериорная вероятность 10 из 11.
Если окажется, что у него несколько судимостей, не погашенные кредиты и т.п., то он является недобросовестным, как раз 1 из 11

плотность распределения p(x) является общей, если мы рассматриваем совокупность распределений p(x) = p(x)хор) + p(x)плох). у
плохих и хороший плотность распределения своя, но при совместном рассмотрении они должны давать p(x). 

Оптимальный байесовский классификатор

Теорема 
если известны P(y) и p(x|y), то минимальный средний риск R(a) имеет байесовский классификатор

a(x) argmin\sum_{s\inY}\lambda_{ys}P(y)p(x|y)

\lambda_P{ys} -  учет потерь, если мы перепутали класс y с s. 

Классификатор может быть оптимальным, только если мы правильно отпределили принажлежность к классам.

При обучении модели следует иметь в виду. что выборка может быть сделана не правильно. Например, если у нас два класса, но один класс
превышает другой в 100 раз, но в выборке они отобраны поровну. В таком случае, выборка была сделана неправильно. 

Для решения такой задачи, надо оценить плотность вероятности 

## Три подхода к оцениванию плотностей

1) Параметрическое оценивание плотности:
$$p(x) = \phi(x,\theta)$$

2) Восстановление смеси распределений:
$$p(x)=\sum^{k}_{j=1}w_{j}\phi(x,\theta_{j}), k<<m$$

3) Непараметрическое оценивание плотности:
$$p(x) = \sum^{m}_{i=1}\frac{1}{mV(h)}K(\frac{\rho(x,x_{i})}{h})$$

Рассмотрим первый подход. Для оценки плотности вероятностей используется некая функция фи, которую мы вводим сами. Из недостатков можно 
выделить то, что мы можем совсем промахнемся с функцией и она будет оценивать неправильно. Из преимуществ, какая-либо функция гораздо лучше,
чем вообще без функций. 

Рассматривать будем 3й случай для одномерного случая, или т.н. наивный байесовский классификатор

Первое допущение - это то, что данные случайные величины независимы.

надо выбирать окно достаточного размера, чтобы оценка плотности не вырождалась. 
Если окно слишком большое, то оценка плотности выпрямляется, и будует оцениваться плохо.

# Обоснование оценки Парзена_розенблатта
1) Пусть плотность распределения пришла этой выборки; (обязательное условие)
2) ядро непрерывно и ограничено, убывает быстро, но не слишком быстро
3) С ростом числа выборки, уменьшаем ширину окна до нуля. Но не слишком быстр ои не слишком медленно. 


Для многомерного случая наивный байесовский подход не работает

Поэтому ядро надо сделать многомерным. 
Многомерное ядро = произведение одномерных ядер. 

1) $$\lambda{p}(x) = \frac{1}{m}\sum^{m}_{i=1}\prod^{n}_{j=1}\frac{1}{h_{j}}K\left(\frac{f_{j}(x)-f_{j}(x_{i})}{h_{j}}\right)$$

Надо иметь в виду что данное выражение **не** является наивным байесовским методом, т.к. стоит сумма произведений. Для наивного было бы произведение сумм, большая разница. 
Еще одним отличием является то, что мы не предполагаем независимость, мы ничего не предполагаем, мы просто выбираем ядро. 

Если на X задана функция расстояния $$\rho(x, x')$$
2) $$\lambda{p}_{h}(x) = \frac{1}{mV(h)}\sum^{m}_{i=1}K\left( \frac{\rho(x,x_{i}}{h}\right)$$

где $$V(h) = \int_{X}K\frac{\rho(x,x_{i})}{h}dx$$ - нормирующий множитель

Здесь, V(h) - это такой шар на точке x, у которого объем не меняется от того, в какую точку x мы его поместили (пространство должно быть однородным).

Если ядро K гауссовское а метрика $$\rho$$ - евклидово , то уравнения 1 и 2 совпадут. 

# Метод парзеновского окна

Парзеновская оценка плотности для каждого класса $$y\in Y$$:

$$\lambda{p}_{h}(x|y) = \frac{1}{l_{y}V(h)}\sum_{i:y_{i}=y}K(\frac{\rho(x,x_{i})}{h})$$

Метод парзеновского окна (Parzen window):

$$a(x;X^{l},h)=argmax_{y\in Y}\lambda_{y}\frac{P(y)}{l_{y}}\sum_{i:y_{i}=y}K(\frac{\rho(x,x_{i})}{h})$$

Во втором выражении V(h) выбрасывается, т.к. есть такое свойство, по которому в argmax аргументы, которые не зависят от argmax выбрасываются. 
Здесь, K - это функция близости. Чем ближе объект тем больше значение и наоборот, чем дальше тем ниже. K - убывающая функция.

## Выбор метрики (функция расстояния):
Несмотря на то, что метод у нас был непараметрический, т.е. безмодельный. Нам все равно надо выбрать модель функции расстояния или метрику
Одним из способов выбора метрики является взвешенная метрика Минковского:
$$\rho(x,x')=(\sum^{n}_{j=1}w_{j}|f_{j}(x)-f_{j}(x')|^{p})^{1/p}$$
где $$w_{j}$$ - неотрицательные веса признаков, p>0.

если w = 1, p= 2, то получим евклидову метрику.
 
58:00



