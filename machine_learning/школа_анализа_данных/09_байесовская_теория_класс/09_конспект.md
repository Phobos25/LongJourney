# байесовская теория классификации

допустим мы знаем, плотность распределения p(x,y)

p(x,y) = p(x)P(y|x)=P(y)p(x|y)

P - вероятность (большие буквы)
p - плотность  (маленькие буквы)

P(y) - означает, что объекты приходят с некоторой вероятностью относится к классу y (априорная вероятность)
P(y|x) - апостериорная вероятность класса y. Вероятность, которая была выставлена после ознакомления с данным x

Априорная вероятность выставляется сразу, даже если нам нчиего не известно о конкретный данных. 
Пр.
Допустим, что по данным банка мы знаем, что 1 из 11 заемщиков является недобросовестным и деньги скорее всего не вернет. Это
априорная вероятность. 
Если к нам подходит человек, а мы про него ничего не знаем, то с вероятность 1 из 11 он недобросовестный. 
Если окажется, что он уже брал кредиты и возвращал, то у него апостериорная вероятность 10 из 11.
Если окажется, что у него несколько судимостей, не погашенные кредиты и т.п., то он является недобросовестным, как раз 1 из 11

плотность распределения p(x) является общей, если мы рассматриваем совокупность распределений p(x) = p(x)хор) + p(x)плох). у
плохих и хороший плотность распределения своя, но при совместном рассмотрении они должны давать p(x). 

Оптимальный байесовский классификатор

Теорема 
если известны P(y) и p(x|y), то минимальный средний риск R(a) имеет байесовский классификатор

a(x) argmin\sum_{s\inY}\lambda_{ys}P(y)p(x|y)

\lambda_P{ys} -  учет потерь, если мы перепутали класс y с s. 

Классификатор может быть оптимальным, только если мы правильно отпределили принажлежность к классам.

При обучении модели следует иметь в виду. что выборка может быть сделана не правильно. Например, если у нас два класса, но один класс
превышает другой в 100 раз, но в выборке они отобраны поровну. В таком случае, выборка была сделана неправильно. 

Для решения такой задачи, надо оценить плотность вероятности 

## Три подхода к оцениванию плотностей

1) Параметрическое оценивание плотности:
$$p(x) = \phi(x,\theta)$$

2) Восстановление смеси распределений:
$$p(x)=\sum^{k}_{j=1}w_{j}\phi(x,\theta_{j}), k<<m$$

3) Непараметрическое оценивание плотности:
$$p(x) = \sum^{m}_{i=1}\frac{1}{mV(h)}K(\frac{\rho(x,x_{i})}{h})$$

Рассмотрим первый подход. Для оценки плотности вероятностей используется некая функция фи, которую мы вводим сами. Из недостатков можно 
выделить то, что мы можем совсем промахнемся с функцией и она будет оценивать неправильно. Из преимуществ, какая-либо функция гораздо лучше,
чем вообще без функций. 

Рассматривать будем 3й случай для одномерного случая, или т.н. наивный байесовский классификатор

Первое допущение - это то, что данные случайные величины независимы.

22:00



 




