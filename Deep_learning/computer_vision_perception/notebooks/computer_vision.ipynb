{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "783ba672",
   "metadata": {},
   "source": [
    "### Fully convolutional network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0664b5",
   "metadata": {},
   "source": [
    "Image -> FCN -> 01\n",
    "\n",
    "После обработки изображения FCN мы получаем матрицу состояющую из 0 и 1, где 1 --- это найденные, и распознанные объекты, 0 --- все остальное"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b4e903",
   "metadata": {},
   "source": [
    "Для того, чтобы понять как рботает FCN надо понять, что такое upsampling.\n",
    "\n",
    "**Upsampling** --- это метод увеличения размера рисунка\n",
    "\n",
    "Существует несколько методов: **bed of nails**, **nearest neigbours**, **interpolation**, **transpose convolution**\n",
    "\n",
    "interpolation использует средние значения для заполнения нового рисунка\n",
    "\n",
    "для transpose convolution используются фильтры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "936425cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Concatenate \n",
    "from tensorflow.keras.layers import Input, Add, Conv2DTranspose\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, MeanSquaredError, BinaryCrossentropy\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "from  matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc40178f",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8607c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load directories\n",
    "\n",
    "train_data_dir = '../data/training/image_2/'\n",
    "train_gt_dir   = '../data/training/gt_image_2/'\n",
    "\n",
    "test_data_dir  = '../data/testing/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6e5278d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Examples: 231\n",
      "Number of Validation Examples: 28\n",
      "Number of Testing Examples: 30\n"
     ]
    }
   ],
   "source": [
    "# Number of training examples\n",
    "TRAINSET_SIZE = int(len(os.listdir(train_data_dir)) * 0.8)\n",
    "print(f\"Number of Training Examples: {TRAINSET_SIZE}\")\n",
    "\n",
    "VALIDSET_SIZE = int(len(os.listdir(train_data_dir)) * 0.1)\n",
    "print(f\"Number of Validation Examples: {VALIDSET_SIZE}\")\n",
    "\n",
    "TESTSET_SIZE = int(len(os.listdir(train_data_dir)) - TRAINSET_SIZE - VALIDSET_SIZE)\n",
    "print(f\"Number of Testing Examples: {TESTSET_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d0974c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Constants\n",
    "IMG_SIZE = 128\n",
    "N_CHANNELS = 3\n",
    "N_CLASSES = 1\n",
    "SEED = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4656114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load image and return a dictionary\n",
    "def parse_image(img_path: str) -> dict:\n",
    "    image = tf.io.read_file(img_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.uint8)\n",
    "\n",
    "    # Three types of img paths: um, umm, uu\n",
    "    # gt image paths: um_road, umm_road, uu_road\n",
    "    mask_path = tf.strings.regex_replace(img_path, \"image_2\", \"gt_image_2\")\n",
    "    mask_path = tf.strings.regex_replace(mask_path, \"um_\", \"um_road_\")\n",
    "    mask_path = tf.strings.regex_replace(mask_path, \"umm_\", \"umm_road_\")\n",
    "    mask_path = tf.strings.regex_replace(mask_path, \"uu_\", \"uu_road_\")\n",
    "    \n",
    "    mask = tf.io.read_file(mask_path)\n",
    "    mask = tf.image.decode_png(mask, channels=3)\n",
    "    \n",
    "    non_road_label = np.array([255, 0, 0])\n",
    "    road_label = np.array([255, 0, 255])\n",
    "    other_road_label = np.array([0, 0, 0])\n",
    "    \n",
    "    # Convert to mask to binary mask\n",
    "    mask = tf.experimental.numpy.all(mask == road_label, axis = 2)\n",
    "    mask = tf.cast(mask, tf.uint8)\n",
    "    mask = tf.expand_dims(mask, axis=-1)\n",
    "\n",
    "    return {'image': image, 'segmentation_mask': mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7c4bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset variables\n",
    "all_dataset = tf.data.Dataset.list_files(train_data_dir + \"*.png\", seed=SEED)\n",
    "all_dataset = all_dataset.map(parse_image)\n",
    "\n",
    "train_dataset = all_dataset.take(TRAINSET_SIZE + VALIDSET_SIZE)\n",
    "val_dataset = train_dataset.skip(TRAINSET_SIZE)\n",
    "train_dataset = train_dataset.take(TRAINSET_SIZE)\n",
    "test_dataset = all_dataset.skip(TRAINSET_SIZE + VALIDSET_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c97e170",
   "metadata": {},
   "source": [
    "#### Apply Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cc6a65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow function to rescale images to [0, 1]\n",
    "@tf.function\n",
    "def normalize(input_image: tf.Tensor, input_mask: tf.Tensor) -> tuple:\n",
    "    input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "    return input_image, input_mask\n",
    "\n",
    "# Tensorflow function to apply preprocessing transformations\n",
    "@tf.function\n",
    "def load_image_train(datapoint: dict) -> tuple:\n",
    "    input_image = tf.image.resize(datapoint['image'], (IMG_SIZE, IMG_SIZE))\n",
    "    input_mask = tf.image.resize(datapoint['segmentation_mask'], (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        input_mask = tf.image.flip_left_right(input_mask)\n",
    "\n",
    "    input_image, input_mask = normalize(input_image, input_mask)\n",
    "\n",
    "    return input_image, input_mask\n",
    "\n",
    "# Tensorflow function to preprocess validation images\n",
    "@tf.function\n",
    "def load_image_test(datapoint: dict) -> tuple:\n",
    "    input_image = tf.image.resize(datapoint['image'], (IMG_SIZE, IMG_SIZE))\n",
    "    input_mask = tf.image.resize(datapoint['segmentation_mask'], (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    input_image, input_mask = normalize(input_image, input_mask)\n",
    "\n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9da5d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: ((None, 128, 128, 3), (None, 128, 128, 1)), types: (tf.float32, tf.float32)>\n",
      "<PrefetchDataset shapes: ((None, 128, 128, 3), (None, 128, 128, 1)), types: (tf.float32, tf.float32)>\n",
      "<PrefetchDataset shapes: ((None, 128, 128, 3), (None, 128, 128, 1)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset}\n",
    "\n",
    "# -- Train Dataset --#\n",
    "dataset['train'] = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset['train'] = dataset['train'].shuffle(buffer_size=BUFFER_SIZE, seed=SEED)\n",
    "dataset['train'] = dataset['train'].repeat()\n",
    "dataset['train'] = dataset['train'].batch(BATCH_SIZE)\n",
    "dataset['train'] = dataset['train'].prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "#-- Validation Dataset --#\n",
    "dataset['val'] = dataset['val'].map(load_image_test)\n",
    "dataset['val'] = dataset['val'].repeat()\n",
    "dataset['val'] = dataset['val'].batch(BATCH_SIZE)\n",
    "dataset['val'] = dataset['val'].prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "#-- Testing Dataset --#\n",
    "dataset['test'] = dataset['test'].map(load_image_test)\n",
    "dataset['test'] = dataset['test'].batch(BATCH_SIZE)\n",
    "dataset['test'] = dataset['test'].prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "print(dataset['train'])\n",
    "print(dataset['val'])\n",
    "print(dataset['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375c0cc3",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe54cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to view the images from the directory\n",
    "def display_sample(display_list):\n",
    "    plt.figure(figsize=(18, 18))\n",
    "\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "        plt.axis('off')\n",
    "        \n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8290e66e",
   "metadata": {},
   "source": [
    "#### Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24fa0252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get VGG-16 network as backbone\n",
    "vgg16_model = VGG16()\n",
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b215017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (IMG_SIZE, IMG_SIZE, N_CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff65abe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new model using the VGG network\n",
    "# Input\n",
    "inputs = Input(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e5f5885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG network. Pretrained on imagenet\n",
    "vgg16_model = VGG16(include_top = False, weights = 'imagenet', input_tensor = inputs)\n",
    "\n",
    "#  Encoder layers\n",
    "c1 = vgg16_model.get_layer('block3_pool').output\n",
    "c2 = vgg16_model.get_layer('block4_pool').output\n",
    "c3 = vgg16_model.get_layer('block5_pool').output\n",
    "\n",
    "# Decoder\n",
    "u1 = UpSampling2D((2,2), interpolation = 'bilinear')(c3)\n",
    "d1 = Add()([u1, c2])\n",
    "d1 = Conv2D(256, 1, activation = 'sigmoid')(d1)\n",
    "\n",
    "u2 = UpSampling2D((2,2), interpolation='bilinear')(d1)\n",
    "d2 = Add()([u2, c1])\n",
    "d2 = Conv2D(256, 1, activation = 'sigmoid')(d2)\n",
    "\n",
    "# output\n",
    "u3 = UpSampling2D((8,8), interpolation = 'bilinear')(d2)\n",
    "outputs = Conv2D(N_CLASSES, 1, activation = 'sigmoid')(u3)\n",
    "\n",
    "model = Model(inputs, outputs, name = 'VGG_FCN8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ead8bb9",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6731217",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e50f22ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_iou = tf.keras.metrics.MeanIoU(2)\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss=BinaryCrossentropy(),\n",
    "              metrics=[m_iou])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de91f9db",
   "metadata": {},
   "source": [
    "#### Check Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b429e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a mask out of network prediction\n",
    "def create_mask(pred_mask: tf.Tensor) -> tf.Tensor:\n",
    "    # Round to closest\n",
    "    pred_mask = tf.math.round(pred_mask)\n",
    "    \n",
    "    # [IMG_SIZE, IMG_SIZE] -> [IMG_SIZE, IMG_SIZE, 1]\n",
    "    pred_mask = tf.expand_dims(pred_mask, axis=-1)\n",
    "    return pred_mask\n",
    "\n",
    "# Function to show predictions\n",
    "def show_predictions(dataset=None, num=1):\n",
    "    if dataset:\n",
    "        # Predict and show image from input dataset\n",
    "        for image, mask in dataset.take(num):\n",
    "            pred_mask = model.predict(image)\n",
    "            display_sample([image[0], true_mask, create_mask(pred_mask)])\n",
    "    else:\n",
    "        # Predict and show the sample image\n",
    "        inference = model.predict(sample_image)\n",
    "        display_sample([sample_image[0], sample_mask[0],\n",
    "                        inference[0]])\n",
    "        \n",
    "for image, mask in dataset['train'].take(1):\n",
    "    sample_image, sample_mask = image, mask\n",
    "\n",
    "show_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd79b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a2ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f991f317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072e9b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d89978f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0ca8eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b06e2e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1756d332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8efb85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4a3c61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
