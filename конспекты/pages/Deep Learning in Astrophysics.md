- [Yuan-Sen Ting, Deep Learning in Astrophysics](https://arxiv.org/pdf/2510.10713)
- Быстрое развитие методов использующих различные виды искусственных нейросетей дало разные названия типа "машинного обучения" "глубокого обучения" и "ИИ". Некоторые используют эти три названия как синонимы, но автор данной статьи считает, что важно их различать.
- Машинное обучение это развитие статистических численных методов, поэтому цели и задачи у него такие же. Автор называет такой подход, который не использует искусственные нейросети **классическим машинными обучением**. Одним из самых распространенных методов машинного обучения является линейная регрессия, которая вот уже несколько десятков лет успешно применятся в Астрофизике.
- С развитием современных методов регистрации, также повысилось требование к скорости и качеству обработки. Готовые общепринятые решения не достаточны для задач физики, потому что мало найти паттерны в данных и предсказывать их, но необходимо и объяснять результаты. Поэтому астрофизика выдвигает следующие требования к современным моделям машинного обучения:
	- **Масштабируемость.** Модель должна быть достаточно гибкой, чтобы работать и с миллиардами данных, и с десятками миллионов или даже десятками тысяч. При этом точность и скорость работы должны быть достаточно высокими.
	- **Обобщаемость**. Модель должна уметь работать не только с тренировочными уже знакомыми или похожими данными, но и с данными которые она никогда не видела. Это очень важно, потому могут быть какие-нибудь данные, которые обладают пока еще не известными скрытыми закономерностями.
	- **Эффективность**. Модель должна уметь работать с очень небольшим количеством промаркированных данных и экстраполировать результаты на огромное число не помеченных данных. Т.к. общее число данных очень высокое, а количество хорошо изученных данных пренебрежимо мало, такое требование является не только логичным, но и необходимым.
- Рассмотрим различные методы и то, как они соотносятся с требованиями, которые астрофизика выдвигает к ним.
- ## 1.2 Классическое машинное обучение
  collapsed:: true
	- Одним из самых ранних и распространенных моделей машинного обучения является линейная регрессия. Эта модель отлично масштабируется и способна учиться на небольшом количестве данных, но дает сбой если у данных сложная нелинейная закономерность.
	- Гауссовы процессы (Gassuan Processes) хорошо работают с нелинейными данными, но из-за высоких требований к вычислительным мощностям неспособны работать с большими данными.
	- Random Forest имеет хороший баланс между вычислительной эффективностью и экспрессивностью, но его структура сильно ограничивает предсказания по средним весам тренировочных данных, что уменьшает обобщаемость моделей.
	- Отсюда мы можем сделать вывод, что классические модели не удовлетворяют нашим требованиям.
- ## 1.3 Почему современные наблюдения требуют новых методов
  collapsed:: true
	- Из-за развития инструментов наблюдения и развития вычислительных технологий увеличилось количество данных необходимых для обработки.  От миллиардов источников непрерывно идут данные и только небольшое количество имеет какие-то физические явления. Более того использование излучения на разных частотах требует современных методов обработки данных и поиска закономерностей, которые не могут быть описаны традиционными методами математической статистики
	- Открытия сделанные в последнее время были получены благодаря анализу больших данных: Fast Radio Bursts, черные дыры со звездными массами с большой орбитой, траектория Оумуамуа, транс-нептунский объект OF201 и т.д.
	- Большое количество данных требует другого подхода
- ## 1.4 Глубокое обучение как новое направление
  collapsed:: true
	- Ограничения традиционных методов - невозможность достижения масштабируемости, обобщаемости и эффективности являются узким местом современной астрономии. Нейросети предоставляют альтернативу этим методам. За счет параллелизации на графических картах GPU решается проблема времени вычисления. А также Цыбенко (универсальная теорема аппроксимации) было доказано, что искусственная нейронная сеть прямой связи (feedforward, в которых связи не образуют циклов) с одним скрытым слоем может аппроксимировать любую непрерывную функцию многих переменных с любой точностью при условии достаточного количества нейронов (в скрытом слое).
	- Позже было доказано, что нейросети с бесконечной шириной сходятся к Гауссиане. Точно так же как утверждает центральная предельная теорема, что сумма случайных переменных сходятся в нормальное распределение, бесконечное количество нейронов в слое - через усреднение - становится гауссовским процессом со специфическими ярдами. По этой причине нейросеть обобщает лучше чем простой полином.
	- Хотя это не решает проблему необходимости большого количества данных. Далее мы рассмотрим как это решается современной инновационной архитектурой нейросети, которая превращает нейросети из черного ящика в готовый инструмент по поиску астрономических явлений.
	-
- ## 2.0 Развитие глубокого обучения в астрофизике
  collapsed:: true
	- Сложно отрицать то, что использование глубокого обучения в астрофизике непопулярно. Наоборот, его использование уже конкурирует с машинным обучением, которое во-многом стало стандартом в гамма-астрономии.
	- Ключевая проблема в глубоком обучение это т.н. **inductive bias**, который есть в машинной обучении. Т.е. когда мы сознательно вносим некоторое смещение или, можно сказать, предвзятость в модель, чтобы получить определенное решение. Как бы подталкивая модель к нему. Это делается потому, что мы вносим ограничения, которые даются теоретическими моделями физики, которые необходимо учитывать при предсказании с помощью машинного обучения.
	- Если такое смещение в модель машинного обучения можно задать при помощи формулы, то в DL-модель этого так не сделаешь. Такое смещение можно реализовать только при помощи архитектуры. При этом необходимо помнить, что подбор архитектуры делается не для удобства, а с точки зрения физики, т.е. мы, как бы, вшиваем в саму архитектуру физическую гипотезу.
	- Inductive Bias:
	  Assumptions guiding
	  learning algorithms
	  toward solutions.
	  Encoded through
	  architectural choices,
	  physics constraints,
	  or symmetries rather
	  than explicit
	  functional forms
- ## 2.1 Нейросети как универсальные аппроксиматоры
  collapsed:: true
	- Давайте рассмотрим классическую нейросеть, которая обучается на данных. Это обычный персептрон (**MLP - multilayered perceptron**) с большим количеством слоев. Такая модель может обучиться на что угодно, но обладает существенным недостатком, ему нужно колоссальное количество данных. Потому что сама архитектура не вносит никакого смещения. В этой модели нельзя проверить только одну гипотезу.
	- Как показали последние работы, архитектура нейросети имеет ключевое значение.
- ## 2.2 Выбор архитектуры для определенной гипотезы
  collapsed:: true
	- ### 2.2.1 CNN
	  collapsed:: true
		- CNN (convolutional neural network) - свёрточная нейросеть (СНН). Они получили наиболее широкое распространение в астрономии. СНН позволяет эффективно тренировать модель на рисунках вылавливая частные и общие характеристики рисунков, при этом налагая жесткие ограничения.
		- СНН работает следующим образом: он состоит из большого количества слоев, при этом первые слои улавливают частные и мелкие черты, а с углублением (с каждым последующим слоем) улавливаются все более и более общие черты, тем самым из частного переходя в общее, обобщая данные. Такой подход очень похож на тот подход и те алгоритмы которые астрономы всегда использовали. В этом плане СНН является идеальной архитектурой для модели.
	- ### 2.2.2 RNN
	  collapsed:: true
		- Рекурентные нейросети тоже используются в астрономии, но в данных, которые разбиты по времени (time series), хотя они постепенно заменяются трансформерами
		- Архитектура модели позволяет рассматривать данные "в развитии", т.е. интервал данных рассматривается как развитие какого-то процесса. Т.е. мы можем рассматривать предыдущие данные как "память". В астрономии это взрыв сверхновой например. Или светимость звезды, которая меняется со временем (для этого способность модели "запоминать" предыдущие по времени данные отлично подходит).
		- Модель позволяет предсказывать следующую фазу звезды.
		- В отличии от Hidden Markov Model (HMM), который классифицирует звезды в дискретные классы, РНН позволяет рассматривать звезды в непрерывном движении, каким и является этот процесс.
		- У модели есть свои недостатки, например она со временем "забывает" старые данные. Поэтому есть альтернативы такие как LSTM (Long Short-Term Memory, LSTM, Hochreiter & Schmidhuber 1997) и GRU (Gated Recurrent Unit, GRU, Cho et al., 2014)
		- В отличие от СНН, которые требует данные одинакового размера, РНН может работать с потоком постоянно обновляющихся данных, что делает его идеальным для непрерывно развивающейся системы.
		- Однако, даже LSTM  и GRU, хранят историю в векторах постоянного размера, из-за чего со временем начинают "забывать".
		- Поэтому и были придуманы трансформеры
	- ### 2.2.3. Трансформеры
	  collapsed:: true
		- Трансформеры являют собой смену парадигмы в моделировании последовательностей, и фокусируются на параллельном сравнении каждого наблюдения со всеми другими. Архитектура трансформеров построена на механизме внимания - механизм, которые рассчитывает все отношения между парами входных данных одновременно.
	- Правильный выбор архитектуры модели, который совпадает со структурой данных позволяет достигнуть высокой точности с небольшим количеством данных. И наоборот, если выбор архитектуры неправильный, это ведет к необходимости компенсировать это большим числом данных и ведет к отсутствию сходимости. Успех зависит от понимания структуры данных и выбора архитектуры, которая подходит к структуре ваших данных
	- ### 2.4 Encoding Physical Symmetries
	  collapsed:: true
		- В физике есть симметричность, которую надо запрограммировать в модель. Конечно есть и асимметрии, например, чтобы различать галактики (где рукав, в какую сторону повернут и т.п.). Нам важно с какой стороны рукав, как  он повернут и т.п. С другой стороны светимость галактики не зависит от поворота и положения на рисунке.
		- Модели, которым не важен поворот и положение на рисунке называют инвариантными. Пусть g - функция, которая поворачивает или перемещает на рисунке объект x. Тогда:
			- f(g[x]) = f(x) - инвариантная модель.
		- А модели, которым изменения важны называются эквивариантными:
			- f(g[x]) = g(f[x]).
		- Если мы повернем галактику на 90 градусов, то модель должна обнаруживать повернутую на 90 градусов спираль.
		- Распространенные модели CNN и RNN они инварианты, хотя они сохраняют особенности рисунков внутри слоев, но последний слой вносит ограничения.
		- GNN - является эквивариантной моделью.
		- Однако все эти модели недостаточно совершенны для задач астрофизики.
	- ### 2.6 Учет законов физики через функцию потерь
	  collapsed:: true
		- Одним из способов для улучшения обучаемости модели может служить ввод штрафа за нарушение законов физики. Тогда функция потерь будет:
			- $$L = L_{data} + \lambda_{phys}\cdot L_{phys},$$
			- где L_{data} - разность между полученным значением и требуемым значением. L_{phys} - это разница с математической моделью (т.н. governing equations), \lambda_{phys} - это вес, который контролирует влияние законов физики в нашей модели. Чем она выше, тем строже мы наказываем модель за нарушение законов физики. Это значение будет зависеть от наших задач.
		- governing equations - это уравнения, которые представляют собой фундаментальные законы физики. Либо уравнения полученные через моделирование.
			-
	-
- ## 3.1 Multi-Scale Modeling and Simulation Surrogates
  collapsed:: true
	- Как было показано ранее, применяя правильную архитектуру (2.2), принимая во внимание симметрии исследуемых объектов (2.4) и дополнительного ограничения через моделирование (2.6) мы можем получить очень качественную и заточенную под наши цели модель.
	- Но перед нами стоит еще одна проблема, как же нам научить модель обобщать, и выделять общие характеристики, а не запоминать случайные элементы для получения правильного ответа? Для этого используется пара энкодер-декодер. Эта такая архитектура, которая уменьшает количество параметров по которым обучается наша модель, а затем снова увеличивает. Уменьшение параметров позволяет выкинуть менее общие и менее важные параметры и оставлять самые общие параметры. Такой метод позволяет обобщать данные.
- ## 3.2 Исследование на основе моделирования
  collapsed:: true
	- Использование моделей глубого обучения и нейросетей позволяет не только искать какие-то общие параметры, но и предсказывать результаты. Это позволяет воссоздавать искусственные данные, которые мы можем далее проанализировать.
	- Чтобы понять, что искусственные данные достаточно хороши мы должны создать такой пайплайн: 1) создание искусственных данных; 2) проверка данных на искусственность. Или же генератор - проверяющий.
	- У такого подхода есть недостатки, генератор не будет пытаться воссоздавать все разнообразие реальных данных, а будет пытаться найти какую-то комбинацию, которая позволит обмануть проверяющего, как только он найдет, он будет продолжать генерировать одни и те же данные.
	- Поэтому были созданы разные модели для этого: GAN, Normalizing Flows, Diffusion Models, Flow Matching и др.
	- Но даже такие модели не свободны от критики. Одна из них это "черный ящик". Мы не знаем, что происходит внутри этих моделей, потому что они скрыты от нас. Из-за чего мы не можем ни интерпретировать, ни проверить эти данные.
	- Тут надо заметить, что проблема интерпретации результатов не совсем проблема присущая только моделям, та же проблема существует и с реальными данными. То, что очевидно для одного исследователя, совершенно не очевидно для другого и наоборот.
	- А другая проблема, это проверка моделей. Т.е. нам нужно проверить поведение моделей при других условиях. Это довольно комплексная проблема, которая продолжает вызывать споры. Например, в работе Villaescusa-Navarro et al (2022) было показано, что если взять искусственные данные сгенерированные одной моделью и дать на вход другой модели, то они не смогут пройти тест на качество. Это означает, что модели воссоздавали не физику в данных, а какие-то артефакты, которые позволяли пройти тест на искусственность.
- ## 3.3 Обнаружение аномалий
  collapsed:: true
	- Обнаружение аномалий это еще одно перспективное направление при помощи которого можно находить что-то новое и неизвестное. В обилие информации подсветить то, что может быть интересным для ученых.
	- Для такого используется вариационный автоэнкодер (ВАЭ), который в отличие от обычного автоэнкодера просто сжимает и восстанавливает данные, у ВАЭ этот процесс вероятностный. Т.е. внутри модели получается какое-то облако внутри которого должны лежать параметры (в пределах нормального распределения), если они лежат за ними, то модель дает нам сигнал.
- ## 3.4 Foundation models
  collapsed:: true
	- det. так называют большие модели, которые обучаются на больших количествах данных. Это наиболее общие абстрактные модели, которые после настройки могут применяться для конкретных случаев.
	- В отличие от маленьких моделей, они сохраняют в памяти все необходимое для анализа и поиска. И при дообучении на новых данных они продолжают работать как и раньше.
	- Такие модели являются очень перспективными в астрономии, которая работает с сотнями тысяч изображений, на которых разные ученые ищут разные явления. Но основная цель таких моделей это использование в случаях, когда у нас недостаточное количество данных для обучения.
	- Из-за малого количества данных обучить модель "с нуля" специально для этих данных нельзя.
	- К сожалению на данный момент таких моделей в астрофизике нет. А те что есть, не являются большими фундаментальными моделями. Это модели с большим количеством данных к которым применяют разные дополнительные архитектуры, чтобы получить новые данные. По определению фундаментальных моделей они не должны так функционировать. Мы должны иметь возможность просто на вход подать новые данные и получить искомые результаты.