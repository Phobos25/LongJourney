- Быстрое развитие методов использующих различные виды искусственных нейросетей дало разные названия типа "машинного обучения" "глубокого обучения" и "ИИ". Некоторые используют эти три названия как синонимы, но авторы считают, что важно их различать.
- Машинное обучение это продолжение статистических численных методов, поэтому цели и задачи у него такие же. Автор называет такой подход, который не использует искусственные нейросети **классическим машинными обучением**. Одни из самых распространенных методов машинного обучения является линейная регрессия, которая вот уже несколько десятков лет успешно применятся в Астрофизике.
- С развитием современных методов регистрации, также повысилось требование к скорости и качеству обработки. А т.к. промышленные уже готовые решения не достаточны для задач физики, потому что мало найти паттерны в данных и предсказывать их, но необходимо и объяснять результаты. Поэтому астрофизика выдвигает следующие требования к современным моделям машинного обучения:
	- **Масштабируемость.** Модель должна быть достаточно гибкой, чтобы работать с миллиардами данных, с десятками миллионов или даже десятками тысяч. При этом точность и скорость работы должны быть достаточно высокими.
	- **Обобщаемость**. Модель должна уметь работать не только с тренировочными уже знакомыми данными, но и с данными которые она никогда не видела. Это очень важно, потому могут быть какие-нибудь данные, которые обладают пока еще не известными скрытыми закономерностями.
	- **Эффективность**. Модель должна уметь работать с очень небольшим количеством промаркированных данных и экстраполировать результаты на огромное число не помеченных данных. Т.к. общее число данных очень высокое, а количество хорошо изученных данных пренебрежимо мало, такое требование является не только логичным, но и необходимым.
- Рассмотрим различные методы и то, как они соотносятся с требованиями, которые астрофизика выдвигает к ним.
- ## 1.2 Классическое машинное обучение
  collapsed:: true
	- Одним из самых ранних и распространенных моделей машинного обучения является линейная регрессия. Эта модель отлично масштабируется и способна учиться на небольшом количестве данных, но дает сбой если у данных сложная нелинейная закономерность.
	- Гауссовы процессы (Gassuan Processes) хорошо работают с нелинейными данными, но из-за высоких требований к вычислительным мощностям неспособны работать с большими данными.
	- Random Forest имеет хороший баланс между вычислительной эффективностью и экспрессивностью, но его структура сильно ограничивает предсказания по средним весам тренировочных данных, что уменьшает обобщаемость моделей.
	- Отсюда мы можем сделать вывод, что классические модели не удовлетворяют нашим требованиям.
- ## 1.3 Почему современные наблюдения требуют новых методов
  collapsed:: true
	- Из-за развития инструментов наблюдения и развития вычислительных технологий увеличилось количество данных необходимых для обработки.  От миллиардов источников непрерывно идут данные и только небольшое количество имеет какие-то физические явления. Более того использование излучения на разных частотах требует современных методов обработки данных и поиска закономерностей, которые не могут быть описаны традиционными методами математической статистики
	- Открытия сделанные в последнее время были получены благодаря анализу больших данных: Fast Radio Bursts, черные дыры со звездными массами с большой орбитой, траектория Оумуамуа, транс-нептунский объект OF201 и т.д.
	- Большое количество данных требует другого подхода
- ## 1.4 Глубокое обучение как новое направление
  collapsed:: true
	- Ограничения традиционных методов - невозможность достижения масштабируемости, обобщаемости и эффективности являются узким местом современной астрономии. Нейросети предоставляют альтернативу этим методам. За счет параллелизации на графических картах GPU решается проблема времени вычисления. А также Цыбенко (универсальная теорема аппроксимации) было доказано, что искусственная нейронная сеть прямой связи (feedforward, в которых связи не образуют циклов) с одним скрытым слоем может аппроксимировать любую непрерывную функцию многих переменных с любой точностью при условии достаточного количества нейронов (в скрытом слое).
	- Позже было доказано, что нейросети с бесконечной шириной сходятся к Гауссиане. Точно так же как утверждает центральная предельная теорема, что сумма случайных переменных сходятся в нормальное распределение, бесконечное количество нейронов в слое - через усреднение - становится гауссовским процессом со специфическими ярдами. По этой причине нейросеть обобщает лучше чем простой полином.
	- Хотя это не решает проблему необходимости большого количества данных. Далее мы рассмотрим как это решается современной инновационной архитектурой нейросети, которая превращает нейросети из черного ящика в готовый инструмент по поиску астрономических явлений.
	-
- ## 1.5 Развитие глубокого обучения в астрофизике
	-