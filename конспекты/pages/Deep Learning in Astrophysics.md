- Быстрое развитие методов использующих различные виды искусственных нейросетей дало разные названия типа "машинного обучения" "глубокого обучения" и "ИИ". Некоторые используют эти три названия как синонимы, но авторы считают, что важно их различать.
- Машинное обучение это продолжение статистических численных методов, поэтому цели и задачи у него такие же. Автор называет такой подход, который не использует искусственные нейросети **классическим машинными обучением**. Одни из самых распространенных методов машинного обучения является линейная регрессия, которая вот уже несколько десятков лет успешно применятся в Астрофизике.
- С развитием современных методов регистрации, также повысилось требование к скорости и качеству обработки. А т.к. промышленные уже готовые решения не достаточны для задач физики, потому что мало найти паттерны в данных и предсказывать их, но необходимо и объяснять результаты. Поэтому астрофизика выдвигает следующие требования к современным моделям машинного обучения:
	- **Масштабируемость.** Модель должна быть достаточно гибкой, чтобы работать с миллиардами данных, с десятками миллионов или даже десятками тысяч. При этом точность и скорость работы должны быть достаточно высокими.
	- **Обобщаемость**. Модель должна уметь работать не только с тренировочными уже знакомыми данными, но и с данными которые она никогда не видела. Это очень важно, потому могут быть какие-нибудь данные, которые обладают пока еще не известными скрытыми закономерностями.
	- **Эффективность**. Модель должна уметь работать с очень небольшим количеством промаркированных данных и экстраполировать результаты на огромное число не помеченных данных. Т.к. общее число данных очень высокое, а количество хорошо изученных данных пренебрежимо мало, такое требование является не только логичным, но и необходимым.
- Рассмотрим различные методы и то, как они соотносятся с требованиями, которые астрофизика выдвигает к ним.
- ## 1.2 Классическое машинное обучение
  collapsed:: true
	- Одним из самых ранних и распространенных моделей машинного обучения является линейная регрессия. Эта модель отлично масштабируется и способна учиться на небольшом количестве данных, но дает сбой если у данных сложная нелинейная закономерность.
	- Гауссовы процессы (Gassuan Processes) хорошо работают с нелинейными данными, но из-за высоких требований к вычислительным мощностям неспособны работать с большими данными.
	- Random Forest имеет хороший баланс между вычислительной эффективностью и экспрессивностью, но его структура сильно ограничивает предсказания по средним весам тренировочных данных, что уменьшает обобщаемость моделей.
	- Отсюда мы можем сделать вывод, что классические модели не удовлетворяют нашим требованиям.
- ## 1.3 Почему современные наблюдения требуют новых методов
  collapsed:: true
	- Из-за развития инструментов наблюдения и развития вычислительных технологий увеличилось количество данных необходимых для обработки.  От миллиардов источников непрерывно идут данные и только небольшое количество имеет какие-то физические явления. Более того использование излучения на разных частотах требует современных методов обработки данных и поиска закономерностей, которые не могут быть описаны традиционными методами математической статистики
	- Открытия сделанные в последнее время были получены благодаря анализу больших данных: Fast Radio Bursts, черные дыры со звездными массами с большой орбитой, траектория Оумуамуа, транс-нептунский объект OF201 и т.д.
	- Большое количество данных требует другого подхода
- ## 1.4 Глубокое обучение как новое направление
  collapsed:: true
	- Ограничения традиционных методов - невозможность достижения масштабируемости, обобщаемости и эффективности являются узким местом современной астрономии. Нейросети предоставляют альтернативу этим методам. За счет параллелизации на графических картах GPU решается проблема времени вычисления. А также Цыбенко (универсальная теорема аппроксимации) было доказано, что искусственная нейронная сеть прямой связи (feedforward, в которых связи не образуют циклов) с одним скрытым слоем может аппроксимировать любую непрерывную функцию многих переменных с любой точностью при условии достаточного количества нейронов (в скрытом слое).
	- Позже было доказано, что нейросети с бесконечной шириной сходятся к Гауссиане. Точно так же как утверждает центральная предельная теорема, что сумма случайных переменных сходятся в нормальное распределение, бесконечное количество нейронов в слое - через усреднение - становится гауссовским процессом со специфическими ярдами. По этой причине нейросеть обобщает лучше чем простой полином.
	- Хотя это не решает проблему необходимости большого количества данных. Далее мы рассмотрим как это решается современной инновационной архитектурой нейросети, которая превращает нейросети из черного ящика в готовый инструмент по поиску астрономических явлений.
	-
- ## 2.0 Развитие глубокого обучения в астрофизике
  collapsed:: true
	- Сложно отрицать то, что использование глубокого обучения в астрофизике непопулярно. Наоборот, его использование уже конкурирует с машинным обучением, которое во-многом стало стандартом в гамма-астрономии.
	- Ключевая проблема в глубоком обучение это т.н. **inductive bias**, который есть в машинной обучении. Т.е. когда мы сознательно вносим некоторое смещение или, можно сказать, предвзятость в модель, чтобы получить определенное решение. Как бы подталкивая модель к нему. Это делается потому, что мы вносим ограничения, которые даются теоретическими моделями физики, которые необходимо учитывать при предсказании с помощью машинного обучения.
	- Если такое смещение в модель машинного обучения можно задать при помощи формулы, то в DL-модель этого так не сделаешь. Такое смещение можно реализовать только при помощи архитектуры. При этом необходимо помнить, что подбор архитектуры делается не для удобства, а с точки зрения физики, т.е. мы, как бы, вшиваем в саму архитектуру физическую гипотезу.
	- Inductive Bias:
	  Assumptions guiding
	  learning algorithms
	  toward solutions.
	  Encoded through
	  architectural choices,
	  physics constraints,
	  or symmetries rather
	  than explicit
	  functional forms
- ## 2.1 Нейросети как универсальные аппроксиматоры
  collapsed:: true
	- Давайте рассмотрим классическую нейросеть, которая обучается на данных. Это обычный персептрон (**MLP - multilayered perceptron**) с большим количеством слоев. Такая модель может обучиться на что угодно, но обладает существенным недостатком, ему нужно колоссальное количество данных. Потому что сама архитектура не вносит никакого смещения. В этой модели нельзя проверить только одну гипотезу.
	- Как показали последние работы, архитектура нейросети имеет ключевое значение.
- ## 2.2 Выбор архитектуры для определенной гипотезы
	- ### 2.2.1 CNN
	  collapsed:: true
		- CNN (convolutional neural network) - свёрточная нейросеть (СНН). Они получили наиболее широкое распространение в астрономии. СНН позволяет эффективно тренировать модель на рисунках вылавливая частные и общие характеристики рисунков, при этом налагая жесткие ограничения.
		- СНН работает следующим образом: он состоит из большого количества слоев, при этом первые слои улавливают частные и мелкие черты, а с углублением (с каждым последующим слоем) улавливаются все более и более общие черты, тем самым из частного переходя в общее, обобщая данные. Такой подход очень похож на тот подход и те алгоритмы которые астрономы всегда использовали. В этом плане СНН является идеальной архитектурой для модели.
	- ### 2.2.2 RNN
		- Рекурентные нейросети тоже используются в астрономии, но в данных, которые разбиты по времени (time series), хотя они постепенно заменяются трансформерами
		- Архитектура модели позволяет рассматривать данные "в развитии", т.е. интервал данных рассматривается как развитие какого-то процесса. Т.е. мы можем рассматривать предыдущие данные как "память". В астрономии это взрыв сверхновой например. Или светимость звезды, которая меняется со временем (для этого способность модели "запоминать" предыдущие по времени данные отлично подходит).
		- Модель позволяет предсказывать следующую фазу звезды.
		- В отличии от Hidden Markov Model (HMM), который классифицирует звезды в дискретные классы, РНН позволяет рассматривать звезды в непрерывном движении, каким и является этот процесс.
		- У модели есть свои недостатки, например она со временем "забывает" старые данные. Поэтому есть альтернативы такие как LSTM (Long Short-Term Memory, LSTM, Hochreiter & Schmidhuber 1997) и GRU (Gated Recurrent Unit, GRU, Cho et al., 2014)
		- В отличие от СНН, которые требует данные одинакового размера, РНН может работать с потоком постоянно обновляющихся данных, что делает его идеальным для непрерывно развивающейся системы.
		- Однако, даже LSTM  и GRU, хранят историю в векторах постоянного размера, из-за чего со временем начинают "забывать".
		- Поэтому и были придуманы трансформеры
	- ### 2.2.3. Трансформеры
		-
	-