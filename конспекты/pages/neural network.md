alias:: [[нейросеть]]

- ## Объяснение
	- здесь будет приводиться простое объяснение что такое нейросеть
- ### Что такое нейросеть
  collapsed:: true
	- Нейросеть используется для разных задач. Для этого используются разные модели.
	- Например CNN (convolutional neural network) используется для распознавания изображений.
	- Long short-term memory network - для распознавания речи.
	- Простейшей нейросетью является персептрон (или многослойный персептрон). Который исторически был разработан для распознавания чисел.
	- ![image.png](../assets/image_1756341827152_0.png){:height 242, :width 236}
		- рис. 1
	- Название нейроны связаны с клетками мозга. Но в данном случае надо это представлять как простой **объект хранящий число** (от 0 до 1).
		- Это число внутри нейрона называется **активацией**
		- Представьте, что нейрон загорается при увеличении активации (чем ближе к 1)
	- Для рис. 1 первой слой, состоящий из 784 (28х28) нейронов будет называться первым слоем. Последний слой будет состоять из 10 нейронов, где каждый нейрон отвечает за определенное число (от 0 до 9). Их активация это соответствие тому или иному числу.
	- Слои между 1-м и последним слоем это скрытые слои.
	- Но как работает нейросеть?
		- Слои и нейроны в нем отвечают за определенные закономерности
		- Например, 9 можно разделить на окружность и линию. 8 - на две окружности, 4 - это вертикальная, диагональная и горизонтальные линии. и т.д (на самом деле эти числа можно разделить и на еще большее число компонент, но для простоты будем считать так).
		- Для связи между слоями используются веса, они определяют важность тех или иных нейронов для конечного результата. Но на выходе мы должны получить значение близкое к 1, поэтому надо использовать сигмоидную функцию или ReLu.
		- $$\sigma(x) = \frac{1}{1+e^{-x}}$$
		- Здесь \sigma это функция, а x = (w_{1}a_{1}+w_{2}a_{2}+...+w_{n}a_{n} - b),
		- где w - это вес, a - активация, b - смещение (bias)
		- это только для одного нейрона, а для все сети, лучше представлять это в виде матрицы.
		- В нашем случае таких параметров будет 13002 (784*16 + 16*16 + 16*10 [это веса] + 16+16+10 (bias))
			- Для мысленного эксперимента представьте, как вы будете вручную подбирать все эти параметры.
		- Всегда полезно представлять каким образом веса и смещения себя ведут. Это позволяет лучше представлять решение и быстрее находить способы его оптимизации.
		- использование матриц проще и быстрее. К тому же вычисления на компьютере производятся в виде матриц, для повышения быстродействия.
		- $$\sigma\left(\left[\begin{array}{cccc} w_{0,0} & w_{0,1} & ... & w_{0,n} \\ w_{1,0} & w_{1,1} & ... & w_{1,n}\\ ...& ... & ... & ... \\ w_{k,0} & w_{k,1} & ... & w_{k,n}\end{array}\right] \left[ \begin{array}{c} a_{0}^{(0)} \\ a_{1}^{(0)} \\ ... \\ a_{n}^{(0)}\end{array}\right] + \left[ \begin{array}{c} b_{0} \\ b_{1} \\ ... \\ b_{n}\end{array}\right]\right)$$
		- ML сильно связан с линейной алгеброй. Правильнее рассматривать каждый нейрон как функцию, которая принимает на вход все предыдущие нейроны и выдает значение от 0 до 1. Вообще, в широком смысле, вся нейросеть это функция.
		-
	-
- ### Градиентный спуск. Как обучается нейросеть
	- Для обучения нейросети используется очень большое количество количество правильно помеченных данных, т.е. в случае если мы имеем дело с цифрами написанными от руки, нами же давались бы точные данные о том, что это за цифра. Более того, нам нужна небольшая часть данных, которые наша модель никогда не видела, которая будет работать как тестовые данные для проверки точности работы нашей модели. Потому что модель может "переобучиться", т.е. вместо того, чтобы найти правильные паттерны, которые отвечают за правильное решение задачи, просто "запомнит" некую последовательность. В каком-то смысле это как запомнить таблицу Сивцева (для проверки зрения) и отвечать по памяти, когда медсестра указывает на букву, вместо того, чтобы читать видя глазом.
	- Но тут встает вопрос, каким образом мы оценим обученность модели. Как мы будем обучать нашу модель.
		- Для этого используется функция стоимости. Которая дает численное значение ошибки. Чем меньше это значение тем точнее модель. Для его расчета мы берем квадрат разности между выходными данными и ожидаемым значением и слагаем их с друг другом.
		- К примеру, на вход мы подали 3. На выходе получили 0 = 0.43, 1 = 0.28, 2 = 0.19, 3 = 0.88 и т.д. Тогда,
		- $$Cost(3) = (0.43 - 0)^2 + (0.28 - 0)^2 + (0.19 - 0)^2 + (0.88 - 1)^2 + ...$$
			- Как видно эта функция будет тем больше, чем сильнее ошибается наша модель. В идеале, она должна везде давать 0, а на 3 давать 1, тогда функция стоимости будет равна 0.
	- **Очевидно, обучение модели это есть минимизация функции стоимости**. Но как это сделать? Для этого используется отрицательный градиент, который рассчитывает значения для всех нейронов модели на сколько надо изменить веса для уменьшения функции. При этом размер шага зависит от значения градиента, а направление от знака. Таким образом находится локальный минимум.
	- Для простоты мы представляли, что второй слой ищет линии или кусочки линий, которые в третьем слое использует для поиска паттернов. На самом деле, реальная нейросеть так не работает!
	- Если мы рассмотрим как выглядят веса (т.е. для рисунка 28х28) это не будет похоже ни на что, вряд ли там будем хоть какая-то закономерность, мы не сможем увидеть ни линий, ни окружностей, ни их каких-то частей.
-