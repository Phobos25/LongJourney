alias:: [[нейросеть]]

- ## Объяснение
	- здесь будет приводиться простое объяснение что такое нейросеть
- ### Что такое нейросеть
  collapsed:: true
	- Нейросеть используется для разных задач. Для этого используются разные модели.
	- Например CNN (convolutional neural network) используется для распознавания изображений.
	- Long short-term memory network (LSTM) - подвид RNN рекурентной нейросети, которая используется для распознавания речи.
	- Простейшей нейросетью является персептрон (или многослойный персептрон). Который был разработан для распознавания чисел.
	- ![image.png](../assets/image_1756341827152_0.png){:height 242, :width 236}
		- рис. 1
	- Название нейроны связаны с клетками мозга. Но в данном случае надо это представлять как простой **объект хранящий число** (от 0 до 1).
		- Это число внутри нейрона называется **активацией**
		- Представьте, что нейрон загорается при увеличении активации (чем ближе к 1)
	- Для рис. 1 первой слой, состоящий из 784 (28х28) нейронов будет называться первым слоем. Последний слой будет состоять из 10 нейронов, где каждый нейрон отвечает за определенное число (от 0 до 9). Активация этих нейронов соответствует тому или иному числу.
	- Слои между 1-м и последним слоем называются скрытыми слоями.
	- Но как работает нейросеть?
		- Слои и нейроны в нем отвечают за определенные закономерности
		- Например, 9 можно разделить на окружность и линию. 8 - на две окружности, 4 - это вертикальная, диагональная и горизонтальные линии. и т.д (на самом деле эти числа можно разделить и на еще большее число компонент, но для простоты будем считать так).
		- Для связи между слоями используются веса, они определяют важность тех или иных нейронов для конечного результата. Но на выходе мы должны получить значение близкое к 1, поэтому надо использовать сигмоидную функцию или ReLU.
		- $$\sigma(x) = \frac{1}{1+e^{-x}}$$
		- Здесь \sigma (сигма) это функция, а x = (w_{1}a_{1}+w_{2}a_{2}+...+w_{n}a_{n} - b),
		- где w - это вес, a - активация, b - смещение (bias)
		- это только для одного нейрона, а для все сети, лучше представлять это в виде матрицы.
		- В нашем случае таких параметров будет 13002 (784*16 + 16*16 + 16*10 [это веса] + 16+16+10 (bias))
			- Для мысленного эксперимента представьте, как вы будете вручную подбирать все эти параметры.
		- Всегда полезно представлять каким образом веса и смещения себя ведут. Это позволяет лучше представлять решение и быстрее находить способы его оптимизации.
		- использование матриц проще и быстрее. К тому же вычисления на компьютере производятся в виде матриц, для повышения быстродействия.
		- $$\sigma\left(\left[\begin{array}{cccc} w_{0,0} & w_{0,1} & ... & w_{0,n} \\ w_{1,0} & w_{1,1} & ... & w_{1,n}\\ ...& ... & ... & ... \\ w_{k,0} & w_{k,1} & ... & w_{k,n}\end{array}\right] \left[ \begin{array}{c} a_{0}^{(0)} \\ a_{1}^{(0)} \\ ... \\ a_{n}^{(0)}\end{array}\right] + \left[ \begin{array}{c} b_{0} \\ b_{1} \\ ... \\ b_{n}\end{array}\right]\right)$$
		- ML сильно связан с линейной алгеброй. Правильнее рассматривать каждый нейрон как функцию, которая принимает на вход все предыдущие нейроны и выдает значение от 0 до 1. Вообще, в широком смысле, вся нейросеть это функция.
- ### Градиентный спуск. Как обучается нейросеть
  collapsed:: true
	- Для обучения нейросети используется очень большое количество количество правильно помеченных данных, т.е. в случае если мы имеем дело с цифрами написанными от руки, нами же давались бы точные данные о том, что это за цифра. Более того, нам нужна небольшая часть данных, которые наша модель никогда не видела, которая будет работать как тестовые данные для проверки точности работы нашей модели. Потому что модель может "переобучиться" (overfit), т.е. вместо того, чтобы найти правильные паттерны, которые отвечают за правильное решение задачи, просто "запомнит" некую последовательность. В каком-то смысле это как запомнить таблицу Сивцева (для проверки зрения) и отвечать по памяти, когда окулист указывает на букву, вместо того, чтобы читать.
	- Но тут встает вопрос, каким образом мы оценим обученность модели. Как мы будем обучать нашу модель.
		- Для этого используется функция стоимости. Которая дает численное значение ошибки. Чем меньше это значение тем точнее модель. Для его расчета мы берем квадрат разности между выходными данными и ожидаемым значением и слагаем их с друг другом.
		- К примеру, на вход мы подали 3. На выходе получили ``0 = 0.43, 1 = 0.28, 2 = 0.19, 3 = 0.88`` и т.д. Тогда,
		- $$Cost(3) = (0.43 - 0)^2 + (0.28 - 0)^2 + (0.19 - 0)^2 + (0.88 - 1)^2 + ...$$
			- Как видно эта функция будет тем больше, чем сильнее ошибается наша модель. В идеале, она должна везде давать 0, а на 3 давать 1, тогда функция стоимости будет равна 0.
	- **Очевидно, обучение модели это есть минимизация функции стоимости**. Но как это сделать? Для этого используется отрицательный градиент, который рассчитывает значения для всех нейронов модели на сколько нужно изменить веса для уменьшения функции. При этом размер шага зависит от значения градиента, а направление от знака. Таким образом находится локальный минимум.
	- Для простоты мы представляли, что второй слой ищет линии или кусочки линий, которые в третьем слое использует для поиска паттернов. На самом деле, реальная нейросеть так не работает!
	- Если мы рассмотрим как выглядят веса (т.е. для рисунка 28х28) это не будет похоже ни на что, вряд ли там будем хоть какая-то закономерность, мы не сможем увидеть ни линий, ни окружностей, ни их каких-то частей.
- ### Backpropagation. Обратное распространение
  collapsed:: true
	- обратное распространение это метод который используется для обучения модели. Функция стоимости дает значение для каждого y (до суммирования), поэтому это позволяет рассчитать градиент. Градиент это значение на которое нужно уменьшить веса для получения нужного результата. Градиент берется для всех примеров и усредняется. Т.к. такой расчет идет с последнего слоя к 1-му он называется обратным распространением (backpropagation).
	- из-за большого количества данных, обучение на всех входных данных будет слишком трудоемким. Поэтому используется метод, который называется стохастическим градиентным спуском. Суть метода заключается в том, чтобы перемешать все данные случайным образом и разбить их на мини-батчи по 100 штук. Далее эти мини-батчи подаются для обучения, каждый шаг градиент выполняется после каждого батча. Это не совсем то, что можно было бы получить каждый раз прогоняя по всем данным, но достаточно близко, и это делается в основном для того, чтобы сократить время обучения модели.
	- Итак, подытожим, backpropagation это алгоритм для определения того, как одно обучающее изображение хочет изменить веса и смещения, чтобы максимально уменьшить функцию стоимости. Остальные изображения дают свои пожелания и они усредняются. Полноценный шаг градиентного спуска потребовал бы все изображения, но для ускорения вычисления мы разбиваем наши изображения на мини-батчи и вычисляем каждый шаг на мини-батчах. Мы будем делать такие шаги снова и снова, в итоге мы придем к локальному минимуму функции стоимости и наша модель будет давать более-менее точный результат.
- ### Backpropagation - 2.
  collapsed:: true
	- Давайте для простоты представить простую нейросеть, где в каждом слое будет по-одному нейрону активации.
	- ```
	                       L
	  o   -   o  -  o   -   o
	             a(L-1)    a(L)
	  ```
	- в таком случает функция стоимости C_{0} = (a^{(L)} - y)^{2}
	- a^{(L)} = \sigma(w^{(L)}\cdot a^{(L-1)} + b^{(L)})
	- Для упрощения примем z^{(L)} = (w^{(L)}\cdot a^{(L-1)} + b^{(L)}), тогда
	- a^{(L)} = \sigma(z^{(L)})
	- Такое отношение мы можем представить в виде цепочки отношений. Т.е. w^{(L)}, a^{(L-1)}, b^{(L)} от которых зависит z^{L}. Из z^{L} получаем a^{L} - активацию. В свою очередь из a^{L} и y получаем C_{0}.
	- Т.к. все эти параметры по сути цифры, проще представлять их как взаимосвязанные ползунки, изменение которых влияет на C_{0}. Или представить в виде дифференциала $$\frac{\partial{C_{0}}}{\partial{w^{L}}}$$. Другими словами нам интересна связь w^{L} с C_{0}.
	- $$\frac{\partial{C_{0}}}{\partial{w^{L}}} = \frac{\partial{z^{L}}}{\partial{w^{L}}}\frac{\partial{a^{L}}}{\partial{z^{L}}}\frac{\partial{C_{0}}}{\partial{a^{L}}}$$
	- подставляем значения частных производных и получаем:
	- $$\frac{\partial{C_{0}}}{\partial{w^{L}}} = \frac{\partial{z^{L}}}{\partial{w^{L}}}\frac{\partial{a^{L}}}{\partial{z^{L}}}\frac{\partial{C_{0}}}{\partial{a^{L}}} = a^{(L-1)}\cdot\sigma ' (z^{(L)})\cdot 2(a^{(L)}-y)$$
	- Не надо забывать, что это функция стоимости **только 1 изображения** поэтому их надо усреднить на все данные n (или на размер батча):
	- Для более сложной модели (с большим количеством нейронов в слое) все будет выглядеть примерно так же.
	-
- ### Transformers
  collapsed:: true
	- Наверное самым популярной моделью использующей трансформеры является GPT. Generative Pre-Trained Transformer (Генеративный Предварительно тренированный Трансформер), который использует модель Трансформер, который и является причиной бума ИИ.
	- Трансформер это очень сложная модель которая оперирует Attention Block, который позволяет учитывать контекст данных, потому что они зависят друг от друга. По сути, это небольшая часть данных внутри матрицы, которая коррелирует с друг другом, из-за чего изменение внутри них происходят одновременно. Или можно сказать, что изменение одних данных влечет изменение других.
	- Сама же модель представляет собой большое количество матриц, которые перемножаются с друг другом, а затем подаются на вход многослойного персептрона, а затем снова поступают на трансформер, который в свою очередь идет на перспетрон и так много раз.
	- Трансформер можно разделить на 4 этапа, которые происходят внутри него.
		- Embedding, Attention, MLPs, Unembedding
		- #### Embedding
			- Вообще, понятие embedding можно по-русски назвать токенизацией или предварительной обработкой данных. Для случая работы со звуком он включает разделение звуков на части, для рисунков разделение на клетки или пиксели (причем пиксели могут быть каких угодно размеров), для текста это разделение на части слов или целые слова. Для простоты мы будем считать что каждый токен является отдельным словом (хотя опять же подчеркнем, что это не так, токен это часть слова или даже один слог).
			- Каждый токен можно представить в виде многомерного вектора, который также можно для простоты представить в виде трехмерного вектора. При таком упрощении есть возможность визуализировать вектора в пространстве. Если мы рассмотрим слова похожие по смыслу, они будут лежать очень близко в пространстве, например башня, небоскреб, высотка и т.п. они все будут лежать рядом.
				- ![image.png](../assets/image_1761094764326_0.png)
			- Также, такие слова как мужчина и женщина тоже будут лежать рядом, но между ними будет достаточное расстояние, которое и есть различие пола. Т.е. Король и Королева, даже если они будут лежать в противоположном от мужчины и женщины расстоянии они будут друг от друга примерно на таком же расстоянии что и мужчина и женщина. Т.е. если запишем это математически:
				- Мужчина - Женщина = Король - Королева.
			- Тут заметим, что в реальности они не равны, но это вызвано нашей культурой, потому что Королева это не всегда женская версия Короля (особенно это касается английского языка с такими словами как Drag Queen и т.п.).
			- Также, если мы возьмем слово Гитлер и вычтем из нее Германию и прибавим Италию, то получится Муссолини (по крайней мере очень близко). Т.е. модель понимает, что между Гитлером и Муссолини разница только в том, что они из разных стран. При этом Гитлер и Муссолини будут в одной стороне, а Италия и Германия в другой, т.е. это в модели задается направлением вектора.
		- #### Unembedding
			- Здесь берется последний вектор, и сравнивается со словарем, чтобы получить вероятность появления того или иного слова, который трансформируется с вектора путем нормализации (функции softmax)
				- softmax используется для того, чтобы получить на выходе сумму значений вероятностей, которая должна давать 1, при этом значения должны находиться между 0 и 1. Но в случает машинного обучения у нас получатся значения от -5 до +5 (или каких то других значений, которые не лежат между 0 и 1) и их сумма точно не даст 1. Для этого и используется softmax.
				- Получается, что softmax это функция которая превращает любой набор чисел в вероятность, которая в сумме дает 1. При этом самое большое число в наборе будет ближе к 1, а самое меньшее ближе к 0. Математически это выглядит вот так:
				- пусть у нас есть набор чисел записанных в матрицу x1, x2, ... xn. Мы возьмем экспоненту для каждого числа e^{x1}, e^{x2}, ... e^{xn}, что даст значения > 0 (что избавляет нас от отрицательных чисел). Далее полученные значения суммируются и мы получаем $$\sum^{N-1}_{n=0}e^{xn}$$. И все значения e^{x1}, e^{x2}, ... e^{xn} делятся на полученную сумму, что дает нам вероятности от 0 до 1.
				- ![image.png](../assets/image_1761095094958_0.png)
			- Также, надо упомянуть, что есть softmax with temperature T. Это такой знаменатель на который делятся все значения x1,x2,x3.. xn. Т.е. мы получим e^{x1/T}, e^{x2/T}, ... e^{xn/T}, такой подход используется в чат ГПТ и тому подобных чат ботах, для того, чтобы промпт каждый раз давал разные результаты. При T >> 0 (когда стремится к большим числам), вес будет больше у меньших чисел, а при T = 0, вес будет больше у больших чисел. В генеративных ИИ температура выставляется между T= 0 и 2.
	- Специалисты по ИИ обычно называют значения x1,x2,... xn, которые подаются на вход в функцию softmax logits, а выход probabilities.
- ### Attention in Transformers
  collapsed:: true
	- Что же такое Attention в Трансформере, который является ключевым в них? Для удобства объяснений мы будем считать, что каждый токен представляет собой отдельное слово (хотя на самом деле, токен это часть слова).
	- До этого мы разбирали, что каждый токен это вектор, направление и значение которого зависит от самого слова. Т.е. слова родственные по смыслу будут лежать друг рядом с другом.
	- Если рассматривать Attention, то это будет целый набор векторов, составленный в матрицу. Где каждый вектор представляет собой отдельный токен. И значения представлены таким образом, чтобы произведение чисел придавали контекст словам. Для наглядности рассмотрим пример:
		- American shrew **mole**
		- One **mole** of carbon dioxide
		- Take a biopsy of the **mole**
	- Вектор токена который означает mole будет везде одинаковый, но при добавлении контекста (т.е. произведения слов) должно менять значение вектора таким образом, чтобы учитывался контекст и получался новый вектор.
	- Это если рассматривать короткие предложения. Но attention может хранить в памяти и гораздо больший текст. Например детективный роман. Который будет заканчиваться словом "убийцей является Y". И вот этот самый Y, трансформер и должен предсказать исходя из контекста. Т.е. весь контекст должен дать вектор, который будет наиболее вероятным в данном случае.
	- Рассмотрим пример гораздо проще, чтобы разобраться в работе attention
		- пушистое синее существо обитает в зеленом лесу
		- предположим, что единственное обновление это то, как прилагательные обновляют смысл существительного
		- Это будет называться одной головой внимания (head of attention)
		- А далее мы увидим, что блок внимания состоит из множества голов, работающих параллельно.
		- Каждый эмбеддинг будет вектором который означает каждое слово без всякого контекста, но включает положение слова в предложении.
		- И серией матричных произведений мы должны получить новое слово согласно данному контексту.
			- т.е. пушистый+синий+существо = синее существо с мехом
			- зеленый + лес = зеленый лес
			- слева будут стоят абстрактные общие слова, а справа конкретные примеры.
			- Это упрощенный взгляд на процесс обучения. Потому что реальная работа трансформера включает очень длительный процесс настройки и уточнения весов и параметров только для того, чтобы уменьшить функцию стоимости.
		- Пока рассмотрим в упрощенном виде:
			- существительное существо -> есть ли какие-нибудь прилагательные перед ним? и тогда синий и пушистый отвечают, мы прилагательные и мы стоим перед существительным.
			- Этот вопрос выглядит как какой-то вектор, который называется запрос (query). Отличие вектора-запроса от эмбеддинга в том, что он имеет гораздо меньший размер (например 128).
			- Чтобы получить этот вектор запрос нам надо умножить E3 (существо) на некий вектор W_{Q}. Что даст и сам запрос Q4
			- И таким образом мы умножаем все эмбеддинги на вектор W_{Q} и получаем вектор запрос для каждого токена
			- W_{Q} - это параметры модели, которые получаются путем обучения
			- В то же время у нас есть вторая матрица W_{K}, которая при умножении с токенами дает ключи(key) K. Этот К можно представить в виде ответа на вопрос вектора запроса.
			- Вектор K и Q должны лежать друг рядом с другом, т.е. ответ должен удовлетворять вопросу.
			- Чтобы проверить их совпадение мы перемножаем K и Q. Чем больше значение, тем лучше совпадение. Это можно представить как матрицу с заполненными ячейками. Если ячейка пустая, значит нет совпадения, а если есть то ячейка полностью заполнена. Произведение должно дать большое число в случае совпадения и маленькое (отрицательное) в случае отсутствия совпадения. Токены которые совпадают называется "Attend to", или они коррелируют.
			- ![image.png](../assets/image_1760926392153_0.png)
			- Затем к полученным значениям применятся softmax (по столбцам), чтобы получить значения от 0 до 1.
			- Математически это можно написать в таком виде:
			- $$Attention(Q,K,V)=softmax\left(\frac{K^{T}Q}{\sqrt{d_{k}}}\right)V,$$
			- где $$\sqrt{d_{k}}$$ - это квадратный корень размерности вектора
			- тут надо пояснить, что эффективнее обучать модель предугадывать каждое слово, т.е. что идет за словом пушистый, что идет за пушистый синий, что идет за пушистое синее существо и т.п.
			- И чтобы модель не подсматривала, нам надо те слова, которые идут после искомого (существо), убрать из "уравнения", т.е. при произведении K и Q выставить их значения равным -\inftyю Такой прием называется ***masking***
			- Далее, как нам учесть влияние прилагательного на существительное в качестве контекста? Для простоты представим, что нам нужен третья матрица такого же размера, что и токен, который называется матрица стоимости Value Vector W_{V}
			- После этого мы получим вектор весов, который при умножении с вектором существительного должен двигать его таким образом, чтобы контекст учитывался. Мы получим сумму произведений \Delta E, который необходимо прибавить к токену существо, чтобы получился искомый вариант. и это делается для всех эмбеддингов.
			- Все это и есть одна голова внимания
		- Этот пример касается только т.н. self-attention, но есть еще и cross-attention. Cross-attention используется в сопоставлении слов в разных языках (переводчиках) и сопоставлении текста и голоса. Отличие в том, что в cross-attention не используется masking, потому что порядок слов не важен.
- ### MLP - Multi-Layer Perceptron
	- многослойный персептрон это последовательность линейной регрессии, ReLU и линейной регрессии.
	- Чтобы понять как работает персептрон в случае такой большой нейросети, нам нужно сделать несколько предположений. Допустим, у нас есть предложение: "Майкл Джордан играет в Y". И чтобы модель давала ответ, во что же играет Майкл Джордан, у нас произведение векторов Майкл и Джордан должно давать 1. И этот результирующий вектор должен быть близок к баскетболу.
	- Как это работает внутри перспетрона?
		- Мы берем большую матрицу и умножаем на вектор, чтобы получить еще 1 вектор. RxE.
		- Если имя будет Майкл, то такое произведение должно дать 1. Остальные 0 или меньше. А для Майкла Джордана будет 2. (М+Д)*Е = 2, или 1 и меньше при других случаях. Например, если будет Майкл Фелпс, то совпадение по имени, но по фамилии нет. Майкл Фелпс не играет в баскетбол.
		- Но это если у нас нет баяса. С баясом мы можем предположить, что в случае Майкла Джордана он равен -1, что даст искомое значение 1.
		- Здесь каждую строку матрицы можно представить в виде вопроса, который чатГПТ задает полученному предложению.
		- Тут надо отметить, что первая линейная функция имеет размерность выше, чем который дает Трансформер.
		- Затем мы должны срезать полученные значения на простой да или нет. Т.е. используем ReLU, который все "не совсем" и пограничные ответы (значения ниже 0) превращает в 0, а положительные оставляет без изменения.
		- После ReLU у нас еще одна линейная функция, которая понижает размерность наших данных до уровня необходимого для трансформера.